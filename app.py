import streamlit as st
import pandas as pd
import os
import re
import zipfile
import io
import time
import requests
import json
import numpy as np
from concurrent.futures import ThreadPoolExecutor, as_completed
from tenacity import retry, stop_after_attempt, wait_exponential

# ====================
# Core Classes for AI & Data
# ====================

class LLMClient:
    """å¤„ç†ä¸å¤§æ¨¡å‹çš„äº¤äº’ (Embedding å’Œ Chat)"""
    def __init__(self, config):
        self.base_url = config.get('base_url', '').rstrip('/')
        self.api_key = config.get('api_key')
        self.model = config.get('model')
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
    def get_embedding(self, text):
        """è·å–æ–‡æœ¬çš„å‘é‡è¡¨ç¤º (é»˜è®¤å°è¯•å…¼å®¹ OpenAI æ ¼å¼çš„ embedding æ¥å£)"""
        # æ³¨æ„ï¼šä¸åŒçš„æ¨¡å‹å•† Embedding URL å¯èƒ½ä¸åŒï¼Œè¿™é‡Œé»˜è®¤ä½¿ç”¨ OpenAI å…¼å®¹è·¯å¾„
        # å¯¹äº Geminiï¼Œé€šå¸¸æ˜¯ models/embedding-001
        # ä¸ºäº†å…¼å®¹æ€§ï¼Œè¿™é‡Œåšä¸€ä¸ªç®€å•çš„è·¯å¾„é€‚é…ï¼Œæˆ–è€…ç”±ç”¨æˆ·æŒ‡å®š Embedding Model
        
        # ç®€åŒ–å¤„ç†ï¼šå°è¯•ä½¿ç”¨ text-embedding-004 æˆ–ç”¨æˆ·æŒ‡å®šçš„é€šç”¨ embedding æ¨¡å‹
        embedding_model = "text-embedding-004" # é»˜è®¤ä¸€ä¸ªè¾ƒæ–°çš„æ¨¡å‹
        
        payload = {
            "input": text.replace("\n", " "),
            "model": embedding_model
        }
        
        # å°è¯•æ ‡å‡† OpenAI è·¯å¾„
        url = f"{self.base_url}/embeddings"
        
        try:
            response = requests.post(url, headers=self.headers, json=payload, timeout=10)
            if response.status_code == 200:
                return response.json()['data'][0]['embedding']
            else:
                # å¦‚æœå¤±è´¥ï¼Œå¯¹äº Gemini å¯èƒ½æ˜¯ä¸åŒçš„è·¯å¾„ï¼Œè¿™é‡Œæš‚ä¸åšæå…¶å¤æ‚çš„è‡ªåŠ¨æ¢æµ‹
                # å®é™…ç”Ÿäº§ä¸­åº”å¢åŠ æ›´å¤šçš„ endpoint é€‚é…
                print(f"Embedding failed: {response.text}")
                return None
        except Exception as e:
            print(f"Embedding error: {e}")
            return None

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
    def chat_completion(self, system_prompt, user_prompt):
        """è°ƒç”¨ Chat æ¥å£"""
        payload = {
            "model": self.model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            "temperature": 0.1,
            "response_format": {"type": "json_object"}
        }
        
        response = requests.post(f"{self.base_url}/chat/completions", headers=self.headers, json=payload, timeout=60)
        if response.status_code == 200:
            content = response.json()['choices'][0]['message']['content']
            # æ¸…ç†å¯èƒ½çš„ markdown æ ‡è®°
            content = content.replace("```json", "").replace("```", "")
            return json.loads(content)
        else:
            raise Exception(f"API Error {response.status_code}: {response.text}")

class VectorStore:
    """ç®€å•çš„å†…å­˜å‘é‡æ•°æ®åº“"""
    def __init__(self):
        self.documents = [] # å­˜å‚¨åŸæ–‡ç‰‡æ®µ: {'id': int, 'text': str, 'source': str}
        self.vectors = []   # å­˜å‚¨å¯¹åº”çš„ numpy å‘é‡
        self.llm_client = None

    def set_client(self, client):
        self.llm_client = client

    def add_documents(self, file_corpus):
        """
        å¤„ç†å¹¶å…¥åº“
        file_corpus: [{'name': 'filename', 'content': 'full text'}, ...]
        """
        # 1. Chunking (åˆ‡ç‰‡)
        chunk_size = 500 # å­—ç¬¦æ•°
        overlap = 50 
        
        self.documents = []
        texts_to_embed = []
        
        doc_id = 0
        for file in file_corpus:
            text = file['content']
            name = file['name']
            
            # ç®€å•çš„æ»‘åŠ¨çª—å£åˆ‡ç‰‡
            for i in range(0, len(text), chunk_size - overlap):
                chunk = text[i:i + chunk_size]
                if len(chunk) < 50: continue # è·³è¿‡å¤ªçŸ­çš„
                
                self.documents.append({
                    'id': doc_id,
                    'text': chunk,
                    'source': name
                })
                texts_to_embed.append(chunk)
                doc_id += 1
        
        # 2. Embedding (æ‰¹é‡æˆ–é€ä¸ª)
        # å®é™…ç”Ÿäº§ä¸­åº”è¯¥ Batch APIï¼Œè¿™é‡Œç®€åŒ–ä¸ºé€ä¸ªä½†ç”¨ ThreadPool åŠ é€Ÿ
        if not self.llm_client:
            return
            
        vectors = []
        with st.status("æ­£åœ¨å¯¹åˆ¶åº¦æ–‡æ¡£è¿›è¡Œé‡åŒ–å¤„ç† (Embedding)...") as status:
            total = len(texts_to_embed)
            completed = 0
            
            # ä½¿ç”¨å¹¶å‘åŠ é€Ÿ Embedding
            with ThreadPoolExecutor(max_workers=10) as executor:
                future_to_idx = {executor.submit(self.llm_client.get_embedding, t): i for i, t in enumerate(texts_to_embed)}
                
                # åˆå§‹åŒ–ä¸€ä¸ªå®šé•¿åˆ—è¡¨
                results = [None] * total
                
                for future in as_completed(future_to_idx):
                    idx = future_to_idx[future]
                    vec = future.result()
                    results[idx] = vec
                    
                    completed += 1
                    if completed % 10 == 0:
                        status.update(label=f"æ­£åœ¨é‡åŒ–æ–‡æ¡£... ({completed}/{total})")
            
            # è¿‡æ»¤æ‰å¤±è´¥çš„ Embedding (None) å¹¶åŒæ­¥ç§»é™¤ document
            valid_vectors = []
            valid_docs = []
            for i, vec in enumerate(results):
                if vec is not None:
                    valid_vectors.append(vec)
                    valid_docs.append(self.documents[i])
            
            self.vectors = np.array(valid_vectors)
            self.documents = valid_docs
            status.update(label="æ–‡æ¡£é‡åŒ–å®Œæˆï¼", state="complete")

    def search(self, query_text, top_k=3):
        """è¯­ä¹‰æ£€ç´¢"""
        if self.llm_client is None or len(self.vectors) == 0:
            return []

        query_vec = self.llm_client.get_embedding(query_text)
        if query_vec is None:
            return []
            
        query_vec = np.array(query_vec)
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦: (A . B) / (|A| * |B|)
        # å‡è®¾å‘é‡å·²ç»æ˜¯å½’ä¸€åŒ–çš„ï¼ˆOpenAI embedding é€šå¸¸æ˜¯ï¼‰ï¼Œåˆ™ dot product å³å¯
        # ä¸ºä¿é™©ï¼Œæ‰‹åŠ¨è®¡ç®—å½’ä¸€åŒ–ä½™å¼¦ç›¸ä¼¼åº¦
        norm_vectors = np.linalg.norm(self.vectors, axis=1)
        norm_query = np.linalg.norm(query_vec)
        
        if norm_query == 0: return []
        
        similarities = np.dot(self.vectors, query_vec) / (norm_vectors * norm_query)
        
        # è·å– Top K
        top_indices = np.argsort(similarities)[-top_k:][::-1]
        
        results = []
        for idx in top_indices:
            score = similarities[idx]
            doc = self.documents[idx]
            results.append({
                'source': doc['source'],
                'content': doc['text'],
                'score': float(score)
            })
            
        return results

# ====================
# Helper Functions
# ====================

def process_uploaded_files(uploaded_files):
    """å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶"""
    for uploaded_file in uploaded_files:
        if uploaded_file.name.endswith('.zip'):
            try:
                with zipfile.ZipFile(uploaded_file) as z:
                    for filename in z.namelist():
                        if filename.endswith('/') or filename.startswith('__MACOSX') or filename.startswith('._'):
                            continue
                        if filename.endswith(('.docx', '.xlsx')):
                            with z.open(filename) as f:
                                yield filename, f.read()
            except Exception as e:
                st.error(f"è§£å‹æ–‡ä»¶ {uploaded_file.name} å¤±è´¥: {str(e)}")
        else:
            yield uploaded_file.name, uploaded_file.getvalue()

import docx

def extract_text_from_content(filename, content):
    """æå–çº¯æ–‡æœ¬ (ä½¿ç”¨ robust åº“)"""
    text = ""
    try:
        file_stream = io.BytesIO(content)
        if filename.endswith('.docx'):
            doc = docx.Document(file_stream)
            text = '\n'.join([para.text for para in doc.paragraphs])
        elif filename.endswith('.xlsx'):
            df_dict = pd.read_excel(file_stream, sheet_name=None, header=None)
            text_parts = []
            for sheet_name, df in df_dict.items():
                # å°†æ¯ä¸€è¡Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œç”¨ç©ºæ ¼è¿æ¥
                sheet_text = df.astype(str).apply(lambda x: ' '.join(x), axis=1)
                text_parts.append('\n'.join(sheet_text))
            text = '\n'.join(text_parts)
    except Exception as e:
        print(f"Error parsing {filename}: {e}")
    return text

def parse_regulation_clauses(text):
    """è§£ææ³•è§„æ¡æ¬¾ (ä¼˜åŒ–ç‰ˆ)"""
    pattern = r'(ç¬¬\s*[\dé›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+\s*æ¡|Article\s+\d+)'
    parts = re.split(pattern, text)
    
    clauses = []
    if len(parts) > 1:
        for i in range(1, len(parts), 2):
            title = parts[i].strip()
            content = parts[i+1].strip() if i+1 < len(parts) else ""
            
            # ç®€å•çš„é€‚ç”¨æ€§é¢„åˆ¤
            applicability = "é€‚ç”¨"
            gov_keywords = ["å›½åŠ¡é™¢", "å¿çº§ä»¥ä¸Š", "ç›‘å¯Ÿæœºå…³", "äººæ°‘æ”¿åºœ", "ä¸»ç®¡éƒ¨é—¨", "è¡Œæ”¿æœºå…³"]
            corp_keywords = ["ç”Ÿäº§ç»è¥å•ä½", "ä¼ä¸š", "ç”¨äººå•ä½", "å»ºè®¾å•ä½", "å…¬å¸"]
            
            content_lower = content.lower()
            is_gov = any(k in content_lower for k in gov_keywords)
            is_corp = any(k in content_lower for k in corp_keywords)
            
            if is_gov and not is_corp:
                applicability = "ä¸é€‚ç”¨(æ”¿åºœèŒè´£)"
            
            clauses.append({
                "æ¡æ¬¾å·": title,
                "æ³•è§„æ­£æ–‡": title + " " + content,
                "é€‚ç”¨æ€§": applicability
            })
    return clauses

def evaluate_single_clause(clause, vector_store, llm_client):
    """
    å•ä¸ªæ¡æ¬¾çš„åˆ†æé€»è¾‘ (è®¾è®¡ä¸ºå¹¶å‘è°ƒç”¨)
    """
    row = {
        "æ¡æ¬¾å·": clause['æ¡æ¬¾å·'],
        "æ³•è§„æ­£æ–‡": clause['æ³•è§„æ­£æ–‡'],
        "è¯„ä»·ç»“è®º": "âŒç¼ºå¤±/ä¸ç¬¦åˆ",
        "æ”¯æ’‘è¯æ®": "æœªæ£€ç´¢åˆ°ç›¸å…³åˆ¶åº¦",
        "åŒ¹é…åº¦": 0.0
    }
    
    if clause['é€‚ç”¨æ€§'] != "é€‚ç”¨":
        row['è¯„ä»·ç»“è®º'] = "â—ä¸é€‚ç”¨"
        row['æ”¯æ’‘è¯æ®'] = "æ¡æ¬¾ä¸»ä½“éä¼ä¸š"
        return row

    # 1. è¯­ä¹‰æ£€ç´¢ (Retrieval)
    # é˜ˆå€¼è®¾å®šï¼šå¦‚æœç›¸ä¼¼åº¦ä½äº 0.35ï¼Œè®¤ä¸ºæ ¹æœ¬æ²¡æœ‰ç›¸å…³åˆ¶åº¦ï¼Œç›´æ¥è·³è¿‡ LLM
    search_results = vector_store.search(clause['æ³•è§„æ­£æ–‡'], top_k=3)
    
    if not search_results:
        return row
        
    top_score = search_results[0]['score']
    row['åŒ¹é…åº¦'] = top_score
    
    # é˜ˆå€¼è¿‡æ»¤ (Pre-filtering)
    if top_score < 0.35:
        row['è¯„ä»·ç»“è®º'] = "âŒç¼ºå¤±/ä¸ç¬¦åˆ"
        row['æ”¯æ’‘è¯æ®'] = f"æœªæ‰¾åˆ°åŒ¹é…åˆ¶åº¦ (æœ€é«˜ç›¸ä¼¼åº¦ {top_score:.2f} ä½äºé˜ˆå€¼)"
        return row
        
    # 2. LLM è¯„ä¼° (Evaluation)
    evidence_text = ""
    for i, res in enumerate(search_results):
        evidence_text += f"å‚è€ƒç‰‡æ®µ {i+1} (æ¥æº: {res['source']}, ç›¸ä¼¼åº¦: {res['score']:.2f}):\n{res['content']}\n---\n"
    
    system_prompt = "ä½ æ˜¯ä¸€ä¸ªEHSåˆè§„ä¸“å®¶ã€‚è¯·å¯¹æ¯”æ³•è§„æ¡æ¬¾å’Œä¼ä¸šåˆ¶åº¦ï¼Œåˆ¤æ–­æ˜¯å¦åˆè§„ã€‚"
    user_prompt = f"""
ã€æ³•è§„æ¡æ¬¾ã€‘
{clause['æ³•è§„æ­£æ–‡']}

ã€ä¼ä¸šåˆ¶åº¦å‚è€ƒç‰‡æ®µã€‘
{evidence_text}

è¯·ä¸¥æ ¼åŸºäºä¸Šè¿°å‚è€ƒç‰‡æ®µè¿›è¡Œåˆ¤æ–­ã€‚å¦‚æœä¸ç¬¦åˆæˆ–ç‰‡æ®µä¸ç›¸å…³ï¼Œè¯·ç›´è¯´ã€‚
è¿”å›JSONæ ¼å¼:
{{
    "status": "âœ…å®Œå…¨ç¬¦åˆ" æˆ– "âš ï¸éƒ¨åˆ†ç¬¦åˆ/éœ€å®Œå–„" æˆ– "âŒç¼ºå¤±/ä¸ç¬¦åˆ",
    "evidence": "ç®€è¦å¼•ç”¨çš„åˆ¶åº¦å†…å®¹",
    "reason": "ä¸€å¥è¯åˆ¤å®šç†ç”±"
}}
"""
    try:
        result = llm_client.chat_completion(system_prompt, user_prompt)
        row['è¯„ä»·ç»“è®º'] = result.get('status', 'âŒç¼ºå¤±/ä¸ç¬¦åˆ')
        row['æ”¯æ’‘è¯æ®'] = f"{result.get('evidence', '')}\n(AIç†ç”±: {result.get('reason', '')})"
    except Exception as e:
        row['æ”¯æ’‘è¯æ®'] = f"LLMåˆ†æå¤±è´¥: {str(e)}"
        
    return row

# ====================
# Streamlit UI
# ====================

st.set_page_config(page_title="EHSæ™ºèƒ½åˆè§„å¼•æ“ (Proç‰ˆ)", layout="wide")

if 'results' not in st.session_state:
    st.session_state.results = None

st.title("ğŸ›¡ï¸ EHSæ³•è§„åˆè§„æ€§æ™ºèƒ½è¯„ä»·å¼•æ“ (Proç‰ˆ)")
st.markdown("ğŸš€ **æ ¸å¿ƒå‡çº§**ï¼šé‡‡ç”¨ `Embeddingè¯­ä¹‰å‘é‡åŒ–` + `å¹¶å‘åŠ é€Ÿ`ï¼Œå¤§å¹…æå‡å‡†ç¡®ç‡ä¸åˆ†æé€Ÿåº¦ã€‚")

with st.sidebar:
    st.header("1. é…ç½®ä¸ä¸Šä¼ ")
    llm_base_url = st.text_input("API Base URL", value="https://generativelanguage.googleapis.com/v1beta/openai", help="OpenAI å…¼å®¹æ¥å£åœ°å€")
    llm_api_key = st.text_input("API Key", type="password")
    llm_model_name = st.text_input("Model Name", value="gemini-2.0-flash")
    
    st.divider()
    reg_files = st.file_uploader("ä¸Šä¼ æ³•è§„ (docx/zip)", type=['docx', 'zip'], accept_multiple_files=True, key="reg")
    policy_files = st.file_uploader("ä¸Šä¼ åˆ¶åº¦ (docx/xlsx/zip)", type=['docx', 'xlsx', 'zip'], accept_multiple_files=True, key="pol")

if st.button("ğŸš€ å¼€å§‹æé€Ÿåˆ†æ", type="primary"):
    if not (reg_files and policy_files and llm_api_key):
        st.error("è¯·ç¡®ä¿æ–‡ä»¶å·²ä¸Šä¼ ä¸” API Key å·²å¡«å†™ã€‚" )
    else:
        # åˆå§‹åŒ–ç»„ä»¶
        llm_config = {"base_url": llm_base_url, "api_key": llm_api_key, "model": llm_model_name}
        client = LLMClient(llm_config)
        vector_store = VectorStore()
        vector_store.set_client(client)
        
        # 1. å¤„ç†åˆ¶åº¦åº“ (æ„å»ºå‘é‡ç´¢å¼•)
        policy_corpus = []
        for name, content in process_uploaded_files(policy_files):
            text = extract_text_from_content(name, content)
            if text: policy_corpus.append({'name': name, 'content': text})
            
        if not policy_corpus:
            st.error("æ— æ³•ä»åˆ¶åº¦æ–‡ä»¶ä¸­æå–æ–‡æœ¬ã€‚" )
            st.stop()
            
        vector_store.add_documents(policy_corpus)
        
        # 2. è§£ææ³•è§„
        all_clauses = []
        for name, content in process_uploaded_files(reg_files):
            text = extract_text_from_content(name, content)
            clauses = parse_regulation_clauses(text)
            for c in clauses:
                c['source_file'] = name # è®°å½•æ¥æº
                all_clauses.append(c)
                
        st.info(f"å…±è§£æå‡º {len(all_clauses)} æ¡æ³•è§„æ¡æ¬¾ï¼Œæ­£åœ¨å¹¶å‘åˆ†æä¸­...")
        
        # 3. å¹¶å‘åˆ†æ (Map-Reduce)
        progress_bar = st.progress(0)
        status_text = st.empty()
        results_list = []
        
        total_tasks = len(all_clauses)
        completed_tasks = 0
        
        # å¼€å¯çº¿ç¨‹æ±  (IOå¯†é›†å‹ä»»åŠ¡ï¼Œé€‚åˆå¤šçº¿ç¨‹)
        with ThreadPoolExecutor(max_workers=10) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_clause = {
                executor.submit(evaluate_single_clause, clause, vector_store, client): clause 
                for clause in all_clauses
            }
            
            for future in as_completed(future_to_clause):
                try:
                    res = future.result()
                    res['æ³•è§„æ–‡ä»¶'] = future_to_clause[future]['source_file']
                    results_list.append(res)
                except Exception as exc:
                    st.warning(f"æŸæ¡æ¬¾åˆ†æå¼‚å¸¸: {exc}")
                
                completed_tasks += 1
                progress_bar.progress(completed_tasks / total_tasks)
                status_text.text(f"å·²å®Œæˆ: {completed_tasks}/{total_tasks} ...")
                
        st.success("åˆ†æå®Œæˆï¼")
        st.session_state.results = pd.DataFrame(results_list)

# --- ç»“æœå±•ç¤º (å³ä½¿åˆ·æ–°é¡µé¢ï¼Œåªè¦ session_state åœ¨å°±èƒ½æ˜¾ç¤º) ---
if st.session_state.results is not None:
    df = st.session_state.results
    
    st.divider()
    st.subheader("ğŸ“Š åˆ†æç»“æœçœ‹æ¿")
    
    col1, col2, col3 = st.columns(3)
    col1.metric("å®Œå…¨ç¬¦åˆ", len(df[df['è¯„ä»·ç»“è®º']=="âœ…å®Œå…¨ç¬¦åˆ"]))
    col2.metric("éœ€å®Œå–„", len(df[df['è¯„ä»·ç»“è®º']=="âš ï¸éƒ¨åˆ†ç¬¦åˆ/éœ€å®Œå–„"]))
    col3.metric("ç¼ºå¤±/ä¸ç¬¦åˆ", len(df[df['è¯„ä»·ç»“è®º'].str.contains("ç¼ºå¤±|ä¸ç¬¦åˆ")]))
    
    # ç­›é€‰å™¨
    status_filter = st.multiselect("ç­›é€‰ç»“è®º", df['è¯„ä»·ç»“è®º'].unique(), default=df['è¯„ä»·ç»“è®º'].unique())
    show_df = df[df['è¯„ä»·ç»“è®º'].isin(status_filter)]
    
    st.dataframe(
        show_df,
        column_config={
            "æ³•è§„æ­£æ–‡": st.column_config.TextColumn("æ³•è§„è¦æ±‚", width="medium"),
            "æ”¯æ’‘è¯æ®": st.column_config.TextColumn("åˆ¶åº¦è¯æ® & AIç†ç”±", width="large"),
            "åŒ¹é…åº¦": st.column_config.ProgressColumn("è¯­ä¹‰ç›¸ä¼¼åº¦", min_value=0, max_value=1, format="%.2f")
        },
        use_container_width=True,
        height=600
    )
    
    # ä¸‹è½½
    csv = df.to_csv(index=False).encode('utf-8-sig')
    st.download_button("ğŸ“¥ ä¸‹è½½è¯¦ç»†æŠ¥è¡¨ (CSV)", csv, "ehs_compliance_report.csv", "text/csv")