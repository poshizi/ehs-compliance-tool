import streamlit as st
import pandas as pd
import os
import re
import zipfile
import io
import time
import requests
import json
import numpy as np
import docx 
import pickle
import hashlib
import xml.etree.ElementTree as ET
from urllib.parse import urljoin, quote, unquote
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH
from concurrent.futures import ThreadPoolExecutor, as_completed
from tenacity import retry, stop_after_attempt, wait_exponential

# ====================
# Configuration & Constants
# ====================
CACHE_DIR = "vector_store_cache"
if not os.path.exists(CACHE_DIR):
    os.makedirs(CACHE_DIR)

# ====================
# Lightweight WebDAV Client (With Folder Support)
# ====================

class SimpleWebDavClient:
    """åŸºäº requests çš„è½»é‡çº§ WebDAV å®¢æˆ·ç«¯"""
    def __init__(self, base_url, username, password):
        self.base_url = base_url.rstrip('/') 
        self.auth = (username, password)
        self.session = requests.Session()
        self.session.auth = self.auth
    
    def list(self, path="/"):
        """åˆ—å‡ºæŒ‡å®šè·¯å¾„ä¸‹çš„æ–‡ä»¶å’Œæ–‡ä»¶å¤¹"""
        path = path.strip('/')
        full_url = f"{self.base_url}/{path}/" if path else f"{self.base_url}/"
        
        headers = {'Depth': '1'}
        try:
            response = self.session.request('PROPFIND', full_url, headers=headers)
            if response.status_code in [200, 207]:
                return self._parse_propfind(response.content, full_url)
            else:
                raise Exception(f"WebDAV Error: {response.status_code} - {response.text}")
        except Exception as e:
            raise Exception(f"Connection failed: {str(e)}")

    def download(self, path):
        """ä¸‹è½½æ–‡ä»¶å†…å®¹"""
        # path æ˜¯ç›¸å¯¹äº base_url çš„è·¯å¾„
        full_url = f"{self.base_url}/{quote(path.strip('/'))}"
        response = self.session.get(full_url)
        if response.status_code == 200:
            return response.content
        else:
            raise Exception(f"Download failed: {response.status_code}")

    def _parse_propfind(self, xml_content, current_url):
        """è§£æ XML å“åº”è·å–æ–‡ä»¶åˆ—è¡¨ï¼ŒåŒºåˆ†æ–‡ä»¶å’Œæ–‡ä»¶å¤¹"""
        items = []
        try:
            root = ET.fromstring(xml_content)
            # å¤„ç† namespaceï¼ŒWebDAV å“åº”é€šå¸¸å¸¦æœ‰å¤æ‚çš„ namespace
            # ç®€å•å¤„ç†ï¼šå¿½ç•¥ namespace ç›´æ¥æŸ¥æ‰¾ local name
            for response in root.findall('.//{DAV:}response'):
                href = response.find('.//{DAV:}href').text
                href = unquote(href)
                
                # åˆ¤æ–­æ˜¯å¦æ˜¯é›†åˆ (æ–‡ä»¶å¤¹)
                resourcetype = response.find('.//{DAV:}resourcetype')
                is_collection = False
                if resourcetype is not None:
                    if resourcetype.find('.//{DAV:}collection') is not None:
                        is_collection = True
                
                # æå–ç›¸å¯¹è·¯å¾„/åç§°
                # href é€šå¸¸æ˜¯ /dav/folder/file.docx
                # æˆ‘ä»¬éœ€è¦æå–å‡ºæ˜¾ç¤ºåç§°
                name = href.rstrip('/').split('/')[-1]
                
                # è¿‡æ»¤æ‰å½“å‰ç›®å½•æœ¬èº«
                # æ¯”è¾ƒ href å’Œ current_url çš„è·¯å¾„éƒ¨åˆ†æ˜¯å¦ä¸€è‡´
                # è¿™é‡Œç®€å•æ¯”å¯¹ name æ˜¯å¦ä¸ºç©º (æ ¹ç›®å½•) æˆ– href æ˜¯å¦ç­‰äº current_url
                
                if not name: continue 
                
                items.append({
                    'name': name,
                    'path': href, # ä¿ç•™å®Œæ•´ href ç”¨äºå¯¼èˆª
                    'is_folder': is_collection
                })
        except Exception as e:
            print(f"XML Parsing Error: {e}")
            pass
        
        # è¿‡æ»¤æ‰å½“å‰ç›®å½•è‡ªå·± (é€šå¸¸å®ƒæ˜¯åˆ—è¡¨çš„ç¬¬ä¸€ä¸ªï¼Œä¸”åå­—å’Œå½“å‰ç›®å½•ä¸€æ ·)
        # è¿™é‡Œåšä¸€ä¸ªç®€å•çš„å»é‡é€»è¾‘ï¼šå¦‚æœåˆ—è¡¨é‡Œæœ‰ path ç»“å°¾å’Œè¯·æ±‚ path ä¸€æ ·çš„ï¼Œå»æ‰
        return items

# ====================
# Core Classes for AI & Data
# ====================

class LLMClient:
    """å¤„ç†ä¸å¤§æ¨¡å‹çš„äº¤äº’ (Embedding å’Œ Chat)"""
    def __init__(self, config):
        self.base_url = config.get('base_url', '').rstrip('/')
        self.api_key = config.get('api_key')
        self.model = config.get('model')
        self.embedding_model = config.get('embedding_model', 'text-embedding-004')
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        self.embedding_failed = False 

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
    def get_embedding(self, text):
        """è·å–æ–‡æœ¬çš„å‘é‡è¡¨ç¤º"""
        if self.embedding_failed: return None
        
        payload = {
            "input": text.replace("\n", " "),
            "model": self.embedding_model
        }
        url = f"{self.base_url}/embeddings"
        
        try:
            response = requests.post(url, headers=self.headers, json=payload, timeout=10)
            if response.status_code == 200:
                data = response.json()
                if 'data' in data and len(data['data']) > 0:
                    return data['data'][0]['embedding']
            else:
                print(f"Embedding failed: {response.status_code}")
                return None
        except Exception as e:
            print(f"Embedding error: {e}")
            return None

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
    def chat_completion(self, system_prompt, user_prompt):
        """è°ƒç”¨ Chat æ¥å£"""
        payload = {
            "model": self.model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            "temperature": 0.1,
            "response_format": {"type": "json_object"}
        }
        try:
            response = requests.post(f"{self.base_url}/chat/completions", headers=self.headers, json=payload, timeout=60)
            if response.status_code == 200:
                content = response.json()['choices'][0]['message']['content']
                content = content.replace("```json", "").replace("```", "")
                return json.loads(content)
            else:
                raise Exception(f"API Error {response.status_code}: {response.text}")
        except Exception as e:
            raise Exception(f"Request failed: {str(e)}")
            
    def test_connection(self):
        results = {"chat": False, "embedding": False, "msg": ""}
        try:
            self.chat_completion("Test", "Reply JSON: {'status': 'ok'}")
            results['chat'] = True
        except Exception as e: results['msg'] += f"Chat Error: {e}\n"
        try:
            if self.get_embedding("test"): results['embedding'] = True
            else: results['msg'] += "Embedding Error: None (Check model)\n"
        except Exception as e: results['msg'] += f"Embedding Exception: {e}\n"
        return results

class VectorStore:
    """æŒä¹…åŒ–å‘é‡æ•°æ®åº“ (æ”¯æŒåˆ¶åº¦åº“ + æ³•è§„åº“)"""
    def __init__(self):
        self.documents = []   # åˆ¶åº¦æ–‡æ¡£
        self.vectors = []     # åˆ¶åº¦å‘é‡
        self.regulations = [] # æ³•è§„æ–‡æ¡£ (ç¼“å­˜ç”¨)
        self.llm_client = None

    def set_client(self, client):
        self.llm_client = client

    def save_to_disk(self, name="default"):
        """ä¿å­˜å®Œæ•´åº“ (åˆ¶åº¦ + æ³•è§„)"""
        path = os.path.join(CACHE_DIR, f"{name}.pkl")
        data = {
            "documents": self.documents,
            "vectors": self.vectors,
            "regulations": self.regulations
        }
        with open(path, "wb") as f:
            pickle.dump(data, f)
        return path

    def load_from_disk(self, name="default"):
        """åŠ è½½åº“"""
        path = os.path.join(CACHE_DIR, f"{name}.pkl")
        if os.path.exists(path):
            with open(path, "rb") as f:
                data = pickle.load(f)
                self.documents = data.get("documents", [])
                self.vectors = data.get("vectors", [])
                self.regulations = data.get("regulations", [])
            return True
        return False

    def add_documents(self, file_corpus):
        """æ·»åŠ åˆ¶åº¦æ–‡æ¡£å¹¶å‘é‡åŒ–"""
        chunk_size = 500
        overlap = 50 
        new_docs = []
        texts_to_embed = []
        start_doc_id = len(self.documents)
        
        for file in file_corpus:
            if any(d['source'] == file['name'] for d in self.documents): continue
            
            text = file['content']
            for i in range(0, len(text), chunk_size - overlap):
                chunk = text[i:i + chunk_size]
                if len(chunk) < 50: continue 
                keywords = set(re.split(r'[ï¼Œã€‚ï¼›ï¼š\s]', chunk))
                keywords = [k for k in keywords if len(k) > 1]
                new_docs.append({'id': start_doc_id, 'text': chunk, 'source': file['name'], 'keywords': keywords})
                texts_to_embed.append(chunk)
                start_doc_id += 1
        
        if not texts_to_embed: return

        if self.llm_client:
            with st.status(f"æ­£åœ¨å‘é‡åŒ– {len(texts_to_embed)} ä¸ªæ–°åˆ¶åº¦ç‰‡æ®µ...") as status:
                new_vectors = [None] * len(texts_to_embed)
                with ThreadPoolExecutor(max_workers=5) as executor:
                    future_to_idx = {executor.submit(self.llm_client.get_embedding, t): i for i, t in enumerate(texts_to_embed)}
                    for future in as_completed(future_to_idx):
                        idx = future_to_idx[future]
                        new_vectors[idx] = future.result()
                
                self.documents.extend(new_docs)
                self.vectors = (list(self.vectors) if len(self.vectors)>0 else []) + new_vectors
                status.update(label="åˆ¶åº¦å…¥åº“å®Œæˆ", state="complete")

    def add_regulations(self, file_corpus):
        """æ·»åŠ æ³•è§„æ–‡æ¡£ (æ— éœ€å‘é‡åŒ–ï¼Œä»…è§£æä¿å­˜)"""
        count = 0
        for file in file_corpus:
            # æŸ¥é‡
            if any(r['name'] == file['name'] for r in self.regulations): continue
            self.regulations.append(file) # file: {'name': name, 'content': text}
            count += 1
        return count

    def search(self, query_text, top_k=3):
        """æ··åˆæ£€ç´¢ (ä»…é’ˆå¯¹åˆ¶åº¦åº“)"""
        vec_results = []
        valid_indices = [i for i, v in enumerate(self.vectors) if v is not None]
        
        if valid_indices and self.llm_client:
            query_vec = self.llm_client.get_embedding(query_text)
            if query_vec is not None:
                q_v = np.array(query_vec)
                norm_q = np.linalg.norm(q_v)
                matrix = np.array([self.vectors[i] for i in valid_indices])
                norm_matrix = np.linalg.norm(matrix, axis=1)
                if norm_q > 0:
                    scores = np.dot(matrix, q_v) / (norm_matrix * norm_q)
                    top_k_indices = np.argsort(scores)[-top_k:][::-1]
                    for idx_in_valid in top_k_indices:
                        real_idx = valid_indices[idx_in_valid]
                        score = scores[idx_in_valid]
                        if score > 0: vec_results.append({'doc': self.documents[real_idx], 'score': float(score), 'method': 'vector'})

        kw_results = []
        query_keywords = [k for k in re.split(r'[ï¼Œã€‚ï¼›ï¼š\s]', query_text) if len(k) > 1]
        for doc in self.documents:
            overlap = sum(1 for k in query_keywords if k in doc['keywords']) 
            if overlap > 0:
                score = overlap / (len(query_keywords) + 1) * 0.8 
                kw_results.append({'doc': doc, 'score': score, 'method': 'keyword'})
        
        kw_results.sort(key=lambda x: x['score'], reverse=True)
        kw_results = kw_results[:top_k]

        combined = vec_results + kw_results
        seen_ids = set()
        final_results = []
        combined.sort(key=lambda x: x['score'], reverse=True)
        for res in combined:
            did = res['doc']['id']
            if did not in seen_ids:
                final_results.append({'source': res['doc']['source'], 'content': res['doc']['text'], 'score': res['score']})
                seen_ids.add(did)
            if len(final_results) >= top_k: break
        return final_results

# ====================
# Helper Functions
# ====================

def process_uploaded_files(uploaded_files):
    for uploaded_file in uploaded_files:
        if uploaded_file.name.endswith('.zip'):
            try:
                with zipfile.ZipFile(uploaded_file) as z:
                    for filename in z.namelist():
                        if filename.endswith('/') or filename.startswith('__MACOSX') or filename.startswith('._'): continue
                        if filename.endswith(('.docx', '.xlsx')):
                            with z.open(filename) as f: yield filename, f.read()
            except Exception as e: st.error(f"è§£å‹å¤±è´¥: {e}")
        else: yield uploaded_file.name, uploaded_file.getvalue()

def extract_text_from_content(filename, content):
    text = ""
    try:
        file_stream = io.BytesIO(content)
        if filename.endswith('.docx'):
            doc = docx.Document(file_stream)
            text = '\n'.join([para.text for para in doc.paragraphs])
        elif filename.endswith('.xlsx'):
            df_dict = pd.read_excel(file_stream, sheet_name=None, header=None)
            text_parts = []
            for sheet_name, df in df_dict.items():
                sheet_text = df.astype(str).apply(lambda x: ' '.join(x), axis=1)
                text_parts.append('\n'.join(sheet_text))
            text = '\n'.join(text_parts)
    except Exception as e: print(f"Error parsing {filename}: {e}")
    return text

def parse_regulation_clauses(text):
    pattern = r'(ç¬¬\s*[\dé›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+\s*æ¡|Article\s+\d+)'
    parts = re.split(pattern, text)
    clauses = []
    if len(parts) > 1:
        for i in range(1, len(parts), 2):
            title = parts[i].strip()
            content = parts[i+1].strip() if i+1 < len(parts) else ""
            clauses.append({"æ¡æ¬¾å·": title, "æ³•è§„æ­£æ–‡": title + " " + content, "é€‚ç”¨æ€§": "é€‚ç”¨"})
    return clauses

def evaluate_single_clause(clause, vector_store, llm_client):
    row = {"æ¡æ¬¾å·": clause['æ¡æ¬¾å·'], "æ³•è§„æ­£æ–‡": clause['æ³•è§„æ­£æ–‡'], "è¯„ä»·ç»“è®º": "âŒç¼ºå¤±/ä¸ç¬¦åˆ", "å·®è·åˆ†æ": "æœªæ£€ç´¢åˆ°ç›¸å…³åˆ¶åº¦", "æ”¹è¿›å»ºè®®": "è¯·è¡¥å……ç›¸å…³ç®¡ç†è§„å®š", "æ”¯æ’‘è¯æ®": "æ— ", "åŒ¹é…åº¦": 0.0}

    search_results = vector_store.search(clause['æ³•è§„æ­£æ–‡'], top_k=3)
    top_score = search_results[0]['score'] if search_results else 0
    row['åŒ¹é…åº¦'] = top_score
        
    evidence_text = ""
    if search_results:
        for i, res in enumerate(search_results):
            evidence_text += f"å‚è€ƒåˆ¶åº¦ç‰‡æ®µ {i+1} (æ¥æº: {res['source']}):\n{res['content'][:800]}\n---\n"
    else:
        evidence_text = "æœªæ£€ç´¢åˆ°ä»»ä½•ç›¸å…³çš„ä¼ä¸šå†…éƒ¨åˆ¶åº¦æ–‡æ¡£ã€‚"
    
    system_prompt = """ä½ æ˜¯ä¸€åå…·æœ‰20å¹´ç»éªŒçš„EHSç®¡ç†ä¸“å®¶ï¼Œç²¾é€šä¸­å›½EHSæ³•è§„æ ‡å‡†ï¼Œæ“…é•¿ä»“å‚¨ç‰©æµåœºæ™¯ã€‚
    è¯·å¯¹ç»™å®šçš„æ³•è§„æ¡æ¬¾è¿›è¡Œåˆè§„æ€§è¯„ä»·ã€‚ä¸¥æ ¼æ‰§è¡Œä»¥ä¸‹æ€ç»´é“¾ï¼š
    1. è§£è¯»ï¼šç†è§£æ¡æ¬¾æ ¸å¿ƒè¦æ±‚ï¼ˆäººæœºæ–™æ³•ç¯ï¼‰ï¼Œåˆ¤å®šæ˜¯å¦é€‚ç”¨äºç‰©æµä»“å‚¨ä¼ä¸šã€‚å¦‚æœä¸é€‚ç”¨ï¼Œç›´æ¥æ ‡è®°â€œä¸é€‚ç”¨â€ã€‚
    2. æ¯”å¯¹ï¼šå¯¹æ¯”æ³•è§„è¦æ±‚ä¸æä¾›çš„ä¼ä¸šåˆ¶åº¦ç‰‡æ®µã€‚æ˜¯å¦è¦†ç›–æ‰€æœ‰è¦ç´ ï¼Ÿé’ˆå¯¹ç‰©æµåœºæ™¯æ˜¯å¦å…·ä½“å¯æ‰§è¡Œï¼Ÿ
    3. åˆ¤å®šï¼šç»™å‡ºå®šæ€§ç»“è®ºã€‚"""
    
    user_prompt = f"""
    ã€æ³•è§„æ¡æ¬¾ã€‘
    {clause['æ³•è§„æ­£æ–‡']}

    ã€ä¼ä¸šåˆ¶åº¦ç°çŠ¶ï¼ˆæ£€ç´¢åˆ°çš„æœ€ç›¸å…³ç‰‡æ®µï¼‰ã€‘
    {evidence_text}

    ã€ä»»åŠ¡è¦æ±‚ã€‘
    è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¿”å› JSON ç»“æœï¼š
    {{
        "applicability": "é€‚ç”¨" æˆ– "ä¸é€‚ç”¨",
        "compliance_status": "å®Œå…¨ç¬¦åˆ" æˆ– "éƒ¨åˆ†ç¬¦åˆ/éœ€å®Œå–„" æˆ– "ç¼ºå¤±/ä¸ç¬¦åˆ" æˆ– "ä¸é€‚ç”¨",
        "gap_analysis": "150å­—ä»¥å†…ã€‚è‹¥ä¸é€‚ç”¨å¡«'ä¸é€‚ç”¨'ã€‚è‹¥é€‚ç”¨ï¼Œåˆ†æå…·ä½“ç¼ºäº†ä»€ä¹ˆæˆ–è¯´æ˜åˆè§„ç‚¹ã€‚",
        "improvement_suggestion": "è‹¥å®Œå…¨ç¬¦åˆå¡«'æ— 'ã€‚å¦åˆ™ç»™å‡º1-2æ¡å»ºè®®ã€‚",
        "evidence_summary": "åˆ—å‡ºæœ€åŒ¹é…çš„åˆ¶åº¦åç§°åŠå…³é”®å¥æ‘˜è¦"
    }}
    """
    try:
        result = llm_client.chat_completion(system_prompt, user_prompt)
        status = result.get('compliance_status', 'ç¼ºå¤±/ä¸ç¬¦åˆ')
        if "å®Œå…¨ç¬¦åˆ" in status: status = "âœ…å®Œå…¨ç¬¦åˆ"
        elif "éƒ¨åˆ†" in status or "éœ€å®Œå–„" in status: status = "âš ï¸éƒ¨åˆ†ç¬¦åˆ/éœ€å®Œå–„"
        elif "ä¸é€‚ç”¨" in status: status = "â—ä¸é€‚ç”¨"
        else: status = "âŒç¼ºå¤±/ä¸ç¬¦åˆ"
        
        row['è¯„ä»·ç»“è®º'] = status
        row['å·®è·åˆ†æ'] = result.get('gap_analysis', 'æ— åˆ†æ')
        row['æ”¹è¿›å»ºè®®'] = result.get('improvement_suggestion', 'æ— å»ºè®®')
        row['æ”¯æ’‘è¯æ®'] = result.get('evidence_summary', 'æ— ')
    except Exception as e:
        row['å·®è·åˆ†æ'] = f"LLMåˆ†æå¤±è´¥: {str(e)}"
    return row

def generate_word_report(df_results, summary_stats):
    doc = Document()
    title = doc.add_heading('EHSæ³•è§„åˆè§„æ€§è¯„ä»·æŠ¥å‘Š', 0)
    title.alignment = WD_ALIGN_PARAGRAPH.CENTER
    doc.add_paragraph(f"è¯„ä»·æ—¥æœŸ: {time.strftime('%Y-%m-%d')}")
    
    # 1. æ€»ä½“è¯„ä»· - ç³»ç»Ÿæ€§æ±‡æ€»
    doc.add_heading('ç¬¬ä¸€éƒ¨åˆ†ï¼šæ€»ä½“è¯„ä»·ä¸ç®¡ç†å»ºè®®', level=1)
    
    # 1.1 æ•°æ®æ¦‚è§ˆ
    doc.add_heading('1.1 è¯„ä»·æ•°æ®æ¦‚è§ˆ', level=2)
    p = doc.add_paragraph()
    p.add_run(f"æœ¬æ¬¡è¯„ä»·å…±åˆ†ææ³•è§„æ¡æ¬¾ {summary_stats['total']} æ¡ï¼Œå…¶ä¸­ï¼š\n")
    p.add_run(f"âœ… å®Œå…¨ç¬¦åˆ: {summary_stats['compliant']} æ¡ ({summary_stats['compliant']/summary_stats['total']*100:.1f}%)\n").font.color.rgb = RGBColor(0, 128, 0)
    p.add_run(f"âš ï¸ éƒ¨åˆ†ç¬¦åˆ/éœ€å®Œå–„: {summary_stats['partial']} æ¡ ({summary_stats['partial']/summary_stats['total']*100:.1f}%)\n").font.color.rgb = RGBColor(255, 165, 0)
    p.add_run(f"âŒ ç¼ºå¤±/ä¸ç¬¦åˆ: {summary_stats['non_compliant']} æ¡ ({summary_stats['non_compliant']/summary_stats['total']*100:.1f}%)\n").font.color.rgb = RGBColor(255, 0, 0)
    
    # 1.2 å…³é”®é£é™©é¢†åŸŸ
    doc.add_heading('1.2 å…³é”®é£é™©é¢†åŸŸè¯†åˆ«', level=2)
    risk_df = df_results[df_results['è¯„ä»·ç»“è®º'].str.contains("ç¼ºå¤±|ä¸ç¬¦åˆ|éƒ¨åˆ†")]
    if not risk_df.empty:
        doc.add_paragraph("ä»¥ä¸‹æ¡æ¬¾å­˜åœ¨åˆè§„é£é™©ï¼Œéœ€é‡ç‚¹å…³æ³¨ï¼š")
        risk_table = doc.add_table(rows=1, cols=3)
        risk_table.style = 'Table Grid'
        headers = ["æ¡æ¬¾å·", "é£é™©æè¿° (å·®è·åˆ†æ)", "æ”¹è¿›å»ºè®®"]
        for i, h in enumerate(headers):
            cell = risk_table.rows[0].cells[i]
            cell.text = h
            cell.paragraphs[0].runs[0].font.bold = True
        
        for idx, row in risk_df.head(10).iterrows(): # ä»…åˆ—å‡ºå‰10æ¡é¿å…è¿‡é•¿
            r = risk_table.add_row().cells
            r[0].text = row['æ¡æ¬¾å·']
            r[1].text = row['å·®è·åˆ†æ']
            r[2].text = row['æ”¹è¿›å»ºè®®']
        if len(risk_df) > 10:
            doc.add_paragraph(f"...(å¦æœ‰ {len(risk_df)-10} æ¡é£é™©æ¡æ¬¾ï¼Œè¯¦è§é™„è¡¨)")
    else:
        doc.add_paragraph("æœ¬æ¬¡è¯„ä»·æœªå‘ç°é‡å¤§åˆè§„é£é™©ï¼Œåˆ¶åº¦ä½“ç³»æ•´ä½“è¿è¡Œè‰¯å¥½ã€‚")

    # 1.3 ä¸“å®¶ç»“è®º
    doc.add_heading('1.3 ä¸“å®¶ç»¼åˆç»“è®º', level=2)
    if summary_stats['non_compliant'] > 5:
        conclusion = "ç»“è®ºï¼šä¼ä¸šç°è¡Œåˆ¶åº¦åœ¨æ ¸å¿ƒè¦ç´ ä¸Šå­˜åœ¨æ˜æ˜¾ç¼ºå¤±ï¼Œåˆè§„é£é™©è¾ƒé«˜ã€‚å»ºè®®ç«‹å³å¯åŠ¨ä¸“é¡¹æ•´æ”¹ï¼Œä¼˜å…ˆå®Œå–„ä¸Šè¿°è¯†åˆ«å‡ºçš„é£é™©é¢†åŸŸï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹ç‰©æµç°åœºä½œä¸šçš„ç®¡æ§è§„å®šã€‚"
    elif summary_stats['partial'] > 5:
        conclusion = "ç»“è®ºï¼šä¼ä¸šåˆ¶åº¦æ¡†æ¶åŸºæœ¬å¥å…¨ï¼Œä½†åœ¨æ‰§è¡Œç»†èŠ‚å’Œå…·ä½“è½åœ°æªæ–½ä¸Šä»æœ‰ä¼˜åŒ–ç©ºé—´ã€‚å»ºè®®é’ˆå¯¹â€œéœ€å®Œå–„â€æ¡æ¬¾è¿›è¡Œä¿®è®¢ï¼Œç»†åŒ–ç®¡ç†æµç¨‹å’Œè´£ä»»äººã€‚"
    else:
        conclusion = "ç»“è®ºï¼šä¼ä¸šEHSç®¡ç†åˆ¶åº¦ä½“ç³»å¥å…¨ï¼Œä¸æ³•è§„è¦æ±‚åŒ¹é…åº¦é«˜ã€‚å»ºè®®å®šæœŸå›é¡¾æ›´æ–°ï¼Œä¿æŒæŒç»­åˆè§„ã€‚"
    doc.add_paragraph(conclusion)

    # 2. è¯¦ç»†çŸ©é˜µ
    doc.add_heading('ç¬¬äºŒéƒ¨åˆ†ï¼šè¯¦ç»†åˆè§„æ€§è¯„ä»·çŸ©é˜µ', level=1)
    table = doc.add_table(rows=1, cols=6)
    table.style = 'Table Grid'
    headers = ["åºå·", "æ³•è§„æ¡æ¬¾", "è¯„ä»·ç»“è®º", "å·®è·åˆ†æä¸è®ºæ®", "æ”¹è¿›å»ºè®®", "æ”¯æ’‘è¯æ®"]
    for i, header in enumerate(headers):
        table.rows[0].cells[i].text = header
        
    for idx, row in df_results.iterrows():
        row_cells = table.add_row().cells
        row_cells[0].text = str(row.get('åºå·', idx + 1))
        row_cells[1].text = str(row.get('æ³•è§„æ­£æ–‡', ''))
        row_cells[2].text = str(row.get('è¯„ä»·ç»“è®º', ''))
        row_cells[3].text = str(row.get('å·®è·åˆ†æ', ''))
        row_cells[4].text = str(row.get('æ”¹è¿›å»ºè®®', ''))
        row_cells[5].text = str(row.get('æ”¯æ’‘è¯æ®', ''))
        
    f = io.BytesIO()
    doc.save(f)
    f.seek(0)
    return f

# ====================
# Streamlit UI
# ====================

st.set_page_config(page_title="EHSä¸“å®¶åˆè§„ç³»ç»Ÿ (Enterprise)", layout="wide")

if 'results' not in st.session_state: st.session_state.results = None
if 'vector_store' not in st.session_state: st.session_state.vector_store = VectorStore()
if 'current_webdav_path' not in st.session_state: st.session_state.current_webdav_path = "/"

st.title("ğŸ›¡ï¸ EHSæ³•è§„åˆè§„æ€§æ™ºèƒ½è¯„ä»·ç³»ç»Ÿ (Enterprise)")

with st.sidebar:
    st.header("1. API é…ç½®")
    llm_base_url = st.text_input("API Base URL", value="https://generativelanguage.googleapis.com/v1beta/openai")
    llm_api_key = st.text_input("API Key", type="password")
    col_m1, col_m2 = st.columns(2)
    with col_m1: llm_model_name = st.text_input("Chat Model", value="gemini-2.0-flash")
    with col_m2: embedding_model_name = st.text_input("Embedding Model", value="text-embedding-004")
    
    if st.button("ğŸ”Œ æµ‹è¯•è¿é€šæ€§", use_container_width=True):
        if not llm_api_key: st.error("è¯·å…ˆè¾“å…¥ API Key")
        else:
            with st.spinner("æ­£åœ¨æµ‹è¯• API è¿æ¥..."):
                cfg = {"base_url": llm_base_url, "api_key": llm_api_key, "model": llm_model_name, "embedding_model": embedding_model_name}
                client = LLMClient(cfg)
                res = client.test_connection()
                if res['chat']: st.success(f"âœ… Chat ({llm_model_name}): é€šç•…")
                else: st.error(f"âŒ Chat å¤±è´¥: {res['msg']}")
                if res['embedding']: st.success(f"âœ… Embedding ({embedding_model_name}): é€šç•…")
                else: st.error(f"âŒ Embedding å¤±è´¥: {res['msg']}")

    if llm_api_key:
        llm_config = {"base_url": llm_base_url, "api_key": llm_api_key, "model": llm_model_name, "embedding_model": embedding_model_name}
        client = LLMClient(llm_config)
        st.session_state.vector_store.set_client(client)
    
    st.divider()
    st.header("ğŸ’¾ çŸ¥è¯†åº“ç®¡ç†")
    db_name = st.text_input("åº“åç§°", value="ehs_master_index")
    col_db1, col_db2 = st.columns(2)
    with col_db1:
        if st.button("ğŸ’¾ ä¿å­˜å…¨åº“"):
            path = st.session_state.vector_store.save_to_disk(db_name)
            st.success(f"å·²ä¿å­˜: {path}")
    with col_db2:
        if st.button("ğŸ“‚ åŠ è½½å…¨åº“"):
            if st.session_state.vector_store.load_from_disk(db_name):
                st.success(f"å·²åŠ è½½!")
            else: st.error("æ–‡ä»¶ä¸å­˜åœ¨")

st.info(f"ğŸ“š å½“å‰çŸ¥è¯†åº“: åˆ¶åº¦ç‰‡æ®µ {len(st.session_state.vector_store.documents)} ä¸ª | å·²å­˜æ³•è§„ {len(st.session_state.vector_store.regulations)} ä¸ª")

tab1, tab2, tab3 = st.tabs(["ğŸ“‚ æœ¬åœ°ä¸Šä¼ ", "â˜ï¸ WebDAV è¿œç¨‹åº“", "ğŸš€ å¼€å§‹è¯„ä¼°"])

with tab1:
    col_u1, col_u2 = st.columns(2)
    with col_u1:
        st.subheader("ä¸Šä¼ åˆ¶åº¦ (ä¾æ®)")
        policy_files_local = st.file_uploader("åˆ¶åº¦æ–‡ä»¶", type=['docx', 'xlsx', 'zip'], accept_multiple_files=True, key="pol_local", label_visibility="collapsed")
        if st.button("ğŸ“¥ åˆ¶åº¦å…¥åº“ (å‘é‡åŒ–)"):
            if policy_files_local:
                corpus = []
                for name, content in process_uploaded_files(policy_files_local):
                    text = extract_text_from_content(name, content)
                    if text: corpus.append({'name': name, 'content': text})
                st.session_state.vector_store.add_documents(corpus)
    
    with col_u2:
        st.subheader("ä¸Šä¼ æ³•è§„ (æ ‡å‡†)")
        reg_files_local = st.file_uploader("æ³•è§„æ–‡ä»¶", type=['docx', 'zip'], accept_multiple_files=True, key="reg_local", label_visibility="collapsed")
        if st.button("ğŸ“¥ æ³•è§„å…¥åº“ (è§£æä¿å­˜)"):
            if reg_files_local:
                corpus = []
                for name, content in process_uploaded_files(reg_files_local):
                    text = extract_text_from_content(name, content)
                    if text: corpus.append({'name': name, 'content': text})
                count = st.session_state.vector_store.add_regulations(corpus)
                st.success(f"æ–°å¢ {count} ä¸ªæ³•è§„æ–‡æ¡£")

with tab2:
    st.markdown("### â˜ï¸ WebDAV æ–‡ä»¶æµè§ˆå™¨")
    col_w1, col_w2, col_w3 = st.columns([2, 1, 1])
    webdav_url = col_w1.text_input("URL", help="https://dav.example.com/")
    webdav_user = col_w2.text_input("User")
    webdav_pass = col_w3.text_input("Pass", type="password")
    
    if st.button("ğŸ”— è¿æ¥/åˆ·æ–°ç›®å½•"):
        try:
            wd_client = SimpleWebDavClient(webdav_url, webdav_user, webdav_pass)
            items = wd_client.list(st.session_state.current_webdav_path)
            st.session_state.webdav_items = items
            st.session_state.wd_client = wd_client
        except Exception as e: st.error(f"è¿æ¥å¤±è´¥: {e}")

    # å¯¼èˆªæ 
    if 'webdav_items' in st.session_state:
        st.markdown(f"**å½“å‰è·¯å¾„**: `{st.session_state.current_webdav_path}`")
        if st.session_state.current_webdav_path != "/":
            if st.button("â¬†ï¸ è¿”å›ä¸Šä¸€çº§"):
                parent = os.path.dirname(st.session_state.current_webdav_path.rstrip('/'))
                st.session_state.current_webdav_path = parent if parent else "/"
                st.rerun()

        # æ–‡ä»¶å¤¹åˆ—è¡¨
        folders = [i for i in st.session_state.webdav_items if i['is_folder']]
        files = [i for i in st.session_state.webdav_items if not i['is_folder'] and i['name'].endswith(('.docx', '.xlsx', '.zip'))]
        
        if folders:
            st.markdown("#### ğŸ“ æ–‡ä»¶å¤¹")
            cols = st.columns(4)
            for i, f in enumerate(folders):
                if cols[i % 4].button(f"ğŸ“‚ {f['name']}", key=f['path']):
                    # è¿™é‡Œ path å¯èƒ½æ˜¯å®Œæ•´ URL æˆ–è€…æ˜¯ç›¸å¯¹è·¯å¾„ï¼Œå–å†³äºæœåŠ¡å™¨è¿”å›
                    # æˆ‘ä»¬éœ€è¦æå–ç›¸å¯¹è·¯å¾„
                    # ç®€å•ç‚¹ï¼šç›´æ¥ç”¨ name æ‹¼æ¥åˆ° current_path
                    new_path = f"{st.session_state.current_webdav_path.rstrip('/')}/{f['name']}"
                    st.session_state.current_webdav_path = new_path
                    st.rerun()

        # æ–‡ä»¶åˆ—è¡¨
        st.markdown("#### ğŸ“„ æ–‡ä»¶")
        selected_wd_files = st.multiselect("é€‰æ‹©æ–‡ä»¶", [f['name'] for f in files])
        action = st.radio("æ“ä½œ:", ["åˆ¶åº¦å…¥åº“ (å‘é‡åŒ–)", "æ³•è§„å…¥åº“ (ä¿å­˜)"])
        
        if st.button("â¬‡ï¸ ä¸‹è½½å¹¶å¤„ç†"):
            corpus = []
            for fname in selected_wd_files:
                try:
                    full_p = f"{st.session_state.current_webdav_path.rstrip('/')}/{fname}"
                    content = st.session_state.wd_client.download(full_p)
                    text = extract_text_from_content(fname, content)
                    if text: corpus.append({'name': fname, 'content': text})
                except Exception as e: st.error(f"Error {fname}: {e}")
            
            if action.startswith("åˆ¶åº¦"):
                st.session_state.vector_store.add_documents(corpus)
                st.success("WebDAV åˆ¶åº¦å·²å…¥åº“")
            else:
                c = st.session_state.vector_store.add_regulations(corpus)
                st.success(f"WebDAV æ³•è§„å·²ä¿å­˜ ({c}ä¸ª)")

with tab3:
    st.subheader("æ‰§è¡Œåˆè§„æ€§åˆ†æ")
    
    # ä»å·²ä¿å­˜çš„æ³•è§„ä¸­é€‰æ‹©
    saved_regs = [r['name'] for r in st.session_state.vector_store.regulations]
    
    if not saved_regs:
        st.warning("æ³•è§„åº“ä¸ºç©ºï¼Œè¯·å…ˆåœ¨ Tab 1 æˆ– Tab 2 ä¸Šä¼ /ä¿å­˜æ³•è§„æ–‡ä»¶ã€‚ ×•×œ××—×¨ ××›×Ÿ ×œ×—×¥ ×¢×œ ×›×¤×ª×•×¨ '×”×•×¡×£ ×§×‘×¦×™×' ×›×“×™ ×œ×”×•×¡×™×£ ××•×ª× ×œ×××’×¨ ×”× ×ª×•× ×™×.")
    else:
        selected_reg_names = st.multiselect("é€‰æ‹©è¦åˆ†æçš„æ³•è§„", saved_regs, default=saved_regs[0] if saved_regs else None)
        
        if st.button("ğŸš€ å¼€å§‹ä¸“å®¶çº§è¯„ä¼°", type="primary"):
            if not selected_reg_names:
                st.error("è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªæ³•è§„æ–‡ä»¶")
                st.stop()
                
            # è·å–é€‰ä¸­çš„æ³•è§„å†…å®¹
            target_regs = [r for r in st.session_state.vector_store.regulations if r['name'] in selected_reg_names]
            
            all_clauses = []
            for doc in target_regs:
                clauses = parse_regulation_clauses(doc['content'])
                for i, c in enumerate(clauses):
                    c['source_file'] = doc['name']
                    c['åºå·'] = i + 1
                    all_clauses.append(c)
            
            st.info(f"åˆ†æä¸­... å…± {len(all_clauses)} æ¡æ¬¾")
            
            results_list = []
            progress_bar = st.progress(0)
            completed = 0
            
            with ThreadPoolExecutor(max_workers=10) as executor:
                future_to_clause = {executor.submit(evaluate_single_clause, clause, st.session_state.vector_store, client): clause for clause in all_clauses}
                for future in as_completed(future_to_clause):
                    res = future.result()
                    res['æ³•è§„æ–‡ä»¶'] = future_to_clause[future]['source_file']
                    res['åºå·'] = future_to_clause[future]['åºå·']
                    results_list.append(res)
                    completed += 1
                    progress_bar.progress(completed / len(all_clauses))
            
            st.success("å®Œæˆ!")
            results_list.sort(key=lambda x: x['åºå·'])
            st.session_state.results = pd.DataFrame(results_list)

    if st.session_state.results is not None:
        df = st.session_state.results
        summary_stats = {
            "total": len(df),
            "compliant": len(df[df['è¯„ä»·ç»“è®º'].str.contains("å®Œå…¨ç¬¦åˆ")]),
            "partial": len(df[df['è¯„ä»·ç»“è®º'].str.contains("éƒ¨åˆ†")]),
            "non_compliant": len(df[df['è¯„ä»·ç»“è®º'].str.contains("ç¼ºå¤±|ä¸ç¬¦åˆ")])
        }
        st.dataframe(df)
        word_file = generate_word_report(df, summary_stats)
        st.download_button("ğŸ“¥ ä¸‹è½½ç³»ç»Ÿæ€§è¯„ä»·æŠ¥å‘Š", word_file, "EHS_System_Report.docx", "application/vnd.openxmlformats-officedocument.wordprocessingml.document")