import streamlit as st
import pandas as pd
import os
import re
import zipfile
import io
import time
import requests
import json

# ====================
# æ ¸å¿ƒé€»è¾‘å‡½æ•° (å¤ç”¨ä¸“å®¶ç»éªŒ)
# ====================

def process_uploaded_files(uploaded_files):
    """
    ç”Ÿæˆå™¨ï¼šå¤„ç†ä¸Šä¼ çš„æ–‡ä»¶åˆ—è¡¨ï¼Œè‡ªåŠ¨è§£å‹zipåŒ…
    Yields: (filename, content_bytes)
    """
    for uploaded_file in uploaded_files:
        if uploaded_file.name.endswith('.zip'):
            try:
                with zipfile.ZipFile(uploaded_file) as z:
                    for filename in z.namelist():
                        # è·³è¿‡æ–‡ä»¶å¤¹å’Œéšè—æ–‡ä»¶
                        if filename.endswith('/') or filename.startswith('__MACOSX') or filename.startswith('._'):
                            continue
                        if filename.endswith(('.docx', '.xlsx')):
                            with z.open(filename) as f:
                                yield filename, f.read()
            except Exception as e:
                st.error(f"è§£å‹æ–‡ä»¶ {uploaded_file.name} å¤±è´¥: {str(e)}")
        else:
            yield uploaded_file.name, uploaded_file.getvalue()

def extract_text_from_content(filename, content):
    """ä»æ–‡ä»¶å†…å®¹ä¸­æå–çº¯æ–‡æœ¬"""
    text = ""
    try:
        if filename.endswith('.docx'):
            with zipfile.ZipFile(io.BytesIO(content)) as zf:
                xml = zf.read('word/document.xml').decode('utf-8')
                text = re.sub(r'<[^>]+>', '', xml)
        elif filename.endswith('.xlsx'):
            with zipfile.ZipFile(io.BytesIO(content)) as zf:
                if 'xl/sharedStrings.xml' in zf.namelist():
                    xml = zf.read('xl/sharedStrings.xml').decode('utf-8')
                    text = re.sub(r'<[^>]+>', '', xml)
    except Exception as e:
        st.error(f"è§£ææ–‡ä»¶ {filename} å¤±è´¥: {str(e)}")
    return text

def parse_regulation_clauses(text):
    """å°†æ³•è§„æ–‡æœ¬æ‹†è§£ä¸ºæ¡æ¬¾åˆ—è¡¨"""
    # åŒ¹é… "ç¬¬Xæ¡" çš„æ¨¡å¼ï¼Œæ”¯æŒä¸­æ–‡æ•°å­—
    pattern = r'(ç¬¬[é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+æ¡)'
    parts = re.split(pattern, text)
    
    clauses = []
    if len(parts) > 1:
        # parts[0] æ˜¯å‰è¨€ï¼Œparts[1]æ˜¯"ç¬¬ä¸€æ¡", parts[2]æ˜¯å†…å®¹...
        for i in range(1, len(parts), 2):
            title = parts[i]
            content = parts[i+1].strip() if i+1 < len(parts) else ""
            
            # ç®€å•çš„é€‚ç”¨æ€§åˆ¤æ–­é€»è¾‘ï¼ˆæ’é™¤çº¯æ”¿åºœèŒè´£ï¼‰
            applicability = "é€‚ç”¨"
            gov_keywords = ["å›½åŠ¡é™¢", "å¿çº§ä»¥ä¸Š", "ç›‘å¯Ÿæœºå…³", "äººæ°‘æ”¿åºœ", "ä¸»ç®¡éƒ¨é—¨"]
            # å¦‚æœä¸»è¦æ˜¯åœ¨è®²æ”¿åºœåº”è¯¥åšä»€ä¹ˆï¼Œä¸”æ²¡æœ‰æåŠâ€œç”Ÿäº§ç»è¥å•ä½â€
            if any(k in content[:20] for k in gov_keywords) and "ç”Ÿäº§ç»è¥å•ä½" not in content[:50]:
                applicability = "ä¸é€‚ç”¨(æ”¿åºœèŒè´£)"
            
            full_text = title + " " + content
            clauses.append({
                "æ¡æ¬¾å·": title,
                "æ³•è§„æ­£æ–‡": full_text,
                "é€‚ç”¨æ€§": applicability
            })
    return clauses

def calculate_match_score(clause_text, policy_text):
    """è®¡ç®—åŒ¹é…åº¦å¾—åˆ† (åŸºäºç®€å•çš„å…³é”®è¯é‡å )"""
    # ç®€å•çš„åˆ†è¯ï¼šæŒ‰æ ‡ç‚¹ç¬¦å·åˆ†å‰²
    keywords = re.split(r'[ï¼Œã€‚ï¼›ï¼šã€â€œâ€]', clause_text)
    keywords = [k for k in keywords if len(k) > 2] # ä»…ä¿ç•™æœ‰æ„ä¹‰çš„è¯
    
    score = 0
    matched_words = []
    
    for k in keywords:
        if k in policy_text:
            score += len(k)
            matched_words.append(k)
            
    return score, list(set(matched_words))

def check_llm_compliance(clause, policy_candidates, api_config):
    """
    ä½¿ç”¨å¤§æ¨¡å‹è¿›è¡Œåˆè§„æ€§åˆ¤å®š
    clause: {æ¡æ¬¾å·, æ³•è§„æ­£æ–‡}
    policy_candidates: [{name, content, score}, ...] (Top N candidates)
    api_config: {base_url, api_key, model}
    """
    if not api_config.get('api_key'):
        return None

    # æ„é€  Prompt
    candidates_text = ""
    for i, p in enumerate(policy_candidates):
        # æˆªå–ç›¸å…³æ€§æœ€é«˜çš„ç‰‡æ®µ (ç®€å•å¤„ç†ï¼šå–å‰1000å­—ç¬¦æˆ–å…³é”®è¯é™„è¿‘ï¼Œè¿™é‡Œæš‚å–å‰1500å­—ç¬¦ä»¥èŠ‚çœtoken)
        # å®é™…ç”Ÿäº§ä¸­åº”ä½¿ç”¨å‘é‡æ£€ç´¢é…åˆRAGï¼Œè¿™é‡ŒåŸºäºå…³é”®è¯åŒ¹é…ç»“æœåšç®€å•ä¸Šä¸‹æ–‡å¡«å……
        content_snippet = p['content'][:2000] + "..." 
        candidates_text += f"Document {i+1} [{p['name']}]:\n{content_snippet}\n\n"

    system_prompt = "ä½ æ˜¯ä¸€ä½èµ„æ·±çš„EHSåˆè§„æ€§å®¡è®¡ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®æä¾›çš„ä¼ä¸šå†…éƒ¨åˆ¶åº¦æ–‡æ¡£ï¼Œåˆ¤æ–­å…¶æ˜¯å¦ç¬¦åˆç»™å®šçš„æ³•è§„æ¡æ¬¾è¦æ±‚ã€‚"
    user_prompt = f"""
è¯·åˆ†æä»¥ä¸‹æ³•è§„æ¡æ¬¾ä¸ä¼ä¸šåˆ¶åº¦çš„ç¬¦åˆæƒ…å†µï¼š

ã€æ³•è§„æ¡æ¬¾ã€‘
{clause['æ³•è§„æ­£æ–‡']}

ã€ä¼ä¸šå†…éƒ¨åˆ¶åº¦å‚è€ƒã€‘
{candidates_text}

ã€ä»»åŠ¡è¦æ±‚ã€‘
1. åˆ¤æ–­ä¼ä¸šåˆ¶åº¦æ˜¯å¦è¦†ç›–å¹¶ç¬¦åˆè¯¥æ¡æ¬¾è¦æ±‚ã€‚
2. ç»™å‡ºè¯„ä»·ç»“è®ºï¼Œå¿…é¡»ä»ä»¥ä¸‹é€‰é¡¹ä¸­é€‰æ‹©ä¸€ä¸ªï¼š "âœ…å®Œå…¨ç¬¦åˆ", "âš ï¸éƒ¨åˆ†ç¬¦åˆ/éœ€å®Œå–„", "âŒç¼ºå¤±/ä¸ç¬¦åˆ", "â—ä¸é€‚ç”¨"ã€‚
3. æä¾›æ”¯æ’‘è¯æ®ï¼Œå¼•ç”¨å…·ä½“çš„åˆ¶åº¦åç§°å’Œå…³é”®å†…å®¹ã€‚
4. å¦‚æœæ¡æ¬¾ä¸»è¦æ¶‰åŠæ”¿åºœç›‘ç®¡èŒè´£è€Œéä¼ä¸šä¹‰åŠ¡ï¼Œè¯·æ ‡æ³¨ä¸º "â—ä¸é€‚ç”¨"ã€‚

è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
  "compliance_status": "è¯„ä»·ç»“è®º",
  "evidence": "æ”¯æ’‘è¯æ®(ç®€ç»ƒæ¦‚æ‹¬)",
  "reasoning": "åˆ¤å®šç†ç”±"
}}
"""

    headers = {
        "Authorization": f"Bearer {api_config['api_key']}",
        "Content-Type": "application/json"
    }
    
    payload = {
        "model": api_config['model'],
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        "temperature": 0.1,
        "response_format": {"type": "json_object"}
    }

    try:
        response = requests.post(f"{api_config['base_url']}/chat/completions", headers=headers, json=payload, timeout=30)
        if response.status_code == 200:
            result = response.json()
            content = result['choices'][0]['message']['content']
            return json.loads(content)
        else:
            st.warning(f"LLM APIè¯·æ±‚å¤±è´¥: {response.status_code} - {response.text}")
            return None
    except Exception as e:
        st.warning(f"LLMè°ƒç”¨å¼‚å¸¸: {str(e)}")
        return None

def generate_markdown_report(summary_data, df_result):
    """ç”Ÿæˆ Markdown æ ¼å¼çš„æŠ¥å‘Š"""
    report = f"""# EHSæ³•è§„åˆè§„æ€§è¯„ä»·æŠ¥å‘Š

**è¯„ä»·æ—¥æœŸ**: {time.strftime("%Y-%m-%d")}

## ç¬¬ä¸€éƒ¨åˆ†ï¼šæ€»ä½“è¯„ä»·

**1. è¯„ä»·æ¦‚å†µ**
*   **åˆ†ææ¡æ¬¾æ€»æ•°**: {summary_data['total']}
*   **å®Œå…¨ç¬¦åˆæ¡æ¬¾æ•°**: {summary_data['compliant']}
*   **éƒ¨åˆ†ç¬¦åˆ/éœ€å®Œå–„æ¡æ¬¾æ•°**: {summary_data['partial']}
*   **ä¸é€‚ç”¨/ç¼ºå¤±æ¡æ¬¾æ•°**: {summary_data['non_compliant']}
*   **æ€»ä½“åˆè§„ç‡**: {summary_data['compliance_rate']:.1f}% (å®Œå…¨ç¬¦åˆ + éƒ¨åˆ†ç¬¦åˆ)

**2. è¯„ä»·ç»“è®ºç»¼è¿°**
æœ¬æ¬¡è¯„ä»·é’ˆå¯¹ä¸Šä¼ çš„æ³•è§„æ–‡ä»¶ä¸ä¼ä¸šå†…éƒ¨åˆ¶åº¦è¿›è¡Œäº†è‡ªåŠ¨æ¯”å¯¹ã€‚
{ "æ€»ä½“åˆè§„æƒ…å†µè‰¯å¥½ã€‚" if summary_data['compliance_rate'] > 80 else "å­˜åœ¨ä¸€å®šåˆè§„é£é™©ï¼Œå»ºè®®é‡ç‚¹å…³æ³¨ç¼ºå¤±å’Œéƒ¨åˆ†ç¬¦åˆçš„æ¡æ¬¾ã€‚" }

---

## ç¬¬äºŒéƒ¨åˆ†ï¼šè¯¦ç»†åˆè§„æ€§è¯„ä»·çŸ©é˜µ

| åºå· | æ³•è§„æ–‡ä»¶ | æ¡æ¬¾å· | è¯„ä»·ç»“è®º | æ”¯æ’‘è¯æ® |
| :--- | :--- | :--- | :--- | :--- |
"""
    for _, row in df_result.iterrows():
        # æ¸…ç†æ¢è¡Œç¬¦ä»¥å…ç ´åè¡¨æ ¼æ ¼å¼
        evidence = str(row['æ”¯æ’‘è¯æ®']).replace('\n', '<br>').replace('|', '\|')
        # æˆªæ–­è¿‡é•¿çš„è¯æ®
        if len(evidence) > 100:
            evidence = evidence[:100] + "..."
            
        report += f"| {row['åºå·']} | {row['æ³•è§„æ–‡ä»¶']} | {row['æ¡æ¬¾å·']} | {row['è¯„ä»·ç»“è®º']} | {evidence} |\n"
        
    return report

def analyze_compliance(reg_files, policy_files, progress_bar, status_text, llm_config=None):
    """æ‰§è¡Œåˆè§„æ€§åˆ†æçš„ä¸»æµç¨‹"""
    
    # 1. é¢„å¤„ç†åˆ¶åº¦æ–‡ä»¶åº“
    status_text.text("æ­£åœ¨æ„å»ºåˆ¶åº¦çŸ¥è¯†åº“...")
    policy_corpus = []
    
    # ä½¿ç”¨ process_uploaded_files å¤„ç†æ–‡ä»¶ï¼Œå¯èƒ½åŒ…å«è§£å‹åçš„å¤šä¸ªæ–‡ä»¶
    processed_policies = list(process_uploaded_files(policy_files))
    total_policies = len(processed_policies)
    
    for idx, (p_name, p_content) in enumerate(processed_policies):
        p_text = extract_text_from_content(p_name, p_content)
        if p_text:
            policy_corpus.append({
                "name": p_name,
                "content": p_text
            })
        progress_bar.progress((idx + 1) / total_policies * 0.1) # é¢„å¤„ç†å 10%è¿›åº¦

    all_results = []
    
    # 2. é€ä¸ªåˆ†ææ³•è§„æ–‡ä»¶
    processed_regs = list(process_uploaded_files(reg_files))
    
    for r_name, r_content in processed_regs:
        r_text = extract_text_from_content(r_name, r_content)
        clauses = parse_regulation_clauses(r_text)
        
        total_clauses = len(clauses)
        if total_clauses == 0:
            st.warning(f"æ–‡ä»¶ {r_name} æœªè¯†åˆ«åˆ°æœ‰æ•ˆæ¡æ¬¾ï¼Œè¯·ç¡®è®¤æ ¼å¼ã€‚")
            continue
            
        current_results = []
        
        for idx, clause in enumerate(clauses):
            # æ›´æ–°è¿›åº¦æ¡
            progress = 0.1 + ((idx + 1) / total_clauses * 0.9)
            progress_bar.progress(progress)
            status_text.text(f"æ­£åœ¨åˆ†æ {r_name}: {clause['æ¡æ¬¾å·']}...")
            
            row = {
                "åºå·": idx + 1,
                "æ³•è§„æ–‡ä»¶": r_name,
                "æ¡æ¬¾å·": clause['æ¡æ¬¾å·'],
                "æ³•è§„æ­£æ–‡": clause['æ³•è§„æ­£æ–‡'],
                "è¯„ä»·ç»“è®º": "âŒç¼ºå¤±/ä¸ç¬¦åˆ", # é»˜è®¤
                "æ”¯æ’‘è¯æ®": "æœªæ£€ç´¢åˆ°ç›¸å…³åˆ¶åº¦",
                "åŒ¹é…åº¦": 0
            }
            
            # ç¬¬ä¸€æ­¥ï¼šå…³é”®è¯åˆç­› (æ‰¾åˆ°Top 3å€™é€‰åˆ¶åº¦)
            candidates = []
            for policy in policy_corpus:
                score, keywords = calculate_match_score(clause['æ³•è§„æ­£æ–‡'], policy['content'])
                if score > 0:
                    candidates.append({
                        "name": policy['name'],
                        "content": policy['content'],
                        "score": score,
                        "keywords": keywords
                    })
            
            # æŒ‰åˆ†æ•°æ’åºå–å‰3
            candidates.sort(key=lambda x: x['score'], reverse=True)
            top_candidates = candidates[:3]
            
            # ç¬¬äºŒæ­¥ï¼šåˆ¤å®šé€»è¾‘ (LLM vs è§„åˆ™)
            llm_result = None
            if llm_config and llm_config.get('api_key') and clause['é€‚ç”¨æ€§'] == "é€‚ç”¨":
                # ä½¿ç”¨ LLM è¿›è¡Œç²¾å‡†åˆ¤å®š
                status_text.text(f"æ­£åœ¨åˆ†æ {r_name}: {clause['æ¡æ¬¾å·']} (AIæ€è€ƒä¸­...)")
                llm_result = check_llm_compliance(clause, top_candidates, llm_config)
            
            if llm_result:
                # é‡‡çº³ LLM ç»“æœ
                row['è¯„ä»·ç»“è®º'] = llm_result.get('compliance_status', "âŒç¼ºå¤±/ä¸ç¬¦åˆ")
                row['æ”¯æ’‘è¯æ®'] = llm_result.get('evidence', "") + f"\n(AIç†ç”±: {llm_result.get('reasoning', '')})"
            else:
                # é™çº§å›é€€åˆ° è§„åˆ™åˆ¤å®š
                if clause['é€‚ç”¨æ€§'] != "é€‚ç”¨":
                    row['è¯„ä»·ç»“è®º'] = "â—ä¸é€‚ç”¨"
                    row['æ”¯æ’‘è¯æ®'] = "æ¡æ¬¾ä¸»ä½“éä¼ä¸š"
                elif top_candidates:
                    best_match = top_candidates[0]
                    best_score = best_match['score']
                    best_keywords = best_match['keywords']
                    
                    row['åŒ¹é…åº¦'] = best_score
                    # æå–åŒ¹é…ç‰‡æ®µ
                    idx = best_match['content'].find(best_keywords[0]) if best_keywords else 0
                    start = max(0, idx - 20)
                    end = min(len(best_match['content']), idx + 100)
                    snippet = best_match['content'][start:end] + "..."
                    
                    row['æ”¯æ’‘è¯æ®'] = f"[{best_match['name']}]\nç›¸å…³å†…å®¹: ...{snippet}"
                    
                    if best_score > 30: 
                        row['è¯„ä»·ç»“è®º'] = "âœ…å®Œå…¨ç¬¦åˆ"
                    elif best_score > 10:
                        row['è¯„ä»·ç»“è®º'] = "âš ï¸éƒ¨åˆ†ç¬¦åˆ/éœ€å®Œå–„"

            current_results.append(row)
        
        all_results.extend(current_results)
        
    return pd.DataFrame(all_results)

# ====================
# Streamlit UI ç•Œé¢
# ====================

st.set_page_config(page_title="EHSåˆè§„æ€§æ™ºèƒ½è¯„ä»·åŠ©æ‰‹", layout="wide")

st.title("ğŸ›¡ï¸ EHSæ³•è§„åˆè§„æ€§æ™ºèƒ½è¯„ä»·åŠ©æ‰‹")
st.markdown("""
æœ¬å·¥å…·ç”¨äºè‡ªåŠ¨æ¯”å¯¹ **å¤–éƒ¨æ³•è§„** ä¸ **å†…éƒ¨åˆ¶åº¦**ï¼Œç”Ÿæˆåˆè§„æ€§è¯„ä»·çŸ©é˜µã€‚
è¯·åœ¨å·¦ä¾§ä¾§è¾¹æ ä¸Šä¼ ç›¸åº”çš„æ–‡ä»¶ã€‚
""")

# --- ä¾§è¾¹æ ï¼šæ–‡ä»¶ä¸Šä¼ ä¸é…ç½® ---
with st.sidebar:
    st.header("ğŸ“‚ æ–‡ä»¶ä¸Šä¼ åŒº")
    
    st.subheader("1. ä¸Šä¼ æ³•è§„æ–‡ä»¶ (æ ‡å‡†)")
    reg_files = st.file_uploader("æ”¯æŒ .docx, .zip (å¦‚: å®‰å…¨æ³•.docx)", type=['docx', 'zip'], accept_multiple_files=True, key="reg")
    
    st.subheader("2. ä¸Šä¼ åˆ¶åº¦æ–‡ä»¶ (ä¾æ®)")
    policy_files = st.file_uploader("æ”¯æŒ .docx, .xlsx, .zip (å¦‚: ç®¡ç†æ‰‹å†Œ)", type=['docx', 'xlsx', 'zip'], accept_multiple_files=True, key="pol")
    
    st.divider()
    
    st.header("ğŸ¤– AIå¤§æ¨¡å‹é…ç½® (å¯é€‰)")
    st.info("é…ç½®å¤§æ¨¡å‹å¯æ˜¾è‘—æå‡åˆ†æå‡†ç¡®åº¦ï¼Œæ”¯æŒ Gemini æˆ– OpenAI æ ¼å¼ APIã€‚")
    
    llm_base_url = st.text_input("API Base URL", value="https://generativelanguage.googleapis.com/v1beta/openai/", help="ä¾‹å¦‚: https://api.openai.com/v1 æˆ– Gemini çš„ OpenAI å…¼å®¹ç«¯ç‚¹")
    llm_api_key = st.text_input("API Key", type="password", help="åœ¨æ­¤å¤„è¾“å…¥æ‚¨çš„ API Key")
    llm_model_name = st.text_input("Model Name", value="gemini-2.0-flash", help="ä¾‹å¦‚: gemini-2.0-flash, gpt-4o")
    
    llm_config = {
        "base_url": llm_base_url.rstrip('/'),
        "api_key": llm_api_key,
        "model": llm_model_name
    }
    
    st.info("æç¤ºï¼šæ”¯æŒæ‰¹é‡ä¸Šä¼ æˆ–ZIPå‹ç¼©åŒ…ã€‚æ–‡ä»¶è¶Šå¤šï¼Œåˆ†ææ—¶é—´è¶Šé•¿ï¼Œè¯·è€å¿ƒç­‰å¾…ã€‚")

# --- ä¸»ç•Œé¢ï¼šåˆ†ææ§åˆ¶ä¸å±•ç¤º ---

if reg_files and policy_files:
    if st.button("ğŸš€ å¼€å§‹åˆè§„æ€§åŒ¹é…åˆ†æ", type="primary"):
        # è¿›åº¦æ¡å®¹å™¨
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        try:
            # æ‰§è¡Œåˆ†æ
            df_result = analyze_compliance(reg_files, policy_files, progress_bar, status_text, llm_config)
            
            status_text.text("âœ… åˆ†æå®Œæˆï¼")
            progress_bar.progress(100)
            
            # è®¡ç®—æ±‡æ€»æ•°æ®
            total_clauses = len(df_result)
            compliant_count = len(df_result[df_result['è¯„ä»·ç»“è®º'] == "âœ…å®Œå…¨ç¬¦åˆ"])
            partial_count = len(df_result[df_result['è¯„ä»·ç»“è®º'] == "âš ï¸éƒ¨åˆ†ç¬¦åˆ/éœ€å®Œå–„"])
            non_compliant_count = total_clauses - compliant_count - partial_count
            compliance_rate = ((compliant_count + partial_count) / total_clauses * 100) if total_clauses > 0 else 0
            
            summary_data = {
                "total": total_clauses,
                "compliant": compliant_count,
                "partial": partial_count,
                "non_compliant": non_compliant_count,
                "compliance_rate": compliance_rate
            }
            
            st.divider()
            
            # --- ç¬¬ä¸€éƒ¨åˆ†ï¼šæ€»ä½“è¯„ä»· ---
            st.header("ç¬¬ä¸€éƒ¨åˆ†ï¼šæ€»ä½“è¯„ä»·")
            
            # 1. è¯„ä»·æ¦‚å†µ
            st.subheader("1. è¯„ä»·æ¦‚å†µ")
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("åˆ†ææ¡æ¬¾æ€»æ•°", total_clauses)
            with col2:
                st.metric("å®Œå…¨ç¬¦åˆ", compliant_count)
            with col3:
                st.metric("éƒ¨åˆ†ç¬¦åˆ", partial_count)
            with col4:
                st.metric("æ€»ä½“åˆè§„ç‡", f"{compliance_rate:.1f}%")
            
            # 2. è¯„ä»·ç»“è®ºç»¼è¿°
            st.subheader("2. è¯„ä»·ç»“è®ºç»¼è¿°")
            conclusion = "æ€»ä½“åˆè§„æƒ…å†µè‰¯å¥½ã€‚" if compliance_rate > 80 else "å­˜åœ¨ä¸€å®šåˆè§„é£é™©ï¼Œå»ºè®®é‡ç‚¹å…³æ³¨ç¼ºå¤±å’Œéƒ¨åˆ†ç¬¦åˆçš„æ¡æ¬¾ã€‚"
            st.info(f"æœ¬æ¬¡è¯„ä»·é’ˆå¯¹ä¸Šä¼ çš„æ³•è§„æ–‡ä»¶ä¸ä¼ä¸šå†…éƒ¨åˆ¶åº¦è¿›è¡Œäº†è‡ªåŠ¨æ¯”å¯¹ã€‚\n{conclusion}")

            # --- ç¬¬äºŒéƒ¨åˆ†ï¼šè¯¦ç»†è¯„ä»·çŸ©é˜µ ---
            st.header("ç¬¬äºŒéƒ¨åˆ†ï¼šè¯¦ç»†åˆè§„æ€§è¯„ä»·çŸ©é˜µ")
            
            # å¢åŠ ç­›é€‰åŠŸèƒ½
            filter_status = st.multiselect(
                "ç­›é€‰è¯„ä»·ç»“è®º:",
                options=df_result['è¯„ä»·ç»“è®º'].unique(),
                default=df_result['è¯„ä»·ç»“è®º'].unique()
            )
            
            df_display = df_result[df_result['è¯„ä»·ç»“è®º'].isin(filter_status)]
            st.dataframe(
                df_display, 
                use_container_width=True,
                height=600,
                column_config={
                    "æ³•è§„æ­£æ–‡": st.column_config.TextColumn("æ³•è§„æ­£æ–‡", width="medium"),
                    "æ”¯æ’‘è¯æ®": st.column_config.TextColumn("æ”¯æ’‘è¯æ®", width="large"),
                }
            )
            
            # å¯¼å‡ºåŠŸèƒ½
            st.subheader("ğŸ“¥ å¯¼å‡ºæŠ¥å‘Š")
            
            col_d1, col_d2 = st.columns(2)
            
            with col_d1:
                # Markdown æŠ¥å‘Šä¸‹è½½
                md_report = generate_markdown_report(summary_data, df_result)
                st.download_button(
                    label="ğŸ“„ ä¸‹è½½è¯„ä»·æŠ¥å‘Š (Markdown/Word)",
                    data=md_report,
                    file_name=f"EHSåˆè§„æ€§è¯„ä»·æŠ¥å‘Š_{time.strftime('%Y%m%d')}.md",
                    mime="text/markdown",
                )
            
            with col_d2:
                # CSV ä¸‹è½½
                csv = df_result.to_csv(index=False).encode('utf-8-sig')
                st.download_button(
                    label="ğŸ“Š ä¸‹è½½è¯„ä»·æ˜ç»†è¡¨ (CSV)",
                    data=csv,
                    file_name=f"EHSåˆè§„æ€§è¯„ä»·æ˜ç»†_{time.strftime('%Y%m%d')}.csv",
                    mime="text/csv",
                )
            
        except Exception as e:
            st.error(f"åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            st.exception(e)

else:
    st.info("ğŸ‘ˆ è¯·å…ˆåœ¨å·¦ä¾§ä¾§è¾¹æ ä¸Šä¼ è‡³å°‘ä¸€ä¸ªæ³•è§„æ–‡ä»¶å’Œä¸€ä¸ªåˆ¶åº¦æ–‡ä»¶ã€‚")

st.divider()
st.caption("Powered by Gemini EHS Compliance Engine | 2025")
