import streamlit as st
import pandas as pd
import os
import re
import zipfile
import io
import time
import requests
import json
import numpy as np
import docx 
import pickle
import hashlib
from docx import Document
from docx.shared import Pt, Inches
from docx.enum.text import WD_ALIGN_PARAGRAPH
from concurrent.futures import ThreadPoolExecutor, as_completed
from tenacity import retry, stop_after_attempt, wait_exponential
from webdavclient3.client import Client as WebDavClient

# ====================
# Configuration & Constants
# ====================
CACHE_DIR = "vector_store_cache"
if not os.path.exists(CACHE_DIR):
    os.makedirs(CACHE_DIR)

# ====================
# Core Classes for AI & Data
# ====================

class LLMClient:
    """å¤„ç†ä¸å¤§æ¨¡å‹çš„äº¤äº’ (Embedding å’Œ Chat)"""
    def __init__(self, config):
        self.base_url = config.get('base_url', '').rstrip('/')
        self.api_key = config.get('api_key')
        self.model = config.get('model')
        self.embedding_model = config.get('embedding_model', 'text-embedding-004')
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        self.embedding_failed = False 

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
    def get_embedding(self, text):
        """è·å–æ–‡æœ¬çš„å‘é‡è¡¨ç¤º"""
        if self.embedding_failed: return None
        
        payload = {
            "input": text.replace("\n", " "),
            "model": self.embedding_model
        }
        url = f"{self.base_url}/embeddings"
        
        try:
            response = requests.post(url, headers=self.headers, json=payload, timeout=10)
            if response.status_code == 200:
                data = response.json()
                if 'data' in data and len(data['data']) > 0:
                    return data['data'][0]['embedding']
            else:
                print(f"Embedding failed: {response.status_code}")
                return None
        except Exception as e:
            print(f"Embedding error: {e}")
            return None

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
    def chat_completion(self, system_prompt, user_prompt):
        """è°ƒç”¨ Chat æ¥å£"""
        payload = {
            "model": self.model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            "temperature": 0.1,
            "response_format": {"type": "json_object"}
        }
        try:
            response = requests.post(f"{self.base_url}/chat/completions", headers=self.headers, json=payload, timeout=60)
            if response.status_code == 200:
                content = response.json()['choices'][0]['message']['content']
                content = content.replace("```json", "").replace("```", "")
                return json.loads(content)
            else:
                raise Exception(f"API Error {response.status_code}: {response.text}")
        except Exception as e:
            raise Exception(f"Request failed: {str(e)}")
            
    def test_connection(self):
        results = {"chat": False, "embedding": False, "msg": ""}
        try:
            self.chat_completion("Test", "Reply JSON: {'status': 'ok'}")
            results['chat'] = True
        except Exception as e: results['msg'] += f"Chat Error: {e}\n"
        try:
            if self.get_embedding("test"): results['embedding'] = True
            else: results['msg'] += "Embedding Error: None\n"
        except Exception as e: results['msg'] += f"Embedding Exception: {e}\n"
        return results

class VectorStore:
    """æŒä¹…åŒ–å‘é‡æ•°æ®åº“"""
    def __init__(self):
        self.documents = [] 
        self.vectors = []   
        self.llm_client = None
        self.index_name = "default_index"

    def set_client(self, client):
        self.llm_client = client

    def save_to_disk(self, name="default"):
        """ä¿å­˜ç´¢å¼•åˆ°ç£ç›˜"""
        path = os.path.join(CACHE_DIR, f"{name}.pkl")
        data = {
            "documents": self.documents,
            "vectors": self.vectors
        }
        with open(path, "wb") as f:
            pickle.dump(data, f)
        return path

    def load_from_disk(self, name="default"):
        """ä»ç£ç›˜åŠ è½½ç´¢å¼•"""
        path = os.path.join(CACHE_DIR, f"{name}.pkl")
        if os.path.exists(path):
            with open(path, "rb") as f:
                data = pickle.load(f)
                self.documents = data["documents"]
                self.vectors = data["vectors"]
            return True
        return False

    def add_documents(self, file_corpus):
        """å¤„ç†å¹¶å…¥åº“"""
        chunk_size = 500
        overlap = 50 
        
        new_docs = []
        texts_to_embed = []
        start_doc_id = len(self.documents)
        
        for file in file_corpus:
            text = file['content']
            name = file['name']
            
            # ç®€å•çš„æŸ¥é‡ (åŸºäºæ–‡ä»¶å)
            if any(d['source'] == name for d in self.documents):
                print(f"Skipping {name}, already exists.")
                continue

            for i in range(0, len(text), chunk_size - overlap):
                chunk = text[i:i + chunk_size]
                if len(chunk) < 50: continue 
                
                keywords = set(re.split(r'[ï¼Œã€‚ï¼›ï¼š\s]', chunk))
                keywords = [k for k in keywords if len(k) > 1]

                new_docs.append({
                    'id': start_doc_id,
                    'text': chunk,
                    'source': name,
                    'keywords': keywords 
                })
                texts_to_embed.append(chunk)
                start_doc_id += 1
        
        if not texts_to_embed:
            return

        # å‘é‡åŒ–
        if self.llm_client:
            with st.status(f"æ­£åœ¨å‘é‡åŒ– {len(texts_to_embed)} ä¸ªæ–°ç‰‡æ®µ...") as status:
                new_vectors = [None] * len(texts_to_embed)
                success_count = 0
                
                with ThreadPoolExecutor(max_workers=5) as executor:
                    future_to_idx = {executor.submit(self.llm_client.get_embedding, t): i for i, t in enumerate(texts_to_embed)}
                    for future in as_completed(future_to_idx):
                        idx = future_to_idx[future]
                        vec = future.result()
                        new_vectors[idx] = vec
                        if vec is not None: success_count += 1
                
                # åˆå¹¶
                self.documents.extend(new_docs)
                if len(self.vectors) == 0:
                    self.vectors = new_vectors
                else:
                    self.vectors = list(self.vectors) + new_vectors # Convert back to list to extend
                
                status.update(label=f"å…¥åº“å®Œæˆ (æˆåŠŸç‡: {success_count}/{len(texts_to_embed)})", state="complete")

    def search(self, query_text, top_k=3):
        """æ··åˆæ£€ç´¢"""
        vec_results = []
        
        # 1. å‘é‡æ£€ç´¢
        # è¿‡æ»¤ None å‘é‡
        valid_indices = [i for i, v in enumerate(self.vectors) if v is not None]
        
        if valid_indices and self.llm_client:
            query_vec = self.llm_client.get_embedding(query_text)
            if query_vec is not None:
                q_v = np.array(query_vec)
                norm_q = np.linalg.norm(q_v)
                
                # æ„å»ºçŸ©é˜µ
                matrix = np.array([self.vectors[i] for i in valid_indices])
                norm_matrix = np.linalg.norm(matrix, axis=1)
                
                if norm_q > 0:
                    # Cosine Sim
                    scores = np.dot(matrix, q_v) / (norm_matrix * norm_q)
                    
                    # è·å– Top K
                    top_k_indices = np.argsort(scores)[-top_k:][::-1]
                    
                    for idx_in_valid in top_k_indices:
                        real_idx = valid_indices[idx_in_valid]
                        score = scores[idx_in_valid]
                        if score > 0:
                            vec_results.append({'doc': self.documents[real_idx], 'score': float(score), 'method': 'vector'})

        # 2. å…³é”®è¯æ£€ç´¢
        kw_results = []
        query_keywords = [k for k in re.split(r'[ï¼Œã€‚ï¼›ï¼š\s]', query_text) if len(k) > 1]
        
        for doc in self.documents:
            overlap = sum(1 for k in query_keywords if k in doc['keywords']) # ä½¿ç”¨é¢„å­˜çš„keywordsé›†åˆåŠ é€Ÿ
            if overlap > 0:
                score = overlap / (len(query_keywords) + 1) * 0.8 
                kw_results.append({'doc': doc, 'score': score, 'method': 'keyword'})
        
        kw_results.sort(key=lambda x: x['score'], reverse=True)
        kw_results = kw_results[:top_k]

        combined = vec_results + kw_results
        seen_ids = set()
        final_results = []
        combined.sort(key=lambda x: x['score'], reverse=True)
        
        for res in combined:
            did = res['doc']['id']
            if did not in seen_ids:
                final_results.append({'source': res['doc']['source'], 'content': res['doc']['text'], 'score': res['score']})
                seen_ids.add(did)
            if len(final_results) >= top_k: break
                
        return final_results

# ====================
# Helper Functions
# ====================

def process_uploaded_files(uploaded_files):
    for uploaded_file in uploaded_files:
        if uploaded_file.name.endswith('.zip'):
            try:
                with zipfile.ZipFile(uploaded_file) as z:
                    for filename in z.namelist():
                        if filename.endswith('/') or filename.startswith('__MACOSX') or filename.startswith('._'): continue
                        if filename.endswith(('.docx', '.xlsx')):
                            with z.open(filename) as f: yield filename, f.read()
            except Exception as e: st.error(f"è§£å‹å¤±è´¥: {e}")
        else: yield uploaded_file.name, uploaded_file.getvalue()

def extract_text_from_content(filename, content):
    text = ""
    try:
        file_stream = io.BytesIO(content)
        if filename.endswith('.docx'):
            doc = docx.Document(file_stream)
            text = '\n'.join([para.text for para in doc.paragraphs])
        elif filename.endswith('.xlsx'):
            df_dict = pd.read_excel(file_stream, sheet_name=None, header=None)
            text_parts = []
            for sheet_name, df in df_dict.items():
                sheet_text = df.astype(str).apply(lambda x: ' '.join(x), axis=1)
                text_parts.append('\n'.join(sheet_text))
            text = '\n'.join(text_parts)
    except Exception as e: print(f"Error parsing {filename}: {e}")
    return text

def parse_regulation_clauses(text):
    pattern = r'(ç¬¬\s*[\dé›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+\s*æ¡|Article\s+\d+)'
    parts = re.split(pattern, text)
    clauses = []
    if len(parts) > 1:
        for i in range(1, len(parts), 2):
            title = parts[i].strip()
            content = parts[i+1].strip() if i+1 < len(parts) else ""
            clauses.append({"æ¡æ¬¾å·": title, "æ³•è§„æ­£æ–‡": title + " " + content, "é€‚ç”¨æ€§": "é€‚ç”¨"})
    return clauses

def evaluate_single_clause(clause, vector_store, llm_client):
    row = {"æ¡æ¬¾å·": clause['æ¡æ¬¾å·'], "æ³•è§„æ­£æ–‡": clause['æ³•è§„æ­£æ–‡'], "è¯„ä»·ç»“è®º": "âŒç¼ºå¤±/ä¸ç¬¦åˆ", "å·®è·åˆ†æ": "æœªæ£€ç´¢åˆ°ç›¸å…³åˆ¶åº¦", "æ”¹è¿›å»ºè®®": "è¯·è¡¥å……ç›¸å…³ç®¡ç†è§„å®š", "æ”¯æ’‘è¯æ®": "æ— ", "åŒ¹é…åº¦": 0.0}

    search_results = vector_store.search(clause['æ³•è§„æ­£æ–‡'], top_k=3)
    top_score = search_results[0]['score'] if search_results else 0
    row['åŒ¹é…åº¦'] = top_score
        
    evidence_text = ""
    if search_results:
        for i, res in enumerate(search_results):
            evidence_text += f"å‚è€ƒåˆ¶åº¦ç‰‡æ®µ {i+1} (æ¥æº: {res['source']}):\n{res['content'][:800]}\n---\n"
    else:
        evidence_text = "æœªæ£€ç´¢åˆ°ä»»ä½•ç›¸å…³çš„ä¼ä¸šå†…éƒ¨åˆ¶åº¦æ–‡æ¡£ã€‚"
    
    system_prompt = """ä½ æ˜¯ä¸€åå…·æœ‰20å¹´ç»éªŒçš„EHSç®¡ç†ä¸“å®¶ï¼Œç²¾é€šä¸­å›½EHSæ³•è§„æ ‡å‡†ï¼Œæ“…é•¿ä»“å‚¨ç‰©æµåœºæ™¯ã€‚
    è¯·å¯¹ç»™å®šçš„æ³•è§„æ¡æ¬¾è¿›è¡Œåˆè§„æ€§è¯„ä»·ã€‚ä¸¥æ ¼æ‰§è¡Œä»¥ä¸‹æ€ç»´é“¾ï¼š
    1. è§£è¯»ï¼šç†è§£æ¡æ¬¾æ ¸å¿ƒè¦æ±‚ï¼ˆäººæœºæ–™æ³•ç¯ï¼‰ï¼Œåˆ¤å®šæ˜¯å¦é€‚ç”¨äºç‰©æµä»“å‚¨ä¼ä¸šã€‚å¦‚æœä¸é€‚ç”¨ï¼Œç›´æ¥æ ‡è®°â€œä¸é€‚ç”¨â€ã€‚
    2. æ¯”å¯¹ï¼šå¯¹æ¯”æ³•è§„è¦æ±‚ä¸æä¾›çš„ä¼ä¸šåˆ¶åº¦ç‰‡æ®µã€‚æ˜¯å¦è¦†ç›–æ‰€æœ‰è¦ç´ ï¼Ÿé’ˆå¯¹ç‰©æµåœºæ™¯æ˜¯å¦å…·ä½“å¯æ‰§è¡Œï¼Ÿ
    3. åˆ¤å®šï¼šç»™å‡ºå®šæ€§ç»“è®ºã€‚
    """
    
    user_prompt = f"""
    ã€æ³•è§„æ¡æ¬¾ã€‘
    {clause['æ³•è§„æ­£æ–‡']}

    ã€ä¼ä¸šåˆ¶åº¦ç°çŠ¶ï¼ˆæ£€ç´¢åˆ°çš„æœ€ç›¸å…³ç‰‡æ®µï¼‰ã€‘
    {evidence_text}

    ã€ä»»åŠ¡è¦æ±‚ã€‘
    è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¿”å› JSON ç»“æœï¼š
    {{
        "applicability": "é€‚ç”¨" æˆ– "ä¸é€‚ç”¨",
        "compliance_status": "å®Œå…¨ç¬¦åˆ" æˆ– "éƒ¨åˆ†ç¬¦åˆ/éœ€å®Œå–„" æˆ– "ç¼ºå¤±/ä¸ç¬¦åˆ" æˆ– "ä¸é€‚ç”¨",
        "gap_analysis": "150å­—ä»¥å†…ã€‚è‹¥ä¸é€‚ç”¨å¡«'ä¸é€‚ç”¨'ã€‚è‹¥é€‚ç”¨ï¼Œåˆ†æå…·ä½“ç¼ºäº†ä»€ä¹ˆæˆ–è¯´æ˜åˆè§„ç‚¹ã€‚",
        "improvement_suggestion": "è‹¥å®Œå…¨ç¬¦åˆå¡«'æ— 'ã€‚å¦åˆ™ç»™å‡º1-2æ¡å»ºè®®ã€‚",
        "evidence_summary": "åˆ—å‡ºæœ€åŒ¹é…çš„åˆ¶åº¦åç§°åŠå…³é”®å¥æ‘˜è¦"
    }}
    """
    try:
        result = llm_client.chat_completion(system_prompt, user_prompt)
        status = result.get('compliance_status', 'ç¼ºå¤±/ä¸ç¬¦åˆ')
        if "å®Œå…¨ç¬¦åˆ" in status: status = "âœ…å®Œå…¨ç¬¦åˆ"
        elif "éƒ¨åˆ†" in status or "éœ€å®Œå–„" in status: status = "âš ï¸éƒ¨åˆ†ç¬¦åˆ/éœ€å®Œå–„"
        elif "ä¸é€‚ç”¨" in status: status = "â—ä¸é€‚ç”¨"
        else: status = "âŒç¼ºå¤±/ä¸ç¬¦åˆ"
        
        row['è¯„ä»·ç»“è®º'] = status
        row['å·®è·åˆ†æ'] = result.get('gap_analysis', 'æ— åˆ†æ')
        row['æ”¹è¿›å»ºè®®'] = result.get('improvement_suggestion', 'æ— å»ºè®®')
        row['æ”¯æ’‘è¯æ®'] = result.get('evidence_summary', 'æ— ')
    except Exception as e:
        row['å·®è·åˆ†æ'] = f"LLMåˆ†æå¤±è´¥: {str(e)}"
    return row

def generate_word_report(df_results, summary_stats):
    doc = Document()
    title = doc.add_heading('EHSæ³•è§„åˆè§„æ€§è¯„ä»·æŠ¥å‘Š', 0)
    title.alignment = WD_ALIGN_PARAGRAPH.CENTER
    doc.add_paragraph(f"è¯„ä»·æ—¥æœŸ: {time.strftime('%Y-%m-%d')}")
    
    doc.add_heading('ç¬¬ä¸€éƒ¨åˆ†ï¼šæ€»ä½“è¯„ä»·', level=1)
    p = doc.add_paragraph()
    p.add_run(f"æœ¬æ¬¡å…±åˆ†ææ³•è§„æ¡æ¬¾ {summary_stats['total']} æ¡ã€‚\n")
    p.add_run(f"âœ… å®Œå…¨ç¬¦åˆ: {summary_stats['compliant']} æ¡\n")
    p.add_run(f"âš ï¸ éƒ¨åˆ†ç¬¦åˆ/éœ€å®Œå–„: {summary_stats['partial']} æ¡\n")
    p.add_run(f"âŒ ç¼ºå¤±/ä¸ç¬¦åˆ: {summary_stats['non_compliant']} æ¡\n")
    
    doc.add_heading('ç¬¬äºŒéƒ¨åˆ†ï¼šè¯¦ç»†åˆè§„æ€§è¯„ä»·çŸ©é˜µ', level=1)
    table = doc.add_table(rows=1, cols=6)
    table.style = 'Table Grid'
    headers = ["åºå·", "æ³•è§„æ¡æ¬¾", "è¯„ä»·ç»“è®º", "å·®è·åˆ†æä¸è®ºæ®", "æ”¹è¿›å»ºè®®", "æ”¯æ’‘è¯æ®"]
    for i, header in enumerate(headers):
        table.rows[0].cells[i].text = header
        
    for idx, row in df_results.iterrows():
        row_cells = table.add_row().cells
        row_cells[0].text = str(row.get('åºå·', idx + 1))
        row_cells[1].text = str(row.get('æ³•è§„æ­£æ–‡', ''))
        row_cells[2].text = str(row.get('è¯„ä»·ç»“è®º', ''))
        row_cells[3].text = str(row.get('å·®è·åˆ†æ', ''))
        row_cells[4].text = str(row.get('æ”¹è¿›å»ºè®®', ''))
        row_cells[5].text = str(row.get('æ”¯æ’‘è¯æ®', ''))
        
    f = io.BytesIO()
    doc.save(f)
    f.seek(0)
    return f

# ====================
# Streamlit UI
# ====================

st.set_page_config(page_title="EHSä¸“å®¶åˆè§„ç³»ç»Ÿ (Enterprise)", layout="wide")

if 'results' not in st.session_state: st.session_state.results = None
if 'vector_store' not in st.session_state: st.session_state.vector_store = VectorStore()

st.title("ğŸ›¡ï¸ EHSæ³•è§„åˆè§„æ€§æ™ºèƒ½è¯„ä»·ç³»ç»Ÿ (Enterprise)")

with st.sidebar:
    st.header("1. API é…ç½®")
    llm_base_url = st.text_input("API Base URL", value="https://generativelanguage.googleapis.com/v1beta/openai")
    llm_api_key = st.text_input("API Key", type="password")
    
    col_m1, col_m2 = st.columns(2)
    with col_m1: llm_model_name = st.text_input("Chat Model", value="gemini-2.0-flash")
    with col_m2: embedding_model_name = st.text_input("Embedding Model", value="text-embedding-004")
    
    # åˆå§‹åŒ– LLM
    if llm_api_key:
        llm_config = {"base_url": llm_base_url, "api_key": llm_api_key, "model": llm_model_name, "embedding_model": embedding_model_name}
        client = LLMClient(llm_config)
        st.session_state.vector_store.set_client(client)
    
    st.divider()
    
    # å‘é‡åº“ç®¡ç†
    st.header("ğŸ’¾ å‘é‡åº“ç®¡ç†")
    db_name = st.text_input("ç´¢å¼•åç§°", value="ehs_master_index")
    col_db1, col_db2 = st.columns(2)
    with col_db1:
        if st.button("ä¿å­˜ç´¢å¼•"):
            path = st.session_state.vector_store.save_to_disk(db_name)
            st.success(f"å·²ä¿å­˜: {path}")
    with col_db2:
        if st.button("åŠ è½½ç´¢å¼•"):
            if st.session_state.vector_store.load_from_disk(db_name):
                st.success(f"å·²åŠ è½½! ({len(st.session_state.vector_store.documents)} ç‰‡æ®µ)")
            else:
                st.error("ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨")

st.info(f"å½“å‰å‘é‡åº“çŠ¶æ€: åŒ…å« {len(st.session_state.vector_store.documents)} ä¸ªåˆ¶åº¦ç‰‡æ®µ")

tab1, tab2, tab3 = st.tabs(["ğŸ“‚ æœ¬åœ°æ–‡ä»¶ä¸Šä¼ ", "â˜ï¸ WebDAV è¿œç¨‹åº“", "ğŸš€ å¼€å§‹è¯„ä¼°"])

with tab1:
    reg_files_local = st.file_uploader("ä¸Šä¼ æ³•è§„ (docx/zip)", type=['docx', 'zip'], accept_multiple_files=True, key="reg_local")
    policy_files_local = st.file_uploader("ä¸Šä¼ åˆ¶åº¦ (docx/xlsx/zip)", type=['docx', 'xlsx', 'zip'], accept_multiple_files=True, key="pol_local")
    
    if st.button("ğŸ“¥ å°†æœ¬åœ°åˆ¶åº¦åŠ å…¥å‘é‡åº“"):
        if policy_files_local:
            corpus = []
            for name, content in process_uploaded_files(policy_files_local):
                text = extract_text_from_content(name, content)
                if text: corpus.append({'name': name, 'content': text})
            st.session_state.vector_store.add_documents(corpus)
            st.success("å…¥åº“å®Œæˆï¼è¯·ç‚¹å‡»ä¾§è¾¹æ ä¿å­˜ç´¢å¼•ã€‚")

with tab2:
    st.markdown("### è¿æ¥åˆ° WebDAV æœåŠ¡å™¨ (å¦‚ Nextcloud/åšæœäº‘)")
    webdav_url = st.text_input("WebDAV URL", help="e.g. https://dav.jianguoyun.com/dav/")
    webdav_user = st.text_input("Username")
    webdav_pass = st.text_input("Password", type="password")
    
    if st.button("ğŸ”— è¿æ¥å¹¶è·å–æ–‡ä»¶åˆ—è¡¨"):
        try:
            options = {'webdav_hostname': webdav_url, 'webdav_login': webdav_user, 'webdav_password': webdav_pass}
            wd_client = WebDavClient(options)
            files = wd_client.list() # List root
            st.session_state.webdav_files = [f for f in files if f.endswith(('.docx', '.zip', '.xlsx'))]
            st.session_state.wd_client = wd_client
            st.success(f"æˆåŠŸè¿æ¥ï¼å‘ç° {len(st.session_state.webdav_files)} ä¸ªæ”¯æŒçš„æ–‡ä»¶ã€‚")
        except Exception as e:
            st.error(f"è¿æ¥å¤±è´¥: {e}")

    if 'webdav_files' in st.session_state:
        selected_files = st.multiselect("é€‰æ‹©è¦åˆ†æçš„æ³•è§„/åˆ¶åº¦æ–‡ä»¶", st.session_state.webdav_files)
        file_type = st.radio("è¿™äº›æ–‡ä»¶æ˜¯:", ["åˆ¶åº¦ (åŠ å…¥å‘é‡åº“)", "æ³•è§„ (ç”¨äºåˆ†æ)"])
        
        if st.button("â¬‡ï¸ ä¸‹è½½å¹¶å¤„ç†é€‰å®šæ–‡ä»¶"):
            downloaded_corpus = []
            for fname in selected_files:
                try:
                    # WebDAV download to memory
                    with st.spinner(f"æ­£åœ¨ä¸‹è½½ {fname}..."):
                        # webdavclient3 download_from returns None, writes to file. We need bytes.
                        # Using buffer
                        buff = io.BytesIO()
                        st.session_state.wd_client.download_from(fname, buff)
                        buff.seek(0)
                        content = buff.read()
                        
                        text = extract_text_from_content(fname, content)
                        if text: downloaded_corpus.append({'name': fname, 'content': text})
                except Exception as e:
                    st.error(f"ä¸‹è½½ {fname} å¤±è´¥: {e}")
            
            if file_type == "åˆ¶åº¦ (åŠ å…¥å‘é‡åº“)":
                st.session_state.vector_store.add_documents(downloaded_corpus)
                st.success("WebDAV åˆ¶åº¦æ–‡ä»¶å·²å…¥åº“ï¼")
            else:
                st.session_state.webdav_reg_corpus = downloaded_corpus
                st.success("WebDAV æ³•è§„æ–‡ä»¶å·²å‡†å¤‡å°±ç»ªï¼")

with tab3:
    st.subheader("æ‰§è¡Œåˆè§„æ€§åˆ†æ")
    st.markdown("æ•°æ®æº: **å·²åŠ è½½çš„å‘é‡åº“** (åˆ¶åº¦) vs **ä¸Šä¼ /é€‰å®šçš„æ³•è§„æ–‡ä»¶**")
    
    if st.button("ğŸš€ å¼€å§‹ä¸“å®¶çº§è¯„ä¼°", type="primary"):
        # å‡†å¤‡æ³•è§„
        reg_corpus = []
        if reg_files_local:
            for name, content in process_uploaded_files(reg_files_local):
                text = extract_text_from_content(name, content)
                reg_corpus.append({'name': name, 'content': text})
        
        if 'webdav_reg_corpus' in st.session_state:
            reg_corpus.extend(st.session_state.webdav_reg_corpus)
            
        if not reg_corpus:
            st.error("è¯·å…ˆä¸Šä¼ æˆ–é€‰æ‹©æ³•è§„æ–‡ä»¶ï¼")
            st.stop()
            
        if len(st.session_state.vector_store.documents) == 0:
            st.error("å‘é‡åº“ä¸ºç©ºï¼è¯·å…ˆä¸Šä¼ åˆ¶åº¦æ–‡ä»¶å¹¶å…¥åº“ã€‚" )
            st.stop()
            
        # è§£ææ³•è§„
        all_clauses = []
        for doc in reg_corpus:
            clauses = parse_regulation_clauses(doc['content'])
            for i, c in enumerate(clauses):
                c['source_file'] = doc['name']
                c['åºå·'] = i + 1
                all_clauses.append(c)
                
        st.info(f"å…±è¯†åˆ«å‡º {len(all_clauses)} æ¡æ³•è§„æ¡æ¬¾ï¼Œå¼€å§‹åˆ†æ...")
        
        # å¹¶å‘æ‰§è¡Œ
        results_list = []
        progress_bar = st.progress(0)
        status_text = st.empty()
        completed = 0
        
        with ThreadPoolExecutor(max_workers=10) as executor:
            future_to_clause = {executor.submit(evaluate_single_clause, clause, st.session_state.vector_store, client): clause for clause in all_clauses}
            for future in as_completed(future_to_clause):
                res = future.result()
                res['æ³•è§„æ–‡ä»¶'] = future_to_clause[future]['source_file']
                res['åºå·'] = future_to_clause[future]['åºå·']
                results_list.append(res)
                completed += 1
                progress_bar.progress(completed / len(all_clauses))
                status_text.text(f"åˆ†æè¿›åº¦: {completed}/{len(all_clauses)}")
                
        st.success("åˆ†æå®Œæˆï¼")
        results_list.sort(key=lambda x: x['åºå·'])
        st.session_state.results = pd.DataFrame(results_list)

    if st.session_state.results is not None:
        df = st.session_state.results
        summary_stats = {
            "total": len(df),
            "compliant": len(df[df['è¯„ä»·ç»“è®º'].str.contains("å®Œå…¨ç¬¦åˆ")]) ,
            "partial": len(df[df['è¯„ä»·ç»“è®º'].str.contains("éƒ¨åˆ†")]) ,
            "non_compliant": len(df[df['è¯„ä»·ç»“è®º'].str.contains("ç¼ºå¤±|ä¸ç¬¦åˆ")])
        }
        
        st.dataframe(df)
        word_file = generate_word_report(df, summary_stats)
        st.download_button("ğŸ“¥ ä¸‹è½½ Word æŠ¥å‘Š", word_file, "EHS_Report.docx", "application/vnd.openxmlformats-officedocument.wordprocessingml.document")