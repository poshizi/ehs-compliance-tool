import streamlit as st
import pandas as pd
import re
import os
import zipfile
import math
from collections import Counter
import time
from io import BytesIO

# ==============================================================================
# 1. æ ¸å¿ƒé€»è¾‘å±‚ (Core Logic) - ä¼˜åŒ–åçš„æ€ç»´é“¾å¼•æ“
# ==============================================================================

class ComplianceEngine:
    def __init__(self):
        # æ ¸å¿ƒå…³é”®è¯æ˜ å°„ï¼ˆè¯­ä¹‰æ‰©å±•åº“ï¼‰
        self.KEYWORD_MAPPING = {
            "ä¸»è¦è´Ÿè´£äºº": ["æ€»ç»ç†", "è‘£äº‹é•¿", "ç¬¬ä¸€è´£ä»»äºº", "è´Ÿè´£äºº", "å…šå§”ä¹¦è®°", "EHSå§”å‘˜ä¼šä¸»ä»»"],
            "å…¨å‘˜å®‰å…¨ç”Ÿäº§è´£ä»»åˆ¶": ["å²—ä½å®‰å…¨è´£ä»»", "ä¸€å²—åŒè´£", "å®‰å…¨èŒè´£", "è´£ä»»ä¹¦", "æ‰¿è¯ºä¹¦", "ç»©æ•ˆè€ƒæ ¸"],
            "èµ„é‡‘æŠ•å…¥": ["å®‰å…¨æŠ•å…¥", "ç»è´¹", "é¢„ç®—", "è´¹ç”¨", "æå–", "å®‰è´£é™©"],
            "æ•™è‚²åŸ¹è®­": ["åŸ¹è®­", "å­¦ä¹ ", "è€ƒæ ¸", "ä¸‰çº§æ•™è‚²", "å¤è®­", "ç»§ç»­æ•™è‚²"],
            "éšæ‚£æ’æŸ¥": ["éšæ‚£æ²»ç†", "æ£€æŸ¥", "å·¡æŸ¥", "è‡ªæŸ¥", "æ•´æ”¹", "åŒé‡é¢„é˜²"],
            "é£é™©åˆ†çº§ç®¡æ§": ["é£é™©è¾¨è¯†", "é£é™©è¯„ä¼°", "å±é™©æº", "é£é™©æ¸…å•", "LEC"],
            "åº”æ€¥æ•‘æ´": ["åº”æ€¥é¢„æ¡ˆ", "æ¼”ç»ƒ", "å¤„ç½®æ–¹æ¡ˆ", "å“åº”", "æ•‘æ´é˜Ÿä¼"],
            "ç›¸å…³æ–¹": ["æ‰¿åŒ…å•†", "å¤–åŒ…", "ä¾›åº”å•†", "æ‰¿è¿æ–¹", "ç§Ÿèµ", "åŠ³åŠ¡æ´¾é£"],
            "ç‰¹ç§ä½œä¸š": ["ç”µå·¥", "ç„Šæ¥", "é«˜å¤„ä½œä¸š", "æŒè¯ä¸Šå²—", "ä½œä¸šè®¸å¯"],
            "åŠ³åŠ¨é˜²æŠ¤ç”¨å“": ["åŠ³ä¿ç”¨å“", "é˜²æŠ¤æœ", "å®‰å…¨å¸½", "PPE"],
            "èŒä¸šå¥åº·": ["èŒä¸šç—…", "ä½“æ£€", "å¥åº·æ¡£æ¡ˆ", "å±å®³å› ç´ ", "å¿ƒç†ç–å¯¼"],
            "ä¸‰åŒæ—¶": ["æ–°å»º", "æ”¹å»º", "æ‰©å»º", "è®¾è®¡", "éªŒæ”¶", "å·¥ç¨‹é¡¹ç›®"],
            "å·¥ä¼š": ["èŒå·¥ä»£è¡¨", "æ°‘ä¸»ç›‘ç£", "å·¥ä¼š", "èŒä»£ä¼š"],
            "ç›‘ç£æ£€æŸ¥": ["é…åˆæ£€æŸ¥", "æ¥å—ç›‘ç£", "è¿æ£€", "åˆè§„æ€§è¯„ä»·"],
            "æ³•å¾‹è´£ä»»": ["è´£ä»»è¿½ç©¶", "è¿è§„å¤„ç½š", "è¡Œæ”¿å¤„åˆ†", "é—®è´£"]
        }
        self.corpus_data = []

    def load_corpus_from_uploaded_files(self, uploaded_files):
        """ä»ä¸Šä¼ çš„æ–‡ä»¶ä¸­æ„å»ºè¯­æ–™åº“"""
        self.corpus_data = []
        for uploaded_file in uploaded_files:
            filename = uploaded_file.name
            try:
                # è¯»å–docxæ–‡æœ¬
                text = self._extract_text_from_docx_stream(uploaded_file)
                # æŒ‰æ®µè½åˆ‡åˆ†ï¼Œä¿ç•™ä¸Šä¸‹æ–‡
                segments = re.split(r'[ã€‚\nï¼›]', text)
                for seg in segments:
                    clean_seg = seg.strip()
                    if len(clean_seg) > 15: # å¿½ç•¥è¿‡çŸ­ç‰‡æ®µ
                        self.corpus_data.append({
                            'file': filename,
                            'content': clean_seg
                        })
            except Exception as e:
                st.error(f"æ— æ³•è¯»å–æ–‡ä»¶ {filename}: {str(e)}")
        return len(self.corpus_data)

    def _extract_text_from_docx_stream(self, file_stream):
        """ä»æ–‡ä»¶æµè§£æDOCX"""
        try:
            with zipfile.ZipFile(file_stream) as z:
                xml_content = z.read('word/document.xml').decode('utf-8')
                text = re.sub(r'<[^>]+>', '', xml_content)
                return text
        except:
            return ""

    def analyze_clause(self, clause_text):
        """æ­¥éª¤1ï¼šè§£è¯»æ¡æ¬¾ (æ€ç»´é“¾ï¼šç†è§£æ³•è§„æ„å›¾)"""
        info = {
            'is_applicable': True,
            'intent': 'general',
            'applicability_reason': 'ä¼ä¸šé€šç”¨åˆè§„ä¹‰åŠ¡',
            'search_keywords': [],
            'required_elements': [] # å¿…é¡»å…·å¤‡çš„é—­ç¯è¦ç´ ï¼šè®°å½•ã€æŠ¥å‘Šã€åŸ¹è®­ç­‰
        }

        # 1. é€‚ç”¨æ€§åˆ¤å®š (æ’é™¤çº¯æ”¿åºœèŒèƒ½)
        gov_keywords = ["å›½åŠ¡é™¢", "å¿çº§ä»¥ä¸Š", "ç›‘å¯Ÿæœºå…³", "åˆ¶å®šæ ‡å‡†", "è´¢æ”¿éƒ¨é—¨", "è¡Œä¸šåä¼š"]
        if any(k in clause_text for k in gov_keywords) and \
           not any(k in clause_text for k in ["ç”Ÿäº§ç»è¥å•ä½", "ä¼ä¸š", "ä¸»è¦è´Ÿè´£äºº", "ä»ä¸šäººå‘˜", "é…åˆ", "æ¥å—"]):
            info['is_applicable'] = False
            info['applicability_reason'] = "å±æ”¿åºœè¡Œæ”¿èŒèƒ½æ¡æ¬¾"
            return info
        
        if "æœ¬æ³•è‡ª" in clause_text and "æ–½è¡Œ" in clause_text:
            info['is_applicable'] = False
            info['applicability_reason'] = "ç”Ÿæ•ˆæ—¶é—´æ¡æ¬¾"
            return info

        if "å«ä¹‰" in clause_text and "ä¸‹åˆ—ç”¨è¯­" in clause_text:
            info['is_applicable'] = True
            info['applicability_reason'] = "æœ¯è¯­å®šä¹‰"
            info['search_keywords'] = ["æœ¯è¯­", "å®šä¹‰", "é™„åˆ™"]

        # 2. æå–é—­ç¯è¦ç´  (æ·±åº¦ç†è§£)
        if "è®°å½•" in clause_text or "æ¡£æ¡ˆ" in clause_text or "å°è´¦" in clause_text:
            info['required_elements'].append("è®°å½•ç•™ç—•")
        if "æŠ¥å‘Š" in clause_text or "é€šæŠ¥" in clause_text:
            info['required_elements'].append("æŠ¥å‘Šæœºåˆ¶")
        if "åŸ¹è®­" in clause_text or "æ•™è‚²" in clause_text:
            info['required_elements'].append("æ•™è‚²åŸ¹è®­")

        # 3. å…³é”®è¯æå–ä¸æ‰©å±•
        found_keys = []
        for key, synonyms in self.KEYWORD_MAPPING.items():
            if key in clause_text or any(s in clause_text for s in synonyms):
                found_keys.extend(synonyms)
                found_keys.append(key)
        
        # è¡¥å……é€šç”¨è¯
        clean_text = re.sub(r'[^\w]', ' ', clause_text)
        raw_words = [w for w in clean_text.split() if len(w) > 1 and w not in ["åº”å½“","å¯ä»¥","å¿…é¡»","å•ä½","è§„å®š"]]
        
        info['search_keywords'] = list(set(found_keys + raw_words[:5]))
        return info

    def search_evidence(self, keywords):
        """æ­¥éª¤2ï¼šæ£€ç´¢ (åœ¨è¯­æ–™åº“ä¸­å¯»æ‰¾æ”¯æ’‘)"""
        if not self.corpus_data or not keywords: return []
        
        matches = []
        for item in self.corpus_data:
            score = 0
            content = item['content']
            hit_words = []
            
            for kw in keywords:
                if kw in content:
                    score += 1
                    hit_words.append(kw)
            
            if score > 0:
                # åˆ¶åº¦ç±»æ–‡ä»¶åŠ æƒ
                if "åˆ¶åº¦" in content or "åŠæ³•" in content or "è§„å®š" in content:
                    score += 0.5
                matches.append({
                    'file': item['file'],
                    'content': content,
                    'score': score,
                    'hits': hit_words
                })
        
        matches.sort(key=lambda x: x['score'], reverse=True)
        return matches[:3]

    def judge_compliance(self, clause_text, analysis, evidence):
        """æ­¥éª¤3-5ï¼šæ¯”å¯¹ä¸åˆ¤å®š (æ€ç»´é“¾ï¼šé€»è¾‘æ¨ç†)"""
        if not analysis['is_applicable']:
            return "â—ä¸é€‚ç”¨", analysis['applicability_reason'], "æ— "

        if not evidence:
            return "âŒç¼ºå¤±/ä¸ç¬¦åˆ", "åˆ¶åº¦åº“ä¸­å®Œå…¨æœªæ£€ç´¢åˆ°ç›¸å…³ç®¡æ§æ¡æ¬¾ï¼Œå­˜åœ¨åˆ¶åº¦ç©ºç™½ã€‚", \
                   f"å»ºè®®æ–°å¢å…³äºâ€œ{analysis['search_keywords'][:3]}â€çš„ä¸“é¡¹ç®¡ç†è§„å®šã€‚"

        top_ev = evidence[0]
        score = top_ev['score']
        
        # é—­ç¯éªŒè¯
        missing_loops = []
        for req in analysis['required_elements']:
            if req == "è®°å½•ç•™ç—•" and not any(w in top_ev['content'] for w in ["è®°å½•", "æ¡£æ¡ˆ", "å°è´¦", "å‡­è¯"]):
                missing_loops.append("è®°å½•è¦æ±‚")
            if req == "æŠ¥å‘Šæœºåˆ¶" and not any(w in top_ev['content'] for w in ["æŠ¥å‘Š", "é€šæŠ¥", "ä¸ŠæŠ¥"]):
                missing_loops.append("æŠ¥å‘Šæµç¨‹")

        if score >= 2.0:
            if missing_loops:
                return "âš ï¸éƒ¨åˆ†ç¬¦åˆ/éœ€å®Œå–„", \
                       f"åˆ¶åº¦æ¶µç›–äº†ä¸»ä½“å†…å®¹ï¼Œä½†ç¼ºä¹{'ã€'.join(missing_loops)}ç­‰é—­ç¯ç®¡ç†è¦æ±‚ã€‚", \
                       f"å»ºè®®åœ¨ã€Š{top_ev['file']}ã€‹ä¸­è¡¥å……{','.join(missing_loops)}çš„å…·ä½“è§„å®šã€‚"
            else:
                return "âœ…å®Œå…¨ç¬¦åˆ", "åˆ¶åº¦æ¡æ¬¾æ˜ç¡®ï¼Œè¦ç´ é½å…¨ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ”¯æ’‘æ³•è§„è¦æ±‚ã€‚", "æ— "
        elif score >= 1.0:
            return "âš ï¸éƒ¨åˆ†ç¬¦åˆ/éœ€å®Œå–„", \
                   "åˆ¶åº¦ä¸­æœ‰æåŠç›¸å…³æ¦‚å¿µï¼Œä½†æ‰§è¡Œç»†èŠ‚ï¼ˆå¦‚é¢‘æ¬¡ã€è´£ä»»äººã€æ ‡å‡†ï¼‰ä¸å¤Ÿæ˜ç¡®ã€‚", \
                   f"å»ºè®®ç»†åŒ–å…³äº{analysis['search_keywords'][:2]}çš„å…·ä½“æ‰§è¡Œç»†åˆ™ã€‚"
        else:
            return "âŒç¼ºå¤±/ä¸ç¬¦åˆ", "æ£€ç´¢åˆ°çš„åˆ¶åº¦å…³è”åº¦æä½ï¼Œæ— æ³•æœ‰æ•ˆæ”¯æ’‘åˆè§„ä¹‰åŠ¡ã€‚", \
                   "éœ€åˆ¶å®šä¸“é¡¹åˆ¶åº¦æˆ–åœ¨ç°æœ‰åˆ¶åº¦ä¸­å¢åŠ ä¸“é—¨ç« èŠ‚ã€‚"

# ==============================================================================
# 2. UI äº¤äº’å±‚ (Streamlit Interface)
# ==============================================================================

def main():
    st.set_page_config(page_title="EHSæ™ºèƒ½åˆè§„è¯„ä»·ç³»ç»Ÿ", layout="wide", page_icon="âš–ï¸")
    
    st.title("âš–ï¸ EHSæ™ºèƒ½åˆè§„è¯„ä»·ç³»ç»Ÿ (ä¸“å®¶ç‰ˆ)")
    st.markdown("""
    æœ¬ç³»ç»Ÿé‡‡ç”¨ **â€œè¯­ä¹‰ç†è§£-å…¨é‡æ£€ç´¢-é—­ç¯éªŒè¯â€** çš„æ·±åº¦æ€ç»´é“¾ï¼Œå¯¹ä¼ä¸šåˆ¶åº¦ä¸å¤–éƒ¨æ³•è§„è¿›è¡Œé€æ¡å¯¹æ ‡è¯„ä»·ã€‚
    """)

    # --- Sidebar: Configuration ---
    with st.sidebar:
        st.header("1. åˆ¶åº¦åº“æ„å»º")
        rule_files = st.file_uploader("ä¸Šä¼ å†…éƒ¨åˆ¶åº¦æ–‡ä»¶ (æ”¯æŒå¤šé€‰)", type=['docx', 'txt'], accept_multiple_files=True)
        
        st.header("2. è¯„ä»·å¯¹è±¡")
        law_file = st.file_uploader("ä¸Šä¼ æ³•è§„æ–‡ä»¶ (å•é€‰)", type=['docx'])
        
        st.info("æç¤ºï¼šæ”¯æŒ docx æ ¼å¼ã€‚ç³»ç»Ÿä¼šè‡ªåŠ¨è§£ææ–‡æ¡£ä¸­çš„æ¡æ¬¾ã€‚")

    # --- Main Area ---
    if rule_files and law_file:
        engine = ComplianceEngine()
        
        # 1. Build Corpus
        with st.spinner('æ­£åœ¨æ„å»ºåˆ¶åº¦çŸ¥è¯†åº“...'):
            corpus_count = engine.load_corpus_from_uploaded_files(rule_files)
        st.success(f"âœ… åˆ¶åº¦åº“æ„å»ºå®Œæˆï¼å…±æ”¶å½• {corpus_count} æ¡ç®¡ç†ç‰‡æ®µã€‚")

        # 2. Process Law
        if st.button("å¼€å§‹åˆè§„è¯„ä»·", type="primary"):
            law_text = engine._extract_text_from_docx_stream(law_file)
            # Regex to split clauses: ç¬¬Xæ¡
            clauses_raw = re.split(r'(ç¬¬[é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+æ¡)', law_text)
            
            results = []
            current_title = ""
            
            # Progress Bar
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            total_parts = len([p for p in clauses_raw if p.strip()])
            processed_count = 0

            # --- Evaluation Loop ---
            for part in clauses_raw:
                if re.match(r'^ç¬¬[é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+æ¡$', part):
                    current_title = part
                elif current_title and part.strip():
                    content = part.strip()
                    processed_count += 1
                    
                    # Update UI
                    progress = min(processed_count / (total_parts/2), 1.0) # Approx
                    progress_bar.progress(progress)
                    status_text.text(f"æ­£åœ¨è¯„ä»·: {current_title}...")

                    # Logic Chain
                    analysis = engine.analyze_clause(content)
                    evidence = []
                    if analysis['is_applicable']:
                        evidence = engine.search_evidence(analysis['search_keywords'])
                    
                    conclusion, gap, suggestion = engine.judge_compliance(content, analysis, evidence)
                    
                    # Format Evidence
                    ev_text = ""
                    if evidence:
                        ev_text = "\n".join([f"[{e['file']}] {e['content'][:50]}..." for e in evidence])
                    else:
                        ev_text = "æ— ç›¸å…³åˆ¶åº¦"

                    results.append({
                        "æ¡æ¬¾å·": current_title,
                        "æ¡æ¬¾å†…å®¹": content,
                        "è¯„ä»·ç»“è®º": conclusion,
                        "å·®è·åˆ†æ": gap,
                        "æ”¹è¿›å»ºè®®": suggestion,
                        "æ”¯æ’‘è¯æ®": ev_text,
                        "is_applicable": analysis['is_applicable']
                    })
                    current_title = ""
            
            progress_bar.progress(100)
            status_text.text("è¯„ä»·å®Œæˆï¼")
            
            # --- 3. Report Generation & Display ---
            df = pd.DataFrame(results)
            
            # Statistics
            total = len(df)
            applicable = df[df['is_applicable'] == True]
            compliant = applicable[applicable['è¯„ä»·ç»“è®º'].str.contains("å®Œå…¨ç¬¦åˆ")]
            partial = applicable[applicable['è¯„ä»·ç»“è®º'].str.contains("éƒ¨åˆ†ç¬¦åˆ")]
            missing = applicable[applicable['è¯„ä»·ç»“è®º'].str.contains("ç¼ºå¤±")]
            
            score = round((len(compliant) * 1 + len(partial) * 0.5) / len(applicable) * 100, 1) if len(applicable) > 0 else 0

            # --- Dashboard ---
            st.divider()
            st.subheader("ğŸ“Š è¯„ä»·ç»“æœæ¦‚è§ˆ")
            
            col1, col2, col3, col4 = st.columns(4)
            col1.metric("åˆè§„å¾—åˆ†", f"{score}åˆ†")
            col2.metric("å®Œå…¨ç¬¦åˆ", f"{len(compliant)}é¡¹")
            col3.metric("éœ€å®Œå–„", f"{len(partial)}é¡¹")
            col4.metric("ç¼ºå¤±/ä¸ç¬¦åˆ", f"{len(missing)}é¡¹", delta_color="inverse")

            # --- Comprehensive Summary (Generating Report Text) ---
            
            # Find top missing keywords
            missing_keywords = []
            for idx, row in missing.iterrows():
                words = re.sub(r'[^\w]', ' ', row['æ¡æ¬¾å†…å®¹']).split()
                valid = [w for w in words if len(w)>2][:3]
                missing_keywords.extend(valid)
            top_risks = [k[0] for k in Counter(missing_keywords).most_common(5)]
            
            report_md = f"""# {law_file.name} åˆè§„è¯„ä»·æŠ¥å‘Š

## ç¬¬ä¸€éƒ¨åˆ†ï¼šæ€»ä½“è¯„ä»·ä¸ç®¡ç†å»ºè®®

### 1. æ•´ä½“æƒ…å†µæ¦‚è§ˆ
æœ¬æ¬¡è¯„ä»·å…±å¯¹ **{total}** ä¸ªæ³•è§„æ¡æ¬¾è¿›è¡Œäº†é€æ¡æ·±åº¦æ‰«æã€‚å…¶ä¸­é€‚ç”¨ä¼ä¸šæ¡æ¬¾ **{len(applicable)}** é¡¹ã€‚
æ•´ä½“åˆè§„å¾—åˆ†ä¸º **{score} åˆ†**ã€‚
*   **åˆè§„äº®ç‚¹**ï¼šåœ¨æ ¸å¿ƒç®¡ç†è¦ç´ ï¼ˆå¦‚{', '.join(list(engine.KEYWORD_MAPPING.keys())[:3])}ï¼‰æ–¹é¢ï¼Œåˆ¶åº¦ä½“ç³»è¾ƒä¸ºå®Œå–„ï¼Œæ”¯æ’‘è¯æ®å……åˆ†ã€‚
*   **é£é™©åˆ†å¸ƒ**ï¼šå‘ç° **{len(missing)}** é¡¹å®Œå…¨ç¼ºå¤±ï¼Œ**{len(partial)}** é¡¹åˆ¶åº¦å­˜åœ¨ç‘•ç–µã€‚

### 2. æ ¸å¿ƒé£é™©é¢†åŸŸ (Top Risks)
ç»æ™ºèƒ½åˆ†æï¼Œä»¥ä¸‹é¢†åŸŸå­˜åœ¨åˆ¶åº¦ç©ºç™½æˆ–ä¸¥é‡ä¸è¶³ï¼Œéœ€é‡ç‚¹å…³æ³¨ï¼š
> **{', '.join(top_risks)}**

### 3. ç³»ç»Ÿæ€§é—®é¢˜è¯Šæ–­
*   **ç®¡ç†é—­ç¯ç¼ºå¤±**ï¼šéƒ¨åˆ†æ¡æ¬¾è™½æœ‰åˆ¶åº¦æåŠï¼Œä½†åœ¨â€œè®°å½•ç•™ç—•â€ã€â€œå®šæœŸæŠ¥å‘Šâ€æˆ–â€œä¸“é¡¹åŸ¹è®­â€ç­‰é—­ç¯ç¯èŠ‚å­˜åœ¨ç¼ºå¤±ã€‚
*   **æ–°æ³•è·Ÿè¿›æ»å**ï¼šé’ˆå¯¹æ³•è§„ä¸­æ–°å¢çš„ç‰¹å®šè¦æ±‚ï¼ˆå¦‚å¿ƒç†ç–å¯¼ã€å®‰è´£é™©ç­‰ï¼‰ï¼Œç°æœ‰è€åˆ¶åº¦å°šæœªåŠæ—¶æ›´æ–°è¦†ç›–ã€‚

### 4. ä¸‹ä¸€æ­¥æ”¹è¿›å»ºè®®
1.  **å¡«è¡¥ç©ºç™½**ï¼šé’ˆå¯¹ä¸Šè¿°æ ¸å¿ƒé£é™©é¢†åŸŸï¼Œç«‹å³åˆ¶å®šä¸“é¡¹ç®¡ç†è§„å®šã€‚
2.  **ç»†åŒ–æ‰§è¡Œ**ï¼šå¯¹åˆ¤å®šä¸ºâ€œéœ€å®Œå–„â€çš„æ¡æ¬¾ï¼Œä¿®è®¢å¯¹åº”åˆ¶åº¦ï¼Œå¢åŠ å…·ä½“çš„æ‰§è¡Œé¢‘ç‡ã€è´£ä»»å²—ä½å’Œè®°å½•è¡¨å•ã€‚
3.  **åˆè§„å®¡æŸ¥**ï¼šå»ºè®®æ¯åŠå¹´è¿›è¡Œä¸€æ¬¡åˆ¶åº¦ä¸æ³•è§„çš„å¯¹æ ‡å®¡æŸ¥ã€‚

---
## ç¬¬äºŒéƒ¨åˆ†ï¼šé€æ¡è¯„ä»·æ˜ç»†è¡¨
"""
            
            st.markdown("### ğŸ“ è¯„ä»·æŠ¥å‘Šæ‘˜è¦")
            st.markdown(report_md)

            # --- Data Table ---
            st.subheader("ğŸ” é€æ¡æ˜ç»†é¢„è§ˆ")
            st.dataframe(df[['æ¡æ¬¾å·', 'æ¡æ¬¾å†…å®¹', 'è¯„ä»·ç»“è®º', 'å·®è·åˆ†æ', 'æ”¹è¿›å»ºè®®']])

            # --- Downloads ---
            st.subheader("ğŸ“¥ æŠ¥å‘Šä¸‹è½½")
            
            # Generate Full Markdown
            full_md = report_md + "\n" + df.to_markdown(index=False)
            
            # Generate Excel
            output = BytesIO()
            with pd.ExcelWriter(output, engine='openpyxl') as writer:
                df.to_excel(writer, index=False, sheet_name='åˆè§„è¯„ä»·è¡¨')
                # Add a summary sheet
                summary_df = pd.DataFrame({
                    "æŒ‡æ ‡": ["æ€»æ¡æ¬¾æ•°", "é€‚ç”¨æ¡æ¬¾æ•°", "å¾—åˆ†", "å®Œå…¨ç¬¦åˆ", "éœ€å®Œå–„", "ç¼ºå¤±"],
                    "æ•°å€¼": [total, len(applicable), score, len(compliant), len(partial), len(missing)]
                })
                summary_df.to_excel(writer, index=False, sheet_name='æ¦‚è§ˆ')
            
            col_d1, col_d2 = st.columns(2)
            with col_d1:
                st.download_button("ä¸‹è½½å®Œæ•´ Word/Markdown æŠ¥å‘Š", full_md, file_name=f"åˆè§„è¯„ä»·æŠ¥å‘Š_{law_file.name}.md")
            with col_d2:
                st.download_button("ä¸‹è½½ Excel æ˜ç»†è¡¨", output.getvalue(), file_name=f"åˆè§„è¯„ä»·æ˜ç»†_{law_file.name}.xlsx")

    else:
        st.info("ğŸ‘ˆ è¯·åœ¨å·¦ä¾§ä¾§è¾¹æ ä¸Šä¼  åˆ¶åº¦æ–‡ä»¶ å’Œ æ³•è§„æ–‡ä»¶ ä»¥å¼€å§‹ã€‚")

if __name__ == "__main__":
    main()
