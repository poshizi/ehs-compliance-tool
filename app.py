import streamlit as st
import pandas as pd
import os
import re
import zipfile
import io
import time
import requests
import json
import numpy as np
import docx 
from concurrent.futures import ThreadPoolExecutor, as_completed
from tenacity import retry, stop_after_attempt, wait_exponential

# ====================
# Core Classes for AI & Data
# ====================

class LLMClient:
    """å¤„ç†ä¸å¤§æ¨¡å‹çš„äº¤äº’ (Embedding å’Œ Chat)"""
    def __init__(self, config):
        self.base_url = config.get('base_url', '').rstrip('/')
        self.api_key = config.get('api_key')
        self.model = config.get('model')
        self.embedding_model = config.get('embedding_model', 'text-embedding-004')
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        self.embedding_failed = False 

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
    def get_embedding(self, text):
        """è·å–æ–‡æœ¬çš„å‘é‡è¡¨ç¤º"""
        if self.embedding_failed: return None
        
        payload = {
            "input": text.replace("\n", " "),
            "model": self.embedding_model
        }
        
        # å°è¯•æ ‡å‡† OpenAI è·¯å¾„
        url = f"{self.base_url}/embeddings"
        
        try:
            response = requests.post(url, headers=self.headers, json=payload, timeout=10)
            if response.status_code == 200:
                data = response.json()
                if 'data' in data and len(data['data']) > 0:
                    return data['data'][0]['embedding']
                else:
                    return None
            else:
                print(f"Embedding failed: {response.status_code} - {response.text}")
                return None
        except Exception as e:
            print(f"Embedding error: {e}")
            return None

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
    def chat_completion(self, system_prompt, user_prompt):
        """è°ƒç”¨ Chat æ¥å£"""
        payload = {
            "model": self.model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            "temperature": 0.1,
            "response_format": {"type": "json_object"}
        }
        
        try:
            response = requests.post(f"{self.base_url}/chat/completions", headers=self.headers, json=payload, timeout=60)
            if response.status_code == 200:
                content = response.json()['choices'][0]['message']['content']
                content = content.replace("```json", "").replace("```", "")
                return json.loads(content)
            else:
                raise Exception(f"API Error {response.status_code}: {response.text}")
        except Exception as e:
            raise Exception(f"Request failed: {str(e)}")
            
    def test_connection(self):
        """æµ‹è¯•è¿æ¥çŠ¶æ€"""
        results = {"chat": False, "embedding": False, "msg": ""}
        
        # 1. æµ‹è¯• Chat
        try:
            self.chat_completion("You are a test bot.", "Reply JSON: {'status': 'ok'}")
            results['chat'] = True
        except Exception as e:
            results['msg'] += f"Chat Error: {str(e)}\n"
            
        # 2. æµ‹è¯• Embedding
        try:
            emb = self.get_embedding("test")
            if emb:
                results['embedding'] = True
            else:
                results['msg'] += "Embedding Error: Returned None (Check model name or API support)\n"
        except Exception as e:
             results['msg'] += f"Embedding Exception: {str(e)}\n"
             
        return results

class VectorStore:
    """æ··åˆæ£€ç´¢æ•°æ®åº“ (å‘é‡ + å…³é”®è¯)"""
    def __init__(self):
        self.documents = [] 
        self.vectors = []   
        self.llm_client = None

    def set_client(self, client):
        self.llm_client = client

    def add_documents(self, file_corpus):
        """å¤„ç†å¹¶å…¥åº“"""
        chunk_size = 500
        overlap = 50 
        
        self.documents = []
        texts_to_embed = []
        
        doc_id = 0
        for file in file_corpus:
            text = file['content']
            name = file['name']
            
            for i in range(0, len(text), chunk_size - overlap):
                chunk = text[i:i + chunk_size]
                if len(chunk) < 50: continue 
                
                # å…³é”®è¯æå–
                keywords = set(re.split(r'[ï¼Œã€‚ï¼›ï¼š\s]', chunk))
                keywords = [k for k in keywords if len(k) > 1]

                self.documents.append({
                    'id': doc_id,
                    'text': chunk,
                    'source': name,
                    'keywords': keywords 
                })
                texts_to_embed.append(chunk)
                doc_id += 1
        
        # å°è¯•å‘é‡åŒ–
        if self.llm_client:
            with st.status("æ­£åœ¨æ„å»ºç´¢å¼• (å°è¯•å‘é‡åŒ– + å…³é”®è¯åº“)...") as status:
                valid_vectors = []
                # ä½¿ç”¨å¹¶å‘
                with ThreadPoolExecutor(max_workers=5) as executor:
                    future_to_idx = {executor.submit(self.llm_client.get_embedding, t): i for i, t in enumerate(texts_to_embed)}
                    
                    results = [None] * len(texts_to_embed)
                    success_count = 0
                    
                    for future in as_completed(future_to_idx):
                        idx = future_to_idx[future]
                        vec = future.result()
                        results[idx] = vec
                        if vec is not None: success_count += 1
                
                self.vectors = results 
                
                if success_count == 0:
                    status.update(label="âš ï¸ å‘é‡åŒ–å…¨éƒ¨å¤±è´¥ (å°†é™çº§ä½¿ç”¨å…³é”®è¯æ£€ç´¢)", state="error")
                    st.warning("æç¤ºï¼šEmbedding API è°ƒç”¨å¤±è´¥ï¼Œå·²è‡ªåŠ¨åˆ‡æ¢ä¸º **å…³é”®è¯åŒ¹é…æ¨¡å¼**ã€‚è¯·æ£€æŸ¥ Embedding Model é…ç½®ã€‚" )
                else:
                    status.update(label=f"ç´¢å¼•æ„å»ºå®Œæˆ (å‘é‡åŒ–æˆåŠŸç‡: {success_count}/{len(texts_to_embed)})", state="complete")

    def search(self, query_text, top_k=3):
        """æ··åˆæ£€ç´¢"""
        vec_results = []
        
        # 1. å‘é‡æ£€ç´¢
        has_vectors = any(v is not None for v in self.vectors)
        if has_vectors and self.llm_client:
            query_vec = self.llm_client.get_embedding(query_text)
            if query_vec is not None:
                q_v = np.array(query_vec)
                norm_q = np.linalg.norm(q_v)
                if norm_q > 0:
                    scores = []
                    for i, doc_vec in enumerate(self.vectors):
                        if doc_vec is None: 
                            scores.append(-1)
                            continue
                        d_v = np.array(doc_vec)
                        norm_d = np.linalg.norm(d_v)
                        if norm_d == 0: scores.append(0)
                        else: scores.append(np.dot(d_v, q_v) / (norm_d * norm_q))
                    
                    top_indices = np.argsort(scores)[-top_k:][::-1]
                    for idx in top_indices:
                        if scores[idx] > 0:
                            vec_results.append({'doc': self.documents[idx], 'score': float(scores[idx]), 'method': 'vector'})

        # 2. å…³é”®è¯æ£€ç´¢ (å…œåº•)
        kw_results = []
        query_keywords = [k for k in re.split(r'[ï¼Œã€‚ï¼›ï¼š\s]', query_text) if len(k) > 1]
        
        for doc in self.documents:
            overlap = sum(1 for k in query_keywords if k in doc['text'])
            if overlap > 0:
                score = overlap / (len(query_keywords) + 1) * 0.8 
                kw_results.append({'doc': doc, 'score': score, 'method': 'keyword'})
        
        kw_results.sort(key=lambda x: x['score'], reverse=True)
        kw_results = kw_results[:top_k]

        combined = vec_results + kw_results
        seen_ids = set()
        final_results = []
        combined.sort(key=lambda x: x['score'], reverse=True)
        
        for res in combined:
            did = res['doc']['id']
            if did not in seen_ids:
                final_results.append({'source': res['doc']['source'], 'content': res['doc']['text'], 'score': res['score']})
                seen_ids.add(did)
            if len(final_results) >= top_k: break
                
        return final_results

# ====================
# Helper Functions
# ====================

def process_uploaded_files(uploaded_files):
    for uploaded_file in uploaded_files:
        if uploaded_file.name.endswith('.zip'):
            try:
                with zipfile.ZipFile(uploaded_file) as z:
                    for filename in z.namelist():
                        if filename.endswith('/') or filename.startswith('__MACOSX') or filename.startswith('._'): continue
                        if filename.endswith(('.docx', '.xlsx')):
                            with z.open(filename) as f: yield filename, f.read()
            except Exception as e: st.error(f"è§£å‹å¤±è´¥: {e}")
        else: yield uploaded_file.name, uploaded_file.getvalue()

def extract_text_from_content(filename, content):
    text = ""
    try:
        file_stream = io.BytesIO(content)
        if filename.endswith('.docx'):
            doc = docx.Document(file_stream)
            text = '\n'.join([para.text for para in doc.paragraphs])
        elif filename.endswith('.xlsx'):
            df_dict = pd.read_excel(file_stream, sheet_name=None, header=None)
            text_parts = []
            for sheet_name, df in df_dict.items():
                sheet_text = df.astype(str).apply(lambda x: ' '.join(x), axis=1)
                text_parts.append('\n'.join(sheet_text))
            text = '\n'.join(text_parts)
    except Exception as e: print(f"Error parsing {filename}: {e}")
    return text

def parse_regulation_clauses(text):
    pattern = r'(ç¬¬\s*[\dé›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+\s*æ¡|Article\s+\d+)'
    parts = re.split(pattern, text)
    clauses = []
    if len(parts) > 1:
        for i in range(1, len(parts), 2):
            title = parts[i].strip()
            content = parts[i+1].strip() if i+1 < len(parts) else ""
            applicability = "é€‚ç”¨"
            gov_keywords = ["å›½åŠ¡é™¢", "å¿çº§ä»¥ä¸Š", "ç›‘å¯Ÿæœºå…³", "äººæ°‘æ”¿åºœ", "ä¸»ç®¡éƒ¨é—¨", "è¡Œæ”¿æœºå…³"]
            corp_keywords = ["ç”Ÿäº§ç»è¥å•ä½", "ä¼ä¸š", "ç”¨äººå•ä½", "å»ºè®¾å•ä½", "å…¬å¸"]
            content_lower = content.lower()
            is_gov = any(k in content_lower for k in gov_keywords)
            is_corp = any(k in content_lower for k in corp_keywords)
            if is_gov and not is_corp: applicability = "ä¸é€‚ç”¨(æ”¿åºœèŒè´£)"
            clauses.append({"æ¡æ¬¾å·": title, "æ³•è§„æ­£æ–‡": title + " " + content, "é€‚ç”¨æ€§": applicability})
    return clauses

def evaluate_single_clause(clause, vector_store, llm_client):
    row = {"æ¡æ¬¾å·": clause['æ¡æ¬¾å·'], "æ³•è§„æ­£æ–‡": clause['æ³•è§„æ­£æ–‡'], "è¯„ä»·ç»“è®º": "âŒç¼ºå¤±/ä¸ç¬¦åˆ", "æ”¯æ’‘è¯æ®": "æœªæ£€ç´¢åˆ°ç›¸å…³åˆ¶åº¦", "åŒ¹é…åº¦": 0.0}
    if clause['é€‚ç”¨æ€§'] != "é€‚ç”¨":
        row['è¯„ä»·ç»“è®º'] = "â—ä¸é€‚ç”¨"
        row['æ”¯æ’‘è¯æ®'] = "æ¡æ¬¾ä¸»ä½“éä¼ä¸š"
        return row

    search_results = vector_store.search(clause['æ³•è§„æ­£æ–‡'], top_k=3)
    if not search_results: return row
    
    top_score = search_results[0]['score']
    row['åŒ¹é…åº¦'] = top_score
    if top_score < 0.15:
        row['è¯„ä»·ç»“è®º'] = "âŒç¼ºå¤±/ä¸ç¬¦åˆ"
        row['æ”¯æ’‘è¯æ®'] = f"æœªæ‰¾åˆ°åŒ¹é…åˆ¶åº¦ (æœ€é«˜åŒ¹é…åº¦ {top_score:.2f} ä½äºé˜ˆå€¼)"
        return row
        
    evidence_text = ""
    for i, res in enumerate(search_results):
        evidence_text += f"ç‰‡æ®µ {i+1} (ç›¸ä¼¼åº¦: {res['score']:.2f}):\n{res['content']}\n---\n"
    
    system_prompt = "ä½ æ˜¯ä¸€ä¸ªEHSåˆè§„ä¸“å®¶ã€‚è¯·å¯¹æ¯”æ³•è§„æ¡æ¬¾å’Œä¼ä¸šåˆ¶åº¦ï¼Œåˆ¤æ–­æ˜¯å¦åˆè§„ã€‚"
    user_prompt = f"ã€æ³•è§„æ¡æ¬¾ã€‘\n{clause['æ³•è§„æ­£æ–‡']}\n\nã€ä¼ä¸šåˆ¶åº¦å‚è€ƒç‰‡æ®µã€‘\n{evidence_text}\n\nè¯·åŸºäºä¸Šè¿°ç‰‡æ®µåˆ¤æ–­ã€‚è‹¥ç¬¦åˆï¼Œå¼•ç”¨åŸæ–‡ã€‚\nè¿”å›JSON:\n{{\n    "status": "âœ…å®Œå…¨ç¬¦åˆ" æˆ– "âš ï¸éƒ¨åˆ†ç¬¦åˆ/éœ€å®Œå–„" æˆ– "âŒç¼ºå¤±/ä¸ç¬¦åˆ",\n    "evidence": "åˆ¶åº¦åŸæ–‡å¼•ç”¨",\n    "reason": "åˆ¤å®šç†ç”±"\n}}"
    
    try:
        result = llm_client.chat_completion(system_prompt, user_prompt)
        row['è¯„ä»·ç»“è®º'] = result.get('status', 'âŒç¼ºå¤±/ä¸ç¬¦åˆ')
        row['æ”¯æ’‘è¯æ®'] = f"{result.get('evidence', '')}\n(AIç†ç”±: {result.get('reason', '')})"
    except Exception as e: row['æ”¯æ’‘è¯æ®'] = f"LLMåˆ†æå¤±è´¥: {str(e)}"
    return row

# ====================
# Streamlit UI
# ====================

st.set_page_config(page_title="EHSæ™ºèƒ½åˆè§„å¼•æ“ (Hybridç‰ˆ)", layout="wide")

if 'results' not in st.session_state: st.session_state.results = None

st.title("ğŸ›¡ï¸ EHSæ³•è§„åˆè§„æ€§æ™ºèƒ½è¯„ä»·å¼•æ“ (Hybrid Pro)")
st.markdown("ğŸš€ **æŠ€æœ¯æ ˆ**: `Embedding` + `Hybrid Search` + `Concurrency`")

with st.sidebar:
    st.header("1. API é…ç½®")
    llm_base_url = st.text_input("API Base URL", value="https://generativelanguage.googleapis.com/v1beta/openai", help="ä¾‹å¦‚ https://api.openai.com/v1")
    llm_api_key = st.text_input("API Key", type="password")
    
    col_m1, col_m2 = st.columns(2)
    with col_m1:
        llm_model_name = st.text_input("Chat Model", value="gemini-2.0-flash")
    with col_m2:
        # æ–°å¢ Embedding Model é€‰æ‹©
        embedding_model_name = st.text_input("Embedding Model", value="text-embedding-004", help="ä¾‹å¦‚ text-embedding-3-small")

    # æ–°å¢æµ‹è¯•æŒ‰é’®
    if st.button("ğŸ”Œ æµ‹è¯• API è¿æ¥", use_container_width=True):
        if not llm_api_key:
            st.error("è¯·å…ˆå¡«å†™ API Key")
        else:
            with st.spinner("æ­£åœ¨æµ‹è¯•è¿æ¥..."):
                test_config = {
                    "base_url": llm_base_url,
                    "api_key": llm_api_key,
                    "model": llm_model_name,
                    "embedding_model": embedding_model_name
                }
                client = LLMClient(test_config)
                res = client.test_connection()
                
                if res['chat']: st.success(f"âœ… Chat Model ({llm_model_name}): è¿æ¥æˆåŠŸ")
                else: st.error(f"âŒ Chat Model è¿æ¥å¤±è´¥")
                
                if res['embedding']: st.success(f"âœ… Embedding Model ({embedding_model_name}): è¿æ¥æˆåŠŸ")
                else: st.error(f"âŒ Embedding Model è¿æ¥å¤±è´¥ (ç³»ç»Ÿå°†è‡ªåŠ¨é™çº§ä¸ºå…³é”®è¯æ£€ç´¢)")
                
                if res['msg']: st.code(res['msg'], language="text")

    st.divider()
    st.header("2. æ–‡ä»¶ä¸Šä¼ ")
    reg_files = st.file_uploader("ä¸Šä¼ æ³•è§„ (docx/zip)", type=['docx', 'zip'], accept_multiple_files=True, key="reg")
    policy_files = st.file_uploader("ä¸Šä¼ åˆ¶åº¦ (docx/xlsx/zip)", type=['docx', 'xlsx', 'zip'], accept_multiple_files=True, key="pol")

if st.button("ğŸš€ å¼€å§‹æé€Ÿåˆ†æ", type="primary"):
    if not (reg_files and policy_files and llm_api_key):
        st.error("è¯·ç¡®ä¿æ–‡ä»¶å·²ä¸Šä¼ ä¸” API Key å·²å¡«å†™ã€‚" )
    else:
        llm_config = {
            "base_url": llm_base_url, 
            "api_key": llm_api_key, 
            "model": llm_model_name,
            "embedding_model": embedding_model_name
        }
        client = LLMClient(llm_config)
        vector_store = VectorStore()
        vector_store.set_client(client)
        
        policy_corpus = []
        for name, content in process_uploaded_files(policy_files):
            text = extract_text_from_content(name, content)
            if text and len(text.strip()) > 0: policy_corpus.append({'name': name, 'content': text})
            else: st.warning(f"æ–‡ä»¶ {name} å†…å®¹ä¸ºç©ºï¼Œå·²è·³è¿‡ã€‚" )
            
        if not policy_corpus:
            st.error("æœ‰æ•ˆåˆ¶åº¦å†…å®¹ä¸ºç©ºã€‚" )
            st.stop()
            
        vector_store.add_documents(policy_corpus)
        
        all_clauses = []
        for name, content in process_uploaded_files(reg_files):
            text = extract_text_from_content(name, content)
            clauses = parse_regulation_clauses(text)
            for c in clauses:
                c['source_file'] = name 
                all_clauses.append(c)
        
        if not all_clauses:
             st.error("æœªè§£æå‡ºä»»ä½•æ³•è§„æ¡æ¬¾ã€‚" )
             st.stop()

        st.info(f"å…± {len(all_clauses)} æ¡æ¡æ¬¾ï¼Œå¼€å§‹åˆ†æ...")
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        results_list = []
        total_tasks = len(all_clauses)
        completed_tasks = 0
        
        with ThreadPoolExecutor(max_workers=10) as executor:
            future_to_clause = {executor.submit(evaluate_single_clause, clause, vector_store, client): clause for clause in all_clauses}
            for future in as_completed(future_to_clause):
                try:
                    res = future.result()
                    res['æ³•è§„æ–‡ä»¶'] = future_to_clause[future]['source_file']
                    results_list.append(res)
                except Exception as exc: st.warning(f"åˆ†æå¼‚å¸¸: {exc}")
                completed_tasks += 1
                progress_bar.progress(completed_tasks / total_tasks)
                status_text.text(f"å·²å®Œæˆ: {completed_tasks}/{total_tasks} ...")
                
        st.success("åˆ†æå®Œæˆï¼")
        st.session_state.results = pd.DataFrame(results_list)

if st.session_state.results is not None:
    df = st.session_state.results
    st.divider()
    st.subheader("ğŸ“Š ç»“æœçœ‹æ¿")
    col1, col2, col3 = st.columns(3)
    col1.metric("å®Œå…¨ç¬¦åˆ", len(df[df['è¯„ä»·ç»“è®º']=="âœ…å®Œå…¨ç¬¦åˆ"]))
    col2.metric("éœ€å®Œå–„", len(df[df['è¯„ä»·ç»“è®º']=="âš ï¸éƒ¨åˆ†ç¬¦åˆ/éœ€å®Œå–„"]))
    col3.metric("ç¼ºå¤±/ä¸ç¬¦åˆ", len(df[df['è¯„ä»·ç»“è®º'].str.contains("ç¼ºå¤±|ä¸ç¬¦åˆ")]))
    
    status_filter = st.multiselect("ç­›é€‰ç»“è®º", df['è¯„ä»·ç»“è®º'].unique(), default=df['è¯„ä»·ç»“è®º'].unique())
    show_df = df[df['è¯„ä»·ç»“è®º'].isin(status_filter)]
    st.dataframe(show_df, column_config={"æ³•è§„æ­£æ–‡": st.column_config.TextColumn("æ³•è§„è¦æ±‚", width="medium"), "æ”¯æ’‘è¯æ®": st.column_config.TextColumn("è¯æ®", width="large"), "åŒ¹é…åº¦": st.column_config.ProgressColumn("åŒ¹é…åº¦", min_value=0, max_value=1, format="%.2f")}, use_container_width=True, height=600)
    csv = df.to_csv(index=False).encode('utf-8-sig')
    st.download_button("ğŸ“¥ ä¸‹è½½è¯¦ç»†æŠ¥è¡¨ (CSV)", csv, "ehs_compliance_report.csv", "text/csv")