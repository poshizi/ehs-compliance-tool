import streamlit as st
import pandas as pd
import os
import re
import zipfile
import io
import time

# ====================
# æ ¸å¿ƒé€»è¾‘å‡½æ•° (å¤ç”¨ä¸“å®¶ç»éªŒ)
# ====================

def process_uploaded_files(uploaded_files):
    """
    ç”Ÿæˆå™¨ï¼šå¤„ç†ä¸Šä¼ çš„æ–‡ä»¶åˆ—è¡¨ï¼Œè‡ªåŠ¨è§£å‹zipåŒ…
    Yields: (filename, content_bytes)
    """
    for uploaded_file in uploaded_files:
        if uploaded_file.name.endswith('.zip'):
            try:
                with zipfile.ZipFile(uploaded_file) as z:
                    for filename in z.namelist():
                        # è·³è¿‡æ–‡ä»¶å¤¹å’Œéšè—æ–‡ä»¶
                        if filename.endswith('/') or filename.startswith('__MACOSX') or filename.startswith('._'):
                            continue
                        if filename.endswith(('.docx', '.xlsx')):
                            with z.open(filename) as f:
                                yield filename, f.read()
            except Exception as e:
                st.error(f"è§£å‹æ–‡ä»¶ {uploaded_file.name} å¤±è´¥: {str(e)}")
        else:
            yield uploaded_file.name, uploaded_file.getvalue()

def extract_text_from_content(filename, content):
    """ä»æ–‡ä»¶å†…å®¹ä¸­æå–çº¯æ–‡æœ¬"""
    text = ""
    try:
        if filename.endswith('.docx'):
            with zipfile.ZipFile(io.BytesIO(content)) as zf:
                xml = zf.read('word/document.xml').decode('utf-8')
                text = re.sub(r'<[^>]+>', '', xml)
        elif filename.endswith('.xlsx'):
            with zipfile.ZipFile(io.BytesIO(content)) as zf:
                if 'xl/sharedStrings.xml' in zf.namelist():
                    xml = zf.read('xl/sharedStrings.xml').decode('utf-8')
                    text = re.sub(r'<[^>]+>', '', xml)
    except Exception as e:
        st.error(f"è§£ææ–‡ä»¶ {filename} å¤±è´¥: {str(e)}")
    return text

def parse_regulation_clauses(text):
    """å°†æ³•è§„æ–‡æœ¬æ‹†è§£ä¸ºæ¡æ¬¾åˆ—è¡¨"""
    # åŒ¹é… "ç¬¬Xæ¡" çš„æ¨¡å¼ï¼Œæ”¯æŒä¸­æ–‡æ•°å­—
    pattern = r'(ç¬¬[é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+æ¡)'
    parts = re.split(pattern, text)
    
    clauses = []
    if len(parts) > 1:
        # parts[0] æ˜¯å‰è¨€ï¼Œparts[1]æ˜¯"ç¬¬ä¸€æ¡", parts[2]æ˜¯å†…å®¹...
        for i in range(1, len(parts), 2):
            title = parts[i]
            content = parts[i+1].strip() if i+1 < len(parts) else ""
            
            # ç®€å•çš„é€‚ç”¨æ€§åˆ¤æ–­é€»è¾‘ï¼ˆæ’é™¤çº¯æ”¿åºœèŒè´£ï¼‰
            applicability = "é€‚ç”¨"
            gov_keywords = ["å›½åŠ¡é™¢", "å¿çº§ä»¥ä¸Š", "ç›‘å¯Ÿæœºå…³", "äººæ°‘æ”¿åºœ", "ä¸»ç®¡éƒ¨é—¨"]
            # å¦‚æœä¸»è¦æ˜¯åœ¨è®²æ”¿åºœåº”è¯¥åšä»€ä¹ˆï¼Œä¸”æ²¡æœ‰æåŠâ€œç”Ÿäº§ç»è¥å•ä½â€
            if any(k in content[:20] for k in gov_keywords) and "ç”Ÿäº§ç»è¥å•ä½" not in content[:50]:
                applicability = "ä¸é€‚ç”¨(æ”¿åºœèŒè´£)"
            
            full_text = title + " " + content
            clauses.append({
                "æ¡æ¬¾å·": title,
                "æ³•è§„æ­£æ–‡": full_text,
                "é€‚ç”¨æ€§": applicability
            })
    return clauses

def calculate_match_score(clause_text, policy_text):
    """è®¡ç®—åŒ¹é…åº¦å¾—åˆ† (åŸºäºç®€å•çš„å…³é”®è¯é‡å )"""
    # ç®€å•çš„åˆ†è¯ï¼šæŒ‰æ ‡ç‚¹ç¬¦å·åˆ†å‰²
    keywords = re.split(r'[ï¼Œã€‚ï¼›ï¼šã€â€œâ€]', clause_text)
    keywords = [k for k in keywords if len(k) > 2] # ä»…ä¿ç•™æœ‰æ„ä¹‰çš„è¯
    
    score = 0
    matched_words = []
    
    for k in keywords:
        if k in policy_text:
            score += len(k)
            matched_words.append(k)
            
    return score, list(set(matched_words))

def generate_markdown_report(summary_data, df_result):
    """ç”Ÿæˆ Markdown æ ¼å¼çš„æŠ¥å‘Š"""
    report = f"""# EHSæ³•è§„åˆè§„æ€§è¯„ä»·æŠ¥å‘Š

**è¯„ä»·æ—¥æœŸ**: {time.strftime("%Y-%m-%d")}

## ç¬¬ä¸€éƒ¨åˆ†ï¼šæ€»ä½“è¯„ä»·

**1. è¯„ä»·æ¦‚å†µ**
*   **åˆ†ææ¡æ¬¾æ€»æ•°**: {summary_data['total']}
*   **å®Œå…¨ç¬¦åˆæ¡æ¬¾æ•°**: {summary_data['compliant']}
*   **éƒ¨åˆ†ç¬¦åˆ/éœ€å®Œå–„æ¡æ¬¾æ•°**: {summary_data['partial']}
*   **ä¸é€‚ç”¨/ç¼ºå¤±æ¡æ¬¾æ•°**: {summary_data['non_compliant']}
*   **æ€»ä½“åˆè§„ç‡**: {summary_data['compliance_rate']:.1f}% (å®Œå…¨ç¬¦åˆ + éƒ¨åˆ†ç¬¦åˆ)

**2. è¯„ä»·ç»“è®ºç»¼è¿°**
æœ¬æ¬¡è¯„ä»·é’ˆå¯¹ä¸Šä¼ çš„æ³•è§„æ–‡ä»¶ä¸ä¼ä¸šå†…éƒ¨åˆ¶åº¦è¿›è¡Œäº†è‡ªåŠ¨æ¯”å¯¹ã€‚
{ "æ€»ä½“åˆè§„æƒ…å†µè‰¯å¥½ã€‚" if summary_data['compliance_rate'] > 80 else "å­˜åœ¨ä¸€å®šåˆè§„é£é™©ï¼Œå»ºè®®é‡ç‚¹å…³æ³¨ç¼ºå¤±å’Œéƒ¨åˆ†ç¬¦åˆçš„æ¡æ¬¾ã€‚" }

---

## ç¬¬äºŒéƒ¨åˆ†ï¼šè¯¦ç»†åˆè§„æ€§è¯„ä»·çŸ©é˜µ

| åºå· | æ³•è§„æ–‡ä»¶ | æ¡æ¬¾å· | è¯„ä»·ç»“è®º | æ”¯æ’‘è¯æ® |
| :--- | :--- | :--- | :--- | :--- |
"""
    for _, row in df_result.iterrows():
        # æ¸…ç†æ¢è¡Œç¬¦ä»¥å…ç ´åè¡¨æ ¼æ ¼å¼
        evidence = str(row['æ”¯æ’‘è¯æ®']).replace('\n', '<br>').replace('|', '\|')
        # æˆªæ–­è¿‡é•¿çš„è¯æ®
        if len(evidence) > 100:
            evidence = evidence[:100] + "..."
            
        report += f"| {row['åºå·']} | {row['æ³•è§„æ–‡ä»¶']} | {row['æ¡æ¬¾å·']} | {row['è¯„ä»·ç»“è®º']} | {evidence} |\n"
        
    return report

def analyze_compliance(reg_files, policy_files, progress_bar, status_text):
    """æ‰§è¡Œåˆè§„æ€§åˆ†æçš„ä¸»æµç¨‹"""
    
    # 1. é¢„å¤„ç†åˆ¶åº¦æ–‡ä»¶åº“
    status_text.text("æ­£åœ¨æ„å»ºåˆ¶åº¦çŸ¥è¯†åº“...")
    policy_corpus = []
    
    # ä½¿ç”¨ process_uploaded_files å¤„ç†æ–‡ä»¶ï¼Œå¯èƒ½åŒ…å«è§£å‹åçš„å¤šä¸ªæ–‡ä»¶
    processed_policies = list(process_uploaded_files(policy_files))
    total_policies = len(processed_policies)
    
    for idx, (p_name, p_content) in enumerate(processed_policies):
        p_text = extract_text_from_content(p_name, p_content)
        if p_text:
            policy_corpus.append({
                "name": p_name,
                "content": p_text
            })
        progress_bar.progress((idx + 1) / total_policies * 0.1) # é¢„å¤„ç†å 10%è¿›åº¦

    all_results = []
    
    # 2. é€ä¸ªåˆ†ææ³•è§„æ–‡ä»¶
    processed_regs = list(process_uploaded_files(reg_files))
    
    for r_name, r_content in processed_regs:
        r_text = extract_text_from_content(r_name, r_content)
        clauses = parse_regulation_clauses(r_text)
        
        total_clauses = len(clauses)
        if total_clauses == 0:
            st.warning(f"æ–‡ä»¶ {r_name} æœªè¯†åˆ«åˆ°æœ‰æ•ˆæ¡æ¬¾ï¼Œè¯·ç¡®è®¤æ ¼å¼ã€‚")
            continue
            
        current_results = []
        
        for idx, clause in enumerate(clauses):
            # æ›´æ–°è¿›åº¦æ¡
            progress = 0.1 + ((idx + 1) / total_clauses * 0.9)
            progress_bar.progress(progress)
            status_text.text(f"æ­£åœ¨åˆ†æ {r_name}: {clause['æ¡æ¬¾å·']}...")
            
            row = {
                "åºå·": idx + 1,
                "æ³•è§„æ–‡ä»¶": r_name,
                "æ¡æ¬¾å·": clause['æ¡æ¬¾å·'],
                "æ³•è§„æ­£æ–‡": clause['æ³•è§„æ­£æ–‡'],
                "è¯„ä»·ç»“è®º": "âŒç¼ºå¤±/ä¸ç¬¦åˆ", # é»˜è®¤
                "æ”¯æ’‘è¯æ®": "æœªæ£€ç´¢åˆ°ç›¸å…³åˆ¶åº¦",
                "åŒ¹é…åº¦": 0
            }
            
            if clause['é€‚ç”¨æ€§'] != "é€‚ç”¨":
                row['è¯„ä»·ç»“è®º'] = "â—ä¸é€‚ç”¨"
                row['æ”¯æ’‘è¯æ®'] = "æ¡æ¬¾ä¸»ä½“éä¼ä¸š"
            else:
                # åœ¨åˆ¶åº¦åº“ä¸­å¯»æ‰¾æœ€ä½³åŒ¹é…
                best_score = 0
                best_match = None
                best_keywords = []
                
                for policy in policy_corpus:
                    score, keywords = calculate_match_score(clause['æ³•è§„æ­£æ–‡'], policy['content'])
                    if score > best_score:
                        best_score = score
                        best_match = policy
                        best_keywords = keywords
                
                # åˆ¤å®šé€»è¾‘
                if best_score > 0:
                    row['åŒ¹é…åº¦'] = best_score
                    # æå–åŒ¹é…ç‰‡æ®µ
                    idx = best_match['content'].find(best_keywords[0])
                    start = max(0, idx - 20)
                    end = min(len(best_match['content']), idx + 100)
                    snippet = best_match['content'][start:end] + "..."
                    
                    row['æ”¯æ’‘è¯æ®'] = f"[{best_match['name']}]\nç›¸å…³å†…å®¹: ...{snippet}"
                    
                    if best_score > 30: # é˜ˆå€¼å¯è°ƒ
                        row['è¯„ä»·ç»“è®º'] = "âœ…å®Œå…¨ç¬¦åˆ"
                    elif best_score > 10:
                        row['è¯„ä»·ç»“è®º'] = "âš ï¸éƒ¨åˆ†ç¬¦åˆ/éœ€å®Œå–„"
                
            current_results.append(row)
        
        all_results.extend(current_results)
        
    return pd.DataFrame(all_results)

# ====================
# Streamlit UI ç•Œé¢
# ====================

st.set_page_config(page_title="EHSåˆè§„æ€§æ™ºèƒ½è¯„ä»·åŠ©æ‰‹", layout="wide")

st.title("ğŸ›¡ï¸ EHSæ³•è§„åˆè§„æ€§æ™ºèƒ½è¯„ä»·åŠ©æ‰‹")
st.markdown("""
æœ¬å·¥å…·ç”¨äºè‡ªåŠ¨æ¯”å¯¹ **å¤–éƒ¨æ³•è§„** ä¸ **å†…éƒ¨åˆ¶åº¦**ï¼Œç”Ÿæˆåˆè§„æ€§è¯„ä»·çŸ©é˜µã€‚
è¯·åœ¨å·¦ä¾§ä¾§è¾¹æ ä¸Šä¼ ç›¸åº”çš„æ–‡ä»¶ã€‚
""")

# --- ä¾§è¾¹æ ï¼šæ–‡ä»¶ä¸Šä¼  ---
with st.sidebar:
    st.header("ğŸ“‚ æ–‡ä»¶ä¸Šä¼ åŒº")
    
    st.subheader("1. ä¸Šä¼ æ³•è§„æ–‡ä»¶ (æ ‡å‡†)")
    reg_files = st.file_uploader("æ”¯æŒ .docx, .zip (å¦‚: å®‰å…¨æ³•.docx)", type=['docx', 'zip'], accept_multiple_files=True, key="reg")
    
    st.subheader("2. ä¸Šä¼ åˆ¶åº¦æ–‡ä»¶ (ä¾æ®)")
    policy_files = st.file_uploader("æ”¯æŒ .docx, .xlsx, .zip (å¦‚: ç®¡ç†æ‰‹å†Œ)", type=['docx', 'xlsx', 'zip'], accept_multiple_files=True, key="pol")
    
    st.info("æç¤ºï¼šæ”¯æŒæ‰¹é‡ä¸Šä¼ æˆ–ZIPå‹ç¼©åŒ…ã€‚æ–‡ä»¶è¶Šå¤šï¼Œåˆ†ææ—¶é—´è¶Šé•¿ï¼Œè¯·è€å¿ƒç­‰å¾…ã€‚")

# --- ä¸»ç•Œé¢ï¼šåˆ†ææ§åˆ¶ä¸å±•ç¤º ---

if reg_files and policy_files:
    if st.button("ğŸš€ å¼€å§‹åˆè§„æ€§åŒ¹é…åˆ†æ", type="primary"):
        # è¿›åº¦æ¡å®¹å™¨
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        try:
            # æ‰§è¡Œåˆ†æ
            df_result = analyze_compliance(reg_files, policy_files, progress_bar, status_text)
            
            status_text.text("âœ… åˆ†æå®Œæˆï¼")
            progress_bar.progress(100)
            
            # è®¡ç®—æ±‡æ€»æ•°æ®
            total_clauses = len(df_result)
            compliant_count = len(df_result[df_result['è¯„ä»·ç»“è®º'] == "âœ…å®Œå…¨ç¬¦åˆ"])
            partial_count = len(df_result[df_result['è¯„ä»·ç»“è®º'] == "âš ï¸éƒ¨åˆ†ç¬¦åˆ/éœ€å®Œå–„"])
            non_compliant_count = total_clauses - compliant_count - partial_count
            compliance_rate = ((compliant_count + partial_count) / total_clauses * 100) if total_clauses > 0 else 0
            
            summary_data = {
                "total": total_clauses,
                "compliant": compliant_count,
                "partial": partial_count,
                "non_compliant": non_compliant_count,
                "compliance_rate": compliance_rate
            }
            
            st.divider()
            
            # --- ç¬¬ä¸€éƒ¨åˆ†ï¼šæ€»ä½“è¯„ä»· ---
            st.header("ç¬¬ä¸€éƒ¨åˆ†ï¼šæ€»ä½“è¯„ä»·")
            
            # 1. è¯„ä»·æ¦‚å†µ
            st.subheader("1. è¯„ä»·æ¦‚å†µ")
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("åˆ†ææ¡æ¬¾æ€»æ•°", total_clauses)
            with col2:
                st.metric("å®Œå…¨ç¬¦åˆ", compliant_count)
            with col3:
                st.metric("éƒ¨åˆ†ç¬¦åˆ", partial_count)
            with col4:
                st.metric("æ€»ä½“åˆè§„ç‡", f"{compliance_rate:.1f}%")
            
            # 2. è¯„ä»·ç»“è®ºç»¼è¿°
            st.subheader("2. è¯„ä»·ç»“è®ºç»¼è¿°")
            conclusion = "æ€»ä½“åˆè§„æƒ…å†µè‰¯å¥½ã€‚" if compliance_rate > 80 else "å­˜åœ¨ä¸€å®šåˆè§„é£é™©ï¼Œå»ºè®®é‡ç‚¹å…³æ³¨ç¼ºå¤±å’Œéƒ¨åˆ†ç¬¦åˆçš„æ¡æ¬¾ã€‚"
            st.info(f"æœ¬æ¬¡è¯„ä»·é’ˆå¯¹ä¸Šä¼ çš„æ³•è§„æ–‡ä»¶ä¸ä¼ä¸šå†…éƒ¨åˆ¶åº¦è¿›è¡Œäº†è‡ªåŠ¨æ¯”å¯¹ã€‚\n{conclusion}")

            # --- ç¬¬äºŒéƒ¨åˆ†ï¼šè¯¦ç»†è¯„ä»·çŸ©é˜µ ---
            st.header("ç¬¬äºŒéƒ¨åˆ†ï¼šè¯¦ç»†åˆè§„æ€§è¯„ä»·çŸ©é˜µ")
            
            # å¢åŠ ç­›é€‰åŠŸèƒ½
            filter_status = st.multiselect(
                "ç­›é€‰è¯„ä»·ç»“è®º:",
                options=df_result['è¯„ä»·ç»“è®º'].unique(),
                default=df_result['è¯„ä»·ç»“è®º'].unique()
            )
            
            df_display = df_result[df_result['è¯„ä»·ç»“è®º'].isin(filter_status)]
            st.dataframe(
                df_display, 
                use_container_width=True,
                height=600,
                column_config={
                    "æ³•è§„æ­£æ–‡": st.column_config.TextColumn("æ³•è§„æ­£æ–‡", width="medium"),
                    "æ”¯æ’‘è¯æ®": st.column_config.TextColumn("æ”¯æ’‘è¯æ®", width="large"),
                }
            )
            
            # å¯¼å‡ºåŠŸèƒ½
            st.subheader("ğŸ“¥ å¯¼å‡ºæŠ¥å‘Š")
            
            col_d1, col_d2 = st.columns(2)
            
            with col_d1:
                # Markdown æŠ¥å‘Šä¸‹è½½
                md_report = generate_markdown_report(summary_data, df_result)
                st.download_button(
                    label="ğŸ“„ ä¸‹è½½è¯„ä»·æŠ¥å‘Š (Markdown/Word)",
                    data=md_report,
                    file_name=f"EHSåˆè§„æ€§è¯„ä»·æŠ¥å‘Š_{time.strftime('%Y%m%d')}.md",
                    mime="text/markdown",
                )
            
            with col_d2:
                # CSV ä¸‹è½½
                csv = df_result.to_csv(index=False).encode('utf-8-sig')
                st.download_button(
                    label="ğŸ“Š ä¸‹è½½è¯„ä»·æ˜ç»†è¡¨ (CSV)",
                    data=csv,
                    file_name=f"EHSåˆè§„æ€§è¯„ä»·æ˜ç»†_{time.strftime('%Y%m%d')}.csv",
                    mime="text/csv",
                )
            
        except Exception as e:
            st.error(f"åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            st.exception(e)

else:
    st.info("ğŸ‘ˆ è¯·å…ˆåœ¨å·¦ä¾§ä¾§è¾¹æ ä¸Šä¼ è‡³å°‘ä¸€ä¸ªæ³•è§„æ–‡ä»¶å’Œä¸€ä¸ªåˆ¶åº¦æ–‡ä»¶ã€‚")

st.divider()
st.caption("Powered by Gemini EHS Compliance Engine | 2025")
