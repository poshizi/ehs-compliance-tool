import streamlit as st
import pandas as pd
import os
import re
import zipfile
import io
import time
import requests
import json
import numpy as np
import docx 
import pickle
import hashlib
import xml.etree.ElementTree as ET
from urllib.parse import urljoin, quote, unquote
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH
from concurrent.futures import ThreadPoolExecutor, as_completed
from tenacity import retry, stop_after_attempt, wait_exponential

# ====================
# Configuration & Constants
# ====================
CACHE_DIR = "vector_store_cache"
if not os.path.exists(CACHE_DIR):
    os.makedirs(CACHE_DIR)

# ====================
# Lightweight WebDAV Client
# ====================
class SimpleWebDavClient:
    def __init__(self, base_url, username, password):
        self.base_url = base_url.rstrip('/') 
        self.auth = (username, password)
        self.session = requests.Session()
        self.session.auth = self.auth
    
    def list(self, path="/"):
        path = path.strip('/')
        full_url = f"{self.base_url}/{path}/" if path else f"{self.base_url}/"
        headers = {'Depth': '1'}
        try:
            response = self.session.request('PROPFIND', full_url, headers=headers)
            if response.status_code in [200, 207]:
                return self._parse_propfind(response.content, full_url)
            else:
                raise Exception(f"WebDAV Error: {response.status_code} - {response.text}")
        except Exception as e:
            raise Exception(f"Connection failed: {str(e)}")

    def download(self, path):
        full_url = f"{self.base_url}/{quote(path.strip('/'))}"
        response = self.session.get(full_url)
        if response.status_code == 200:
            return response.content
        else:
            raise Exception(f"Download failed: {response.status_code}")

    def _parse_propfind(self, xml_content, current_url):
        items = []
        try:
            root = ET.fromstring(xml_content)
            for response in root.findall('.//{DAV:}response'):
                href = response.find('.//{DAV:}href').text
                href = unquote(href)
                resourcetype = response.find('.//{DAV:}resourcetype')
                is_collection = False
                if resourcetype is not None:
                    if resourcetype.find('.//{DAV:}collection') is not None:
                        is_collection = True
                name = href.rstrip('/').split('/')[-1]
                if not name: continue 
                items.append({'name': name, 'path': href, 'is_folder': is_collection})
        except Exception as e: pass
        return items

# ====================
# Core Classes for AI & Data
# ====================

class LLMClient:
    def __init__(self, config):
        self.base_url = config.get('base_url', '').rstrip('/')
        self.api_key = config.get('api_key')
        self.model = config.get('model')
        self.embedding_model = config.get('embedding_model', 'text-embedding-004')
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        self.embedding_failed = False 

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
    def get_embedding(self, text):
        if self.embedding_failed: return None
        payload = {"input": text.replace("\n", " "), "model": self.embedding_model}
        url = f"{self.base_url}/embeddings"
        try:
            response = requests.post(url, headers=self.headers, json=payload, timeout=10)
            if response.status_code == 200:
                data = response.json()
                if 'data' in data and len(data['data']) > 0:
                    return data['data'][0]['embedding']
            return None
        except Exception: return None

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
    def chat_completion(self, system_prompt, user_prompt):
        payload = {
            "model": self.model,
            "messages": [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}],
            "temperature": 0.1,
            "response_format": {"type": "json_object"}
        }
        try:
            response = requests.post(f"{self.base_url}/chat/completions", headers=self.headers, json=payload, timeout=60)
            if response.status_code == 200:
                content = response.json()['choices'][0]['message']['content']
                content = content.replace("```json", "").replace("```", "")
                return json.loads(content)
            else:
                raise Exception(f"API Error {response.status_code}: {response.text}")
        except Exception as e:
            raise Exception(f"Request failed: {str(e)}")
            
    def test_connection(self):
        results = {"chat": False, "embedding": False, "msg": ""}
        try:
            self.chat_completion("Test", "Reply JSON: {'status': 'ok'}")
            results['chat'] = True
        except Exception as e: results['msg'] += f"Chat Error: {e}\n"
        try:
            if self.get_embedding("test"): results['embedding'] = True
            else: results['msg'] += "Embedding Error: None\n"
        except Exception as e: results['msg'] += f"Embedding Exception: {e}\n"
        return results

class VectorStore:
    def __init__(self):
        self.documents = []   # [{'id', 'text', 'source', 'keywords', 'original_idx'}, ...]
        self.vectors = []     
        self.regulations = [] 
        self.eval_cache = {}  # {hash: result}
        self.llm_client = None

    def set_client(self, client):
        self.llm_client = client

    def save_to_disk(self, name="default"):
        path = os.path.join(CACHE_DIR, f"{name}.pkl")
        data = {
            "documents": self.documents,
            "vectors": self.vectors,
            "regulations": self.regulations,
            "eval_cache": self.eval_cache
        }
        with open(path, "wb") as f: pickle.dump(data, f)
        return path

    def load_from_disk(self, name="default"):
        path = os.path.join(CACHE_DIR, f"{name}.pkl")
        if os.path.exists(path):
            with open(path, "rb") as f:
                data = pickle.load(f)
                self.documents = data.get("documents", [])
                self.vectors = data.get("vectors", [])
                self.regulations = data.get("regulations", [])
                self.eval_cache = data.get("eval_cache", {})
            return True
        return False

    def add_documents(self, file_corpus):
        """æ™ºèƒ½æŒ‰æ®µè½åˆ‡ç‰‡"""
        new_docs = []
        texts_to_embed = []
        start_doc_id = len(self.documents)
        
        for file in file_corpus:
            if any(d['source'] == file['name'] for d in self.documents): continue
            
            # Smart Chunking: æŒ‰æ¢è¡Œç¬¦åˆ†å‰²ï¼Œåˆå¹¶æˆ approx 600 chars
            paragraphs = file['content'].split('\n')
            current_chunk = ""
            
            for para in paragraphs:
                para = para.strip()
                if not para: continue
                
                if len(current_chunk) + len(para) < 600:
                    current_chunk += para + "\n"
                else:
                    if len(current_chunk) > 50: # Avoid tiny chunks
                        new_docs.append(self._create_doc_obj(start_doc_id, current_chunk, file['name']))
                        texts_to_embed.append(current_chunk)
                        start_doc_id += 1
                    current_chunk = para + "\n"
            
            # Last chunk
            if len(current_chunk) > 30:
                new_docs.append(self._create_doc_obj(start_doc_id, current_chunk, file['name']))
                texts_to_embed.append(current_chunk)
                start_doc_id += 1

        if not texts_to_embed: return

        if self.llm_client:
            with st.status(f"æ­£åœ¨æ™ºèƒ½å‘é‡åŒ– {len(texts_to_embed)} ä¸ªæ–°ç‰‡æ®µ...") as status:
                new_vectors = [None] * len(texts_to_embed)
                with ThreadPoolExecutor(max_workers=5) as executor:
                    future_to_idx = {executor.submit(self.llm_client.get_embedding, t): i for i, t in enumerate(texts_to_embed)}
                    for future in as_completed(future_to_idx):
                        idx = future_to_idx[future]
                        new_vectors[idx] = future.result()
                
                self.documents.extend(new_docs)
                self.vectors = (list(self.vectors) if len(self.vectors)>0 else []) + new_vectors
                status.update(label="åˆ¶åº¦å…¥åº“å®Œæˆ", state="complete")

    def _create_doc_obj(self, doc_id, text, source):
        keywords = set(re.split(r'[ï¼Œã€‚ï¼›ï¼š\s]', text))
        keywords = [k for k in keywords if len(k) > 1]
        return {'id': doc_id, 'text': text, 'source': source, 'keywords': keywords}

    def add_regulations(self, file_corpus):
        count = 0
        for file in file_corpus:
            if any(r['name'] == file['name'] for r in self.regulations): continue
            self.regulations.append(file)
            count += 1
        return count

    def get_context_window(self, doc_id):
        """è·å–æŒ‡å®šdoc_idçš„å‰åæ–‡"""
        # å¯»æ‰¾ doc_id åœ¨ documents åˆ—è¡¨ä¸­çš„ index
        # å‡è®¾ doc_id æ˜¯è¿ç»­å¢åŠ çš„ï¼Œä½†è¿™ä¸ä¸€å®šé è°±å¦‚æœåˆ é™¤äº†æ–‡æ¡£
        # æˆ‘ä»¬ç”¨ list æŸ¥æ‰¾
        target_idx = -1
        for i, d in enumerate(self.documents):
            if d['id'] == doc_id:
                target_idx = i
                break
        
        if target_idx == -1: return ""
        
        current_doc = self.documents[target_idx]
        context = current_doc['text']
        
        # æ‰¾å‰ä¸€ä¸ª
        if target_idx > 0:
            prev_doc = self.documents[target_idx - 1]
            if prev_doc['source'] == current_doc['source']: # å¿…é¡»åŒæº
                context = prev_doc['text'] + "\n[...ä¸Šä¸‹æ–‡è¿æ¥...]\n" + context
        
        # æ‰¾åä¸€ä¸ª
        if target_idx < len(self.documents) - 1:
            next_doc = self.documents[target_idx + 1]
            if next_doc['source'] == current_doc['source']:
                context = context + "\n[...ä¸Šä¸‹æ–‡è¿æ¥...]\n" + next_doc['text']
                
        return context

    def search(self, query_text, top_k=3):
        vec_results = []
        valid_indices = [i for i, v in enumerate(self.vectors) if v is not None]
        
        if valid_indices and self.llm_client:
            query_vec = self.llm_client.get_embedding(query_text)
            if query_vec is not None:
                q_v = np.array(query_vec)
                norm_q = np.linalg.norm(q_v)
                matrix = np.array([self.vectors[i] for i in valid_indices])
                norm_matrix = np.linalg.norm(matrix, axis=1)
                if norm_q > 0:
                    scores = np.dot(matrix, q_v) / (norm_matrix * norm_q)
                    top_k_indices = np.argsort(scores)[-top_k:][::-1]
                    for idx_in_valid in top_k_indices:
                        real_idx = valid_indices[idx_in_valid]
                        score = scores[idx_in_valid]
                        if score > 0.1: # ç¨å¾®æé«˜çº¯å‘é‡é˜ˆå€¼
                            vec_results.append({'doc': self.documents[real_idx], 'score': float(score), 'method': 'vector'})

        kw_results = []
        query_keywords = [k for k in re.split(r'[ï¼Œã€‚ï¼›ï¼š\s]', query_text) if len(k) > 1]
        for doc in self.documents:
            overlap = sum(1 for k in query_keywords if k in doc['keywords']) 
            if overlap > 0:
                score = overlap / (len(query_keywords) + 1) * 0.9 # æé«˜å…³é”®è¯æƒé‡
                kw_results.append({'doc': doc, 'score': score, 'method': 'keyword'})
        
        combined = vec_results + kw_results
        combined.sort(key=lambda x: x['score'], reverse=True)
        
        seen_ids = set()
        final_results = []
        for res in combined:
            did = res['doc']['id']
            if did not in seen_ids:
                final_results.append(res)
                seen_ids.add(did)
            if len(final_results) >= top_k: break
        return final_results

# ====================
# Helper Functions
# ====================

def process_uploaded_files(uploaded_files):
    for uploaded_file in uploaded_files:
        if uploaded_file.name.endswith('.zip'):
            try:
                with zipfile.ZipFile(uploaded_file) as z:
                    for filename in z.namelist():
                        if filename.endswith('/') or filename.startswith('__MACOSX') or filename.startswith('._'): continue
                        if filename.endswith(('.docx', '.xlsx')):
                            with z.open(filename) as f: yield filename, f.read()
            except Exception as e: st.error(f"è§£å‹å¤±è´¥: {e}")
        else: yield uploaded_file.name, uploaded_file.getvalue()

def extract_text_from_content(filename, content):
    text = ""
    try:
        file_stream = io.BytesIO(content)
        if filename.endswith('.docx'):
            doc = docx.Document(file_stream)
            text = '\n'.join([para.text for para in doc.paragraphs])
        elif filename.endswith('.xlsx'):
            df_dict = pd.read_excel(file_stream, sheet_name=None, header=None)
            text_parts = []
            for sheet_name, df in df_dict.items():
                sheet_text = df.astype(str).apply(lambda x: ' '.join(x), axis=1)
                text_parts.append('\n'.join(sheet_text))
            text = '\n'.join(text_parts)
    except Exception as e: print(f"Error parsing {filename}: {e}")
    return text

def parse_regulation_clauses(text):
    pattern = r'(ç¬¬\s*[\dé›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+\s*æ¡|Article\s+\d+)'
    parts = re.split(pattern, text)
    clauses = []
    if len(parts) > 1:
        for i in range(1, len(parts), 2):
            title = parts[i].strip()
            content = parts[i+1].strip() if i+1 < len(parts) else ""
            clauses.append({"æ¡æ¬¾å·": title, "æ³•è§„æ­£æ–‡": title + " " + content, "é€‚ç”¨æ€§": "é€‚ç”¨"})
    return clauses

def evaluate_single_clause(clause, vector_store, llm_client):
    """ä¸“å®¶è¯„ä¼°æ ¸å¿ƒ (å¸¦ç¼“å­˜ä¸ä¸Šä¸‹æ–‡å¢å¼º)"""
    
    # 1. æ£€ç´¢
    search_results = vector_store.search(clause['æ³•è§„æ­£æ–‡'], top_k=3)
    top_score = search_results[0]['score'] if search_results else 0
    
    # 2. æ„é€  Evidence Context (å¢å¼ºç‰ˆ)
    evidence_text = ""
    evidence_signature = "" # ç”¨äºç¼“å­˜çš„ key
    
    if search_results:
        for i, res in enumerate(search_results):
            # è·å–ä¸Šä¸‹æ–‡ (Context Window)
            expanded_text = vector_store.get_context_window(res['doc']['id'])
            evidence_text += f"å‚è€ƒåˆ¶åº¦ç‰‡æ®µ {i+1} (æ¥æº: {res['doc']['source']}):\n{expanded_text[:1200]}\n---\n"
            evidence_signature += f"{res['doc']['id']}-"
    else:
        evidence_text = "æœªæ£€ç´¢åˆ°ä»»ä½•ç›¸å…³çš„ä¼ä¸šå†…éƒ¨åˆ¶åº¦æ–‡æ¡£ã€‚"
        evidence_signature = "None"
        
    # 3. æ£€æŸ¥ç¼“å­˜
    cache_key = hashlib.md5((clause['æ³•è§„æ­£æ–‡'] + evidence_signature).encode()).hexdigest()
    if cache_key in vector_store.eval_cache:
        # å‘½ä¸­ç¼“å­˜
        cached_row = vector_store.eval_cache[cache_key]
        cached_row['åŒ¹é…åº¦'] = top_score # æ›´æ–°åˆ†æ•°ä»¥é˜²ç®—æ³•å¾®è°ƒ
        return cached_row

    row = {"æ¡æ¬¾å·": clause['æ¡æ¬¾å·'], "æ³•è§„æ­£æ–‡": clause['æ³•è§„æ­£æ–‡'], "è¯„ä»·ç»“è®º": "âŒç¼ºå¤±/ä¸ç¬¦åˆ", "å·®è·åˆ†æ": "æœªæ£€ç´¢åˆ°ç›¸å…³åˆ¶åº¦", "æ”¹è¿›å»ºè®®": "è¯·è¡¥å……ç›¸å…³ç®¡ç†è§„å®š", "æ”¯æ’‘è¯æ®": "æ— ", "åŒ¹é…åº¦": top_score}

    system_prompt = """ä½ æ˜¯ä¸€åå…·æœ‰20å¹´ç»éªŒçš„EHSç®¡ç†ä¸“å®¶ï¼Œç²¾é€šä¸­å›½EHSæ³•è§„æ ‡å‡†ï¼Œæ“…é•¿ä»“å‚¨ç‰©æµåœºæ™¯ã€‚
    è¯·å¯¹ç»™å®šçš„æ³•è§„æ¡æ¬¾è¿›è¡Œåˆè§„æ€§è¯„ä»·ã€‚ä¸¥æ ¼æ‰§è¡Œä»¥ä¸‹æ€ç»´é“¾ï¼š
    1. è§£è¯»ï¼šç†è§£æ¡æ¬¾æ ¸å¿ƒè¦æ±‚ï¼ˆäººæœºæ–™æ³•ç¯ï¼‰ï¼Œåˆ¤å®šæ˜¯å¦é€‚ç”¨äºç‰©æµä»“å‚¨ä¼ä¸šã€‚å¦‚æœä¸é€‚ç”¨ï¼Œç›´æ¥æ ‡è®°â€œä¸é€‚ç”¨â€ã€‚
    2. æ¯”å¯¹ï¼šå¯¹æ¯”æ³•è§„è¦æ±‚ä¸æä¾›çš„ä¼ä¸šåˆ¶åº¦ç‰‡æ®µã€‚æ³¨æ„ï¼šåˆ¶åº¦ç‰‡æ®µå¯èƒ½åŒ…å«ä¸Šä¸‹æ–‡ã€‚
    3. åˆ¤å®šï¼šç»™å‡ºå®šæ€§ç»“è®ºã€‚"""
    
    user_prompt = f"""
    ã€æ³•è§„æ¡æ¬¾ã€‘
    {clause['æ³•è§„æ­£æ–‡']}

    ã€ä¼ä¸šåˆ¶åº¦ç°çŠ¶ï¼ˆåŒ…å«ä¸Šä¸‹æ–‡ï¼‰ã€‘
    {evidence_text}

    ã€ä»»åŠ¡è¦æ±‚ã€‘
    è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¿”å› JSON ç»“æœï¼š
    {{
        "applicability": "é€‚ç”¨" æˆ– "ä¸é€‚ç”¨",
        "compliance_status": "å®Œå…¨ç¬¦åˆ" æˆ– "éƒ¨åˆ†ç¬¦åˆ/éœ€å®Œå–„" æˆ– "ç¼ºå¤±/ä¸ç¬¦åˆ" æˆ– "ä¸é€‚ç”¨",
        "gap_analysis": "150å­—ä»¥å†…ã€‚è‹¥ä¸é€‚ç”¨å¡«'ä¸é€‚ç”¨'ã€‚è‹¥é€‚ç”¨ï¼Œåˆ†æå…·ä½“ç¼ºäº†ä»€ä¹ˆæˆ–è¯´æ˜åˆè§„ç‚¹ã€‚",
        "improvement_suggestion": "è‹¥å®Œå…¨ç¬¦åˆå¡«'æ— 'ã€‚å¦åˆ™ç»™å‡º1-2æ¡å»ºè®®ã€‚",
        "evidence_summary": "åˆ—å‡ºæœ€åŒ¹é…çš„åˆ¶åº¦åç§°åŠå…³é”®å¥æ‘˜è¦"
    }}
    """
    try:
        result = llm_client.chat_completion(system_prompt, user_prompt)
        status = result.get('compliance_status', 'ç¼ºå¤±/ä¸ç¬¦åˆ')
        if "å®Œå…¨ç¬¦åˆ" in status: status = "âœ…å®Œå…¨ç¬¦åˆ"
        elif "éƒ¨åˆ†" in status or "éœ€å®Œå–„" in status: status = "âš ï¸éƒ¨åˆ†ç¬¦åˆ/éœ€å®Œå–„"
        elif "ä¸é€‚ç”¨" in status: status = "â—ä¸é€‚ç”¨"
        else: status = "âŒç¼ºå¤±/ä¸ç¬¦åˆ"
        
        row['è¯„ä»·ç»“è®º'] = status
        row['å·®è·åˆ†æ'] = result.get('gap_analysis', 'æ— åˆ†æ')
        row['æ”¹è¿›å»ºè®®'] = result.get('improvement_suggestion', 'æ— å»ºè®®')
        row['æ”¯æ’‘è¯æ®'] = result.get('evidence_summary', 'æ— ')
        
        # å†™å…¥ç¼“å­˜
        vector_store.eval_cache[cache_key] = row
        
    except Exception as e:
        row['å·®è·åˆ†æ'] = f"LLMåˆ†æå¤±è´¥: {str(e)}"
    return row

def generate_word_report(df_results, summary_stats):
    doc = Document()
    title = doc.add_heading('EHSæ³•è§„åˆè§„æ€§è¯„ä»·æŠ¥å‘Š', 0)
    title.alignment = WD_ALIGN_PARAGRAPH.CENTER
    doc.add_paragraph(f"è¯„ä»·æ—¥æœŸ: {time.strftime('%Y-%m-%d')}")
    
    doc.add_heading('ç¬¬ä¸€éƒ¨åˆ†ï¼šæ€»ä½“è¯„ä»·ä¸ç®¡ç†å»ºè®®', level=1)
    
    doc.add_heading('1.1 è¯„ä»·æ•°æ®æ¦‚è§ˆ', level=2)
    p = doc.add_paragraph()
    p.add_run(f"æœ¬æ¬¡è¯„ä»·å…±åˆ†ææ³•è§„æ¡æ¬¾ {summary_stats['total']} æ¡ï¼Œå…¶ä¸­ï¼š\n")
    p.add_run(f"âœ… å®Œå…¨ç¬¦åˆ: {summary_stats['compliant']} æ¡ ({summary_stats['compliant']/summary_stats['total']*100:.1f}%)\n").font.color.rgb = RGBColor(0, 128, 0)
    p.add_run(f"âš ï¸ éƒ¨åˆ†ç¬¦åˆ/éœ€å®Œå–„: {summary_stats['partial']} æ¡ ({summary_stats['partial']/summary_stats['total']*100:.1f}%)\n").font.color.rgb = RGBColor(255, 165, 0)
    p.add_run(f"âŒ ç¼ºå¤±/ä¸ç¬¦åˆ: {summary_stats['non_compliant']} æ¡ ({summary_stats['non_compliant']/summary_stats['total']*100:.1f}%)\n").font.color.rgb = RGBColor(255, 0, 0)
    
    doc.add_heading('1.2 å…³é”®é£é™©é¢†åŸŸè¯†åˆ«', level=2)
    risk_df = df_results[df_results['è¯„ä»·ç»“è®º'].str.contains("ç¼ºå¤±|ä¸ç¬¦åˆ|éƒ¨åˆ†")]
    if not risk_df.empty:
        doc.add_paragraph("ä»¥ä¸‹æ¡æ¬¾å­˜åœ¨åˆè§„é£é™©ï¼Œéœ€é‡ç‚¹å…³æ³¨ï¼š")
        risk_table = doc.add_table(rows=1, cols=3)
        risk_table.style = 'Table Grid'
        headers = ["æ¡æ¬¾å·", "é£é™©æè¿° (å·®è·åˆ†æ)", "æ”¹è¿›å»ºè®®"]
        for i, h in enumerate(headers):
            cell = risk_table.rows[0].cells[i]
            cell.text = h
            cell.paragraphs[0].runs[0].font.bold = True
        
        for idx, row in risk_df.head(10).iterrows(): 
            r = risk_table.add_row().cells
            r[0].text = row['æ¡æ¬¾å·']
            r[1].text = row['å·®è·åˆ†æ']
            r[2].text = row['æ”¹è¿›å»ºè®®']
        if len(risk_df) > 10:
            doc.add_paragraph(f"...(å¦æœ‰ {len(risk_df)-10} æ¡é£é™©æ¡æ¬¾ï¼Œè¯¦è§é™„è¡¨)")
    else:
        doc.add_paragraph("æœ¬æ¬¡è¯„ä»·æœªå‘ç°é‡å¤§åˆè§„é£é™©ã€‚")

    doc.add_heading('1.3 ä¸“å®¶ç»¼åˆç»“è®º', level=2)
    if summary_stats['non_compliant'] > 5:
        conclusion = "ç»“è®ºï¼šä¼ä¸šç°è¡Œåˆ¶åº¦åœ¨æ ¸å¿ƒè¦ç´ ä¸Šå­˜åœ¨æ˜æ˜¾ç¼ºå¤±ï¼Œåˆè§„é£é™©è¾ƒé«˜ã€‚å»ºè®®ç«‹å³å¯åŠ¨ä¸“é¡¹æ•´æ”¹ã€‚"
    elif summary_stats['partial'] > 5:
        conclusion = "ç»“è®ºï¼šä¼ä¸šåˆ¶åº¦æ¡†æ¶åŸºæœ¬å¥å…¨ï¼Œä½†åœ¨æ‰§è¡Œç»†èŠ‚ä¸Šä»æœ‰ä¼˜åŒ–ç©ºé—´ã€‚"
    else:
        conclusion = "ç»“è®ºï¼šä¼ä¸šEHSç®¡ç†åˆ¶åº¦ä½“ç³»å¥å…¨ã€‚å»ºè®®å®šæœŸå›é¡¾æ›´æ–°ã€‚"
    doc.add_paragraph(conclusion)

    doc.add_heading('ç¬¬äºŒéƒ¨åˆ†ï¼šè¯¦ç»†åˆè§„æ€§è¯„ä»·çŸ©é˜µ', level=1)
    table = doc.add_table(rows=1, cols=6)
    table.style = 'Table Grid'
    headers = ["åºå·", "æ³•è§„æ¡æ¬¾", "è¯„ä»·ç»“è®º", "å·®è·åˆ†æä¸è®ºæ®", "æ”¹è¿›å»ºè®®", "æ”¯æ’‘è¯æ®"]
    for i, header in enumerate(headers):
        table.rows[0].cells[i].text = header
        
    for idx, row in df_results.iterrows():
        row_cells = table.add_row().cells
        row_cells[0].text = str(row.get('åºå·', idx + 1))
        row_cells[1].text = str(row.get('æ³•è§„æ­£æ–‡', ''))
        row_cells[2].text = str(row.get('è¯„ä»·ç»“è®º', ''))
        row_cells[3].text = str(row.get('å·®è·åˆ†æ', ''))
        row_cells[4].text = str(row.get('æ”¹è¿›å»ºè®®', ''))
        row_cells[5].text = str(row.get('æ”¯æ’‘è¯æ®', ''))
        
    f = io.BytesIO()
    doc.save(f)
    f.seek(0)
    return f

# ====================
# Streamlit UI
# ====================

st.set_page_config(page_title="EHSä¸“å®¶åˆè§„ç³»ç»Ÿ (Enterprise)", layout="wide")

if 'results' not in st.session_state: st.session_state.results = None
if 'vector_store' not in st.session_state: st.session_state.vector_store = VectorStore()
if 'current_webdav_path' not in st.session_state: st.session_state.current_webdav_path = "/"

st.title("ğŸ›¡ï¸ EHSæ³•è§„åˆè§„æ€§æ™ºèƒ½è¯„ä»·ç³»ç»Ÿ (Enterprise)")
st.caption("v2.1 | æ™ºèƒ½åˆ†æ®µ | ä¸Šä¸‹æ–‡å¢å¼º | ç»“æœç¼“å­˜ | é£é™©æ´å¯Ÿ")

with st.sidebar:
    st.header("1. API é…ç½®")
    llm_base_url = st.text_input("API Base URL", value="https://generativelanguage.googleapis.com/v1beta/openai")
    llm_api_key = st.text_input("API Key", type="password")
    col_m1, col_m2 = st.columns(2)
    with col_m1: llm_model_name = st.text_input("Chat Model", value="gemini-2.0-flash")
    with col_m2: embedding_model_name = st.text_input("Embedding Model", value="text-embedding-004")
    
    if st.button("ğŸ”Œ æµ‹è¯•è¿é€šæ€§", use_container_width=True):
        if not llm_api_key: st.error("è¯·å…ˆè¾“å…¥ API Key")
        else:
            with st.spinner("æ­£åœ¨æµ‹è¯• API è¿æ¥..."):
                cfg = {"base_url": llm_base_url, "api_key": llm_api_key, "model": llm_model_name, "embedding_model": embedding_model_name}
                client = LLMClient(cfg)
                res = client.test_connection()
                if res['chat']: st.success(f"âœ… Chat: é€šç•…")
                else: st.error(f"âŒ Chat å¤±è´¥: {res['msg']}")
                if res['embedding']: st.success(f"âœ… Embedding: é€šç•…")
                else: st.error(f"âŒ Embedding å¤±è´¥: {res['msg']}")

    if llm_api_key:
        llm_config = {"base_url": llm_base_url, "api_key": llm_api_key, "model": llm_model_name, "embedding_model": embedding_model_name}
        client = LLMClient(llm_config)
        st.session_state.vector_store.set_client(client)
    
    st.divider()
    st.header("ğŸ’¾ çŸ¥è¯†åº“ç®¡ç†")
    db_name = st.text_input("åº“åç§°", value="ehs_master_index")
    col_db1, col_db2 = st.columns(2)
    with col_db1:
        if st.button("ğŸ’¾ ä¿å­˜å…¨åº“"):
            path = st.session_state.vector_store.save_to_disk(db_name)
            st.success(f"å·²ä¿å­˜: {path}")
    with col_db2:
        if st.button("ğŸ“‚ åŠ è½½å…¨åº“"):
            if st.session_state.vector_store.load_from_disk(db_name):
                st.success(f"å·²åŠ è½½!")
            else: st.error("æ–‡ä»¶ä¸å­˜åœ¨")
            
    if st.button("ğŸ—‘ï¸ æ¸…ç©ºç¼“å­˜"):
        st.session_state.vector_store.eval_cache = {}
        st.success("è¯„ä¼°ç»“æœç¼“å­˜å·²æ¸…ç©º")

st.info(f"ğŸ“š å½“å‰çŸ¥è¯†åº“: åˆ¶åº¦ç‰‡æ®µ {len(st.session_state.vector_store.documents)} ä¸ª | å·²å­˜æ³•è§„ {len(st.session_state.vector_store.regulations)} ä¸ª | ç¼“å­˜ç»“æœ {len(st.session_state.vector_store.eval_cache)} æ¡")

tab1, tab2, tab3 = st.tabs(["ğŸ“‚ æœ¬åœ°ä¸Šä¼ ", "â˜ï¸ WebDAV è¿œç¨‹åº“", "ğŸš€ å¼€å§‹è¯„ä¼°"])

with tab1:
    col_u1, col_u2 = st.columns(2)
    with col_u1:
        st.subheader("ä¸Šä¼ åˆ¶åº¦ (ä¾æ®)")
        policy_files_local = st.file_uploader("åˆ¶åº¦æ–‡ä»¶", type=['docx', 'xlsx', 'zip'], accept_multiple_files=True, key="pol_local", label_visibility="collapsed")
        if st.button("ğŸ“¥ åˆ¶åº¦å…¥åº“"):
            if policy_files_local:
                corpus = []
                for name, content in process_uploaded_files(policy_files_local):
                    text = extract_text_from_content(name, content)
                    if text: corpus.append({'name': name, 'content': text})
                st.session_state.vector_store.add_documents(corpus)
    
    with col_u2:
        st.subheader("ä¸Šä¼ æ³•è§„ (æ ‡å‡†)")
        reg_files_local = st.file_uploader("æ³•è§„æ–‡ä»¶", type=['docx', 'zip'], accept_multiple_files=True, key="reg_local", label_visibility="collapsed")
        if st.button("ğŸ“¥ æ³•è§„å…¥åº“"):
            if reg_files_local:
                corpus = []
                for name, content in process_uploaded_files(reg_files_local):
                    text = extract_text_from_content(name, content)
                    if text: corpus.append({'name': name, 'content': text})
                count = st.session_state.vector_store.add_regulations(corpus)
                st.success(f"æ–°å¢ {count} ä¸ªæ³•è§„æ–‡æ¡£")

with tab2:
    st.markdown("### â˜ï¸ WebDAV æ–‡ä»¶æµè§ˆå™¨")
    col_w1, col_w2, col_w3 = st.columns([2, 1, 1])
    webdav_url = col_w1.text_input("URL", help="https://dav.example.com/")
    webdav_user = col_w2.text_input("User")
    webdav_pass = col_w3.text_input("Pass", type="password")
    
    if st.button("ğŸ”— è¿æ¥/åˆ·æ–°ç›®å½•"):
        try:
            wd_client = SimpleWebDavClient(webdav_url, webdav_user, webdav_pass)
            items = wd_client.list(st.session_state.current_webdav_path)
            st.session_state.webdav_items = items
            st.session_state.wd_client = wd_client
        except Exception as e: st.error(f"è¿æ¥å¤±è´¥: {e}")

    if 'webdav_items' in st.session_state:
        st.markdown(f"**å½“å‰è·¯å¾„**: `{st.session_state.current_webdav_path}`")
        if st.session_state.current_webdav_path != "/":
            if st.button("â¬†ï¸ è¿”å›ä¸Šä¸€çº§"):
                parent = os.path.dirname(st.session_state.current_webdav_path.rstrip('/'))
                st.session_state.current_webdav_path = parent if parent else "/"
                st.rerun()

        folders = [i for i in st.session_state.webdav_items if i['is_folder']]
        files = [i for i in st.session_state.webdav_items if not i['is_folder'] and i['name'].endswith(('.docx', '.xlsx', '.zip'))]
        
        if folders:
            st.markdown("#### ğŸ“ æ–‡ä»¶å¤¹")
            cols = st.columns(4)
            for i, f in enumerate(folders):
                if cols[i % 4].button(f"ğŸ“‚ {f['name']}", key=f['path']):
                    new_path = f"{st.session_state.current_webdav_path.rstrip('/')}/{f['name']}"
                    st.session_state.current_webdav_path = new_path
                    st.rerun()

        st.markdown("#### ğŸ“„ æ–‡ä»¶")
        selected_wd_files = st.multiselect("é€‰æ‹©æ–‡ä»¶", [f['name'] for f in files])
        action = st.radio("æ“ä½œ:", ["åˆ¶åº¦å…¥åº“ (å‘é‡åŒ–)", "æ³•è§„å…¥åº“ (ä¿å­˜)"])
        
        if st.button("â¬‡ï¸ ä¸‹è½½å¹¶å¤„ç†"):
            corpus = []
            for fname in selected_wd_files:
                try:
                    full_p = f"{st.session_state.current_webdav_path.rstrip('/')}/{fname}"
                    content = st.session_state.wd_client.download(full_p)
                    text = extract_text_from_content(fname, content)
                    if text: corpus.append({'name': fname, 'content': text})
                except Exception as e: st.error(f"Error {fname}: {e}")
            
            if action.startswith("åˆ¶åº¦"):
                st.session_state.vector_store.add_documents(corpus)
                st.success("WebDAV åˆ¶åº¦å·²å…¥åº“")
            else:
                c = st.session_state.vector_store.add_regulations(corpus)
                st.success(f"WebDAV æ³•è§„å·²ä¿å­˜ ({c}ä¸ª)")

with tab3:
    st.subheader("æ‰§è¡Œåˆè§„æ€§åˆ†æ")
    saved_regs = [r['name'] for r in st.session_state.vector_store.regulations]
    
    if not saved_regs:
        st.warning("æ³•è§„åº“ä¸ºç©ºï¼Œè¯·å…ˆåœ¨ Tab 1 æˆ– Tab 2 ä¸Šä¼ /ä¿å­˜æ³•è§„æ–‡ä»¶ã€‚")
    else:
        selected_reg_names = st.multiselect("é€‰æ‹©è¦åˆ†æçš„æ³•è§„", saved_regs, default=saved_regs[0] if saved_regs else None)
        
        if st.button("ğŸš€ å¼€å§‹ä¸“å®¶çº§è¯„ä¼°", type="primary"):
            if not selected_reg_names:
                st.error("è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªæ³•è§„æ–‡ä»¶")
                st.stop()
                
            target_regs = [r for r in st.session_state.vector_store.regulations if r['name'] in selected_reg_names]
            all_clauses = []
            for doc in target_regs:
                clauses = parse_regulation_clauses(doc['content'])
                for i, c in enumerate(clauses):
                    c['source_file'] = doc['name']
                    c['åºå·'] = i + 1
                    all_clauses.append(c)
            
            st.info(f"åˆ†æä¸­... å…± {len(all_clauses)} æ¡æ¬¾ (å·²å¯ç”¨å¢é‡ç¼“å­˜ä¼˜åŒ–)")
            
            results_list = []
            progress_bar = st.progress(0)
            completed = 0
            
            with ThreadPoolExecutor(max_workers=10) as executor:
                future_to_clause = {executor.submit(evaluate_single_clause, clause, st.session_state.vector_store, client): clause for clause in all_clauses}
                for future in as_completed(future_to_clause):
                    res = future.result()
                    res['æ³•è§„æ–‡ä»¶'] = future_to_clause[future]['source_file']
                    res['åºå·'] = future_to_clause[future]['åºå·']
                    results_list.append(res)
                    completed += 1
                    progress_bar.progress(completed / len(all_clauses))
            
            st.success("å®Œæˆï¼")
            results_list.sort(key=lambda x: x['åºå·'])
            st.session_state.results = pd.DataFrame(results_list)

    if st.session_state.results is not None:
        df = st.session_state.results
        summary_stats = {
            "total": len(df),
            "compliant": len(df[df['è¯„ä»·ç»“è®º'].str.contains("å®Œå…¨ç¬¦åˆ")]) ,
            "partial": len(df[df['è¯„ä»·ç»“è®º'].str.contains("éƒ¨åˆ†")]) ,
            "non_compliant": len(df[df['è¯„ä»·ç»“è®º'].str.contains("ç¼ºå¤±|ä¸ç¬¦åˆ")])
        }
        st.dataframe(df)
        word_file = generate_word_report(df, summary_stats)
        st.download_button("ğŸ“¥ ä¸‹è½½ç³»ç»Ÿæ€§è¯„ä»·æŠ¥å‘Š", word_file, "EHS_System_Report.docx", "application/vnd.openxmlformats-officedocument.wordprocessingml.document")
