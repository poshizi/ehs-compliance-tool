import streamlit as st
import pandas as pd
import os
import re
import zipfile
import io
import time
import requests
import json
import numpy as np
import docx 
from docx import Document
from docx.shared import Pt, Inches
from docx.enum.text import WD_ALIGN_PARAGRAPH
from concurrent.futures import ThreadPoolExecutor, as_completed
from tenacity import retry, stop_after_attempt, wait_exponential

# ====================
# Core Classes for AI & Data
# ====================

class LLMClient:
    """å¤„ç†ä¸å¤§æ¨¡å‹çš„äº¤äº’ (Embedding å’Œ Chat)"""
    def __init__(self, config):
        self.base_url = config.get('base_url', '').rstrip('/')
        self.api_key = config.get('api_key')
        self.model = config.get('model')
        self.embedding_model = config.get('embedding_model', 'text-embedding-004')
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        self.embedding_failed = False 

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
    def get_embedding(self, text):
        """è·å–æ–‡æœ¬çš„å‘é‡è¡¨ç¤º"""
        if self.embedding_failed: return None
        
        payload = {
            "input": text.replace("\n", " "),
            "model": self.embedding_model
        }
        
        # å°è¯•æ ‡å‡† OpenAI è·¯å¾„
        url = f"{self.base_url}/embeddings"
        
        try:
            response = requests.post(url, headers=self.headers, json=payload, timeout=10)
            if response.status_code == 200:
                data = response.json()
                if 'data' in data and len(data['data']) > 0:
                    return data['data'][0]['embedding']
                else:
                    return None
            else:
                print(f"Embedding failed: {response.status_code} - {response.text}")
                return None
        except Exception as e:
            print(f"Embedding error: {e}")
            return None

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
    def chat_completion(self, system_prompt, user_prompt):
        """è°ƒç”¨ Chat æ¥å£"""
        payload = {
            "model": self.model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            "temperature": 0.1,
            "response_format": {"type": "json_object"}
        }
        
        try:
            response = requests.post(f"{self.base_url}/chat/completions", headers=self.headers, json=payload, timeout=60)
            if response.status_code == 200:
                content = response.json()['choices'][0]['message']['content']
                content = content.replace("```json", "").replace("```", "")
                return json.loads(content)
            else:
                raise Exception(f"API Error {response.status_code}: {response.text}")
        except Exception as e:
            raise Exception(f"Request failed: {str(e)}")
            
    def test_connection(self):
        """æµ‹è¯•è¿æ¥çŠ¶æ€"""
        results = {"chat": False, "embedding": False, "msg": ""}
        
        # 1. æµ‹è¯• Chat
        try:
            self.chat_completion("You are a test bot.", "Reply JSON: {'status': 'ok'}")
            results['chat'] = True
        except Exception as e:
            results['msg'] += f"Chat Error: {str(e)}\n"
            
        # 2. æµ‹è¯• Embedding
        try:
            emb = self.get_embedding("test")
            if emb:
                results['embedding'] = True
            else:
                results['msg'] += "Embedding Error: Returned None (Check model name or API support)\n"
        except Exception as e:
             results['msg'] += f"Embedding Exception: {str(e)}\n"
             
        return results

class VectorStore:
    """æ··åˆæ£€ç´¢æ•°æ®åº“ (å‘é‡ + å…³é”®è¯)"""
    def __init__(self):
        self.documents = [] 
        self.vectors = []   
        self.llm_client = None

    def set_client(self, client):
        self.llm_client = client

    def add_documents(self, file_corpus):
        """å¤„ç†å¹¶å…¥åº“"""
        chunk_size = 500
        overlap = 50 
        
        self.documents = []
        texts_to_embed = []
        
        doc_id = 0
        for file in file_corpus:
            text = file['content']
            name = file['name']
            
            for i in range(0, len(text), chunk_size - overlap):
                chunk = text[i:i + chunk_size]
                if len(chunk) < 50: continue 
                
                # å…³é”®è¯æå–
                keywords = set(re.split(r'[ï¼Œã€‚ï¼›ï¼š\s]', chunk))
                keywords = [k for k in keywords if len(k) > 1]

                self.documents.append({
                    'id': doc_id,
                    'text': chunk,
                    'source': name,
                    'keywords': keywords 
                })
                texts_to_embed.append(chunk)
                doc_id += 1
        
        # å°è¯•å‘é‡åŒ–
        if self.llm_client:
            with st.status("æ­£åœ¨æ„å»ºç´¢å¼• (å°è¯•å‘é‡åŒ– + å…³é”®è¯åº“)...") as status:
                valid_vectors = []
                # ä½¿ç”¨å¹¶å‘
                with ThreadPoolExecutor(max_workers=5) as executor:
                    future_to_idx = {executor.submit(self.llm_client.get_embedding, t): i for i, t in enumerate(texts_to_embed)}
                    
                    results = [None] * len(texts_to_embed)
                    success_count = 0
                    
                    for future in as_completed(future_to_idx):
                        idx = future_to_idx[future]
                        vec = future.result()
                        results[idx] = vec
                        if vec is not None: success_count += 1
                
                self.vectors = results 
                
                if success_count == 0:
                    status.update(label="âš ï¸ å‘é‡åŒ–å…¨éƒ¨å¤±è´¥ (å°†é™çº§ä½¿ç”¨å…³é”®è¯æ£€ç´¢)", state="error")
                    st.warning("æç¤ºï¼šEmbedding API è°ƒç”¨å¤±è´¥ï¼Œå·²è‡ªåŠ¨åˆ‡æ¢ä¸º **å…³é”®è¯åŒ¹é…æ¨¡å¼**ã€‚è¯·æ£€æŸ¥ Embedding Model é…ç½®ã€‚" )
                else:
                    status.update(label=f"ç´¢å¼•æ„å»ºå®Œæˆ (å‘é‡åŒ–æˆåŠŸç‡: {success_count}/{len(texts_to_embed)})", state="complete")

    def search(self, query_text, top_k=3):
        """æ··åˆæ£€ç´¢"""
        vec_results = []
        
        # 1. å‘é‡æ£€ç´¢
        has_vectors = any(v is not None for v in self.vectors)
        if has_vectors and self.llm_client:
            query_vec = self.llm_client.get_embedding(query_text)
            if query_vec is not None:
                q_v = np.array(query_vec)
                norm_q = np.linalg.norm(q_v)
                if norm_q > 0:
                    scores = []
                    for i, doc_vec in enumerate(self.vectors):
                        if doc_vec is None: 
                            scores.append(-1)
                            continue
                        d_v = np.array(doc_vec)
                        norm_d = np.linalg.norm(d_v)
                        if norm_d == 0: scores.append(0)
                        else: scores.append(np.dot(d_v, q_v) / (norm_d * norm_q))
                    
                    top_indices = np.argsort(scores)[-top_k:][::-1]
                    for idx in top_indices:
                        if scores[idx] > 0:
                            vec_results.append({'doc': self.documents[idx], 'score': float(scores[idx]), 'method': 'vector'})

        # 2. å…³é”®è¯æ£€ç´¢ (å…œåº•)
        kw_results = []
        query_keywords = [k for k in re.split(r'[ï¼Œã€‚ï¼›ï¼š\s]', query_text) if len(k) > 1]
        
        for doc in self.documents:
            overlap = sum(1 for k in query_keywords if k in doc['text'])
            if overlap > 0:
                score = overlap / (len(query_keywords) + 1) * 0.8 
                kw_results.append({'doc': doc, 'score': score, 'method': 'keyword'})
        
        kw_results.sort(key=lambda x: x['score'], reverse=True)
        kw_results = kw_results[:top_k]

        combined = vec_results + kw_results
        seen_ids = set()
        final_results = []
        combined.sort(key=lambda x: x['score'], reverse=True)
        
        for res in combined:
            did = res['doc']['id']
            if did not in seen_ids:
                final_results.append({'source': res['doc']['source'], 'content': res['doc']['text'], 'score': res['score']})
                seen_ids.add(did)
            if len(final_results) >= top_k: break
                
        return final_results

# ====================
# Helper Functions
# ====================

def process_uploaded_files(uploaded_files):
    for uploaded_file in uploaded_files:
        if uploaded_file.name.endswith('.zip'):
            try:
                with zipfile.ZipFile(uploaded_file) as z:
                    for filename in z.namelist():
                        if filename.endswith('/') or filename.startswith('__MACOSX') or filename.startswith('._'): continue
                        if filename.endswith(('.docx', '.xlsx')):
                            with z.open(filename) as f: yield filename, f.read()
            except Exception as e: st.error(f"è§£å‹å¤±è´¥: {e}")
        else: yield uploaded_file.name, uploaded_file.getvalue()

def extract_text_from_content(filename, content):
    text = ""
    try:
        file_stream = io.BytesIO(content)
        if filename.endswith('.docx'):
            doc = docx.Document(file_stream)
            text = '\n'.join([para.text for para in doc.paragraphs])
        elif filename.endswith('.xlsx'):
            df_dict = pd.read_excel(file_stream, sheet_name=None, header=None)
            text_parts = []
            for sheet_name, df in df_dict.items():
                sheet_text = df.astype(str).apply(lambda x: ' '.join(x), axis=1)
                text_parts.append('\n'.join(sheet_text))
            text = '\n'.join(text_parts)
    except Exception as e: print(f"Error parsing {filename}: {e}")
    return text

def parse_regulation_clauses(text):
    pattern = r'(ç¬¬\s*[\dé›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+\s*æ¡|Article\s+\d+)'
    parts = re.split(pattern, text)
    clauses = []
    if len(parts) > 1:
        for i in range(1, len(parts), 2):
            title = parts[i].strip()
            content = parts[i+1].strip() if i+1 < len(parts) else ""
            applicability = "é€‚ç”¨"
            # åŸºç¡€é¢„åˆ¤ï¼Œå®é™…ç”±LLMåœ¨æ€ç»´é“¾ä¸­æœ€ç»ˆå†³å®š
            clauses.append({"æ¡æ¬¾å·": title, "æ³•è§„æ­£æ–‡": title + " " + content, "é€‚ç”¨æ€§": applicability})
    return clauses

def evaluate_single_clause(clause, vector_store, llm_client):
    """
    åŸºäº EHS ä¸“å®¶æ€ç»´é“¾çš„æ·±åº¦è¯„ä¼°
    """
    row = {
        "æ¡æ¬¾å·": clause['æ¡æ¬¾å·'],
        "æ³•è§„æ­£æ–‡": clause['æ³•è§„æ­£æ–‡'],
        "è¯„ä»·ç»“è®º": "âŒç¼ºå¤±/ä¸ç¬¦åˆ",
        "å·®è·åˆ†æ": "æœªæ£€ç´¢åˆ°ç›¸å…³åˆ¶åº¦",
        "æ”¹è¿›å»ºè®®": "è¯·è¡¥å……ç›¸å…³ç®¡ç†è§„å®š",
        "æ”¯æ’‘è¯æ®": "æ— ",
        "åŒ¹é…åº¦": 0.0
    }

    # 1. æ··åˆæ£€ç´¢
    search_results = vector_store.search(clause['æ³•è§„æ­£æ–‡'], top_k=3)
    
    # æ— è®ºæ˜¯å¦æ£€ç´¢åˆ°ï¼Œéƒ½å¿…é¡»äº¤ç»™ LLM åˆ¤æ–­ï¼ˆå°¤å…¶æ˜¯åˆ¤æ–­æ˜¯å¦é€‚ç”¨ï¼‰
    # å¦‚æœæ²¡æ£€ç´¢åˆ°ï¼ŒLLM ä¼šåŸºäºâ€œæ— ç›¸å…³åˆ¶åº¦â€è¿›è¡Œåˆ¤å®š
    
    top_score = search_results[0]['score'] if search_results else 0
    row['åŒ¹é…åº¦'] = top_score
        
    evidence_text = ""
    if search_results:
        for i, res in enumerate(search_results):
            evidence_text += f"å‚è€ƒåˆ¶åº¦ç‰‡æ®µ {i+1} (æ¥æº: {res['source']}):\n{res['content'][:800]}\n---\n"
    else:
        evidence_text = "æœªæ£€ç´¢åˆ°ä»»ä½•ç›¸å…³çš„ä¼ä¸šå†…éƒ¨åˆ¶åº¦æ–‡æ¡£ã€‚"
    
    # 2. æ„é€ ä¸“å®¶ Prompt
    system_prompt = """ä½ æ˜¯ä¸€åå…·æœ‰20å¹´ç»éªŒçš„EHSç®¡ç†ä¸“å®¶ï¼Œç²¾é€šä¸­å›½EHSæ³•è§„æ ‡å‡†ï¼Œæ“…é•¿ä»“å‚¨ç‰©æµåœºæ™¯ã€‚
    è¯·å¯¹ç»™å®šçš„æ³•è§„æ¡æ¬¾è¿›è¡Œåˆè§„æ€§è¯„ä»·ã€‚ä¸¥æ ¼æ‰§è¡Œä»¥ä¸‹æ€ç»´é“¾ï¼š
    1. è§£è¯»ï¼šç†è§£æ¡æ¬¾æ ¸å¿ƒè¦æ±‚ï¼ˆäººæœºæ–™æ³•ç¯ï¼‰ï¼Œåˆ¤å®šæ˜¯å¦é€‚ç”¨äºç‰©æµä»“å‚¨ä¼ä¸šã€‚å¦‚æœä¸é€‚ç”¨ï¼Œç›´æ¥æ ‡è®°â€œä¸é€‚ç”¨â€ã€‚
    2. æ¯”å¯¹ï¼šå¯¹æ¯”æ³•è§„è¦æ±‚ä¸æä¾›çš„ä¼ä¸šåˆ¶åº¦ç‰‡æ®µã€‚æ˜¯å¦è¦†ç›–æ‰€æœ‰è¦ç´ ï¼Ÿé’ˆå¯¹ç‰©æµåœºæ™¯æ˜¯å¦å…·ä½“å¯æ‰§è¡Œï¼Ÿ
    3. åˆ¤å®šï¼šç»™å‡ºå®šæ€§ç»“è®ºã€‚
    
    è¯·ä¿æŒå®¢è§‚ã€çŠ€åˆ©ã€ç›´æ¥ã€‚"""
    
    user_prompt = f"""
    ã€æ³•è§„æ¡æ¬¾ã€‘
    {clause['æ³•è§„æ­£æ–‡']}

    ã€ä¼ä¸šåˆ¶åº¦ç°çŠ¶ï¼ˆæ£€ç´¢åˆ°çš„æœ€ç›¸å…³ç‰‡æ®µï¼‰ã€‘
    {evidence_text}

    ã€ä»»åŠ¡è¦æ±‚ã€‘
    è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¿”å› JSON ç»“æœï¼š
    {{
        "applicability": "é€‚ç”¨" æˆ– "ä¸é€‚ç”¨",
        "compliance_status": "å®Œå…¨ç¬¦åˆ" æˆ– "éƒ¨åˆ†ç¬¦åˆ/éœ€å®Œå–„" æˆ– "ç¼ºå¤±/ä¸ç¬¦åˆ" æˆ– "ä¸é€‚ç”¨",
        "gap_analysis": "150å­—ä»¥å†…ã€‚è‹¥ä¸é€‚ç”¨å¡«'ä¸é€‚ç”¨'ã€‚è‹¥é€‚ç”¨ï¼Œåˆ†æå…·ä½“ç¼ºäº†ä»€ä¹ˆï¼ˆå¦‚è´£ä»»äººã€é¢‘æ¬¡ã€ç‰©æµç‰¹å®šæªæ–½ï¼‰æˆ–è¯´æ˜é€šè¿‡å“ªå‡ æ¡å®ç°äº†åˆè§„ã€‚",
        "improvement_suggestion": "è‹¥å®Œå…¨ç¬¦åˆå¡«'æ— 'ã€‚å¦åˆ™ç»“åˆç‰©æµä»“å‚¨ç‰¹ç‚¹ç»™å‡º1-2æ¡å…·ä½“å»ºè®®ã€‚",
        "evidence_summary": "åˆ—å‡ºæœ€åŒ¹é…çš„åˆ¶åº¦åç§°åŠå…³é”®å¥æ‘˜è¦ï¼ˆè‹¥æ— åˆ™å¡«'æœªæ£€ç´¢åˆ°ç›¸å…³åˆ¶åº¦'ï¼‰"
    }}
    """
    
    try:
        result = llm_client.chat_completion(system_prompt, user_prompt)
        
        # è§£æç»“æœ
        status = result.get('compliance_status', 'ç¼ºå¤±/ä¸ç¬¦åˆ')
        # ç»Ÿä¸€çŠ¶æ€å›¾æ ‡
        if "å®Œå…¨ç¬¦åˆ" in status: status = "âœ…å®Œå…¨ç¬¦åˆ"
        elif "éƒ¨åˆ†" in status or "éœ€å®Œå–„" in status: status = "âš ï¸éƒ¨åˆ†ç¬¦åˆ/éœ€å®Œå–„"
        elif "ä¸é€‚ç”¨" in status: status = "â—ä¸é€‚ç”¨"
        else: status = "âŒç¼ºå¤±/ä¸ç¬¦åˆ"
        
        row['è¯„ä»·ç»“è®º'] = status
        row['å·®è·åˆ†æ'] = result.get('gap_analysis', 'æ— åˆ†æ')
        row['æ”¹è¿›å»ºè®®'] = result.get('improvement_suggestion', 'æ— å»ºè®®')
        row['æ”¯æ’‘è¯æ®'] = result.get('evidence_summary', 'æ— ')
        
    except Exception as e: 
        row['å·®è·åˆ†æ'] = f"LLMåˆ†æå¤±è´¥: {str(e)}"
        
    return row

def generate_word_report(df_results, summary_stats):
    """ç”Ÿæˆ Word æ ¼å¼çš„åˆè§„æ€§è¯„ä»·æŠ¥å‘Š"""
    doc = Document()
    
    # æ ‡é¢˜
    title = doc.add_heading('EHSæ³•è§„åˆè§„æ€§è¯„ä»·æŠ¥å‘Š', 0)
    title.alignment = WD_ALIGN_PARAGRAPH.CENTER
    
    doc.add_paragraph(f"è¯„ä»·æ—¥æœŸ: {time.strftime('%Y-%m-%d')}")
    
    # ç¬¬ä¸€éƒ¨åˆ†ï¼šæ€»ä½“è¯„ä»·
    doc.add_heading('ç¬¬ä¸€éƒ¨åˆ†ï¼šæ€»ä½“è¯„ä»·', level=1)
    
    # ç»Ÿè®¡æ•°æ®
    p = doc.add_paragraph()
    p.add_run(f"æœ¬æ¬¡å…±åˆ†ææ³•è§„æ¡æ¬¾ {summary_stats['total']} æ¡.\n")
    p.add_run(f"âœ… å®Œå…¨ç¬¦åˆ: {summary_stats['compliant']} æ¡\n")
    p.add_run(f"âš ï¸ éƒ¨åˆ†ç¬¦åˆ/éœ€å®Œå–„: {summary_stats['partial']} æ¡\n")
    p.add_run(f"âŒ ç¼ºå¤±/ä¸ç¬¦åˆ: {summary_stats['non_compliant']} æ¡\n")
    p.add_run(f"â— ä¸é€‚ç”¨: {summary_stats['na']} æ¡")
    
    # ä¸“å®¶ç»¼è¿° (æ¨¡æ‹Ÿ)
    doc.add_heading('ä¸“å®¶ç»¼è¿°ä¸å»ºè®®', level=2)
    overall_conclusion = "æ€»ä½“æ¥çœ‹ï¼Œä¼ä¸šå»ºç«‹äº†åŸºæœ¬çš„EHSç®¡ç†æ¡†æ¶ã€‚ä½†åœ¨ç‰©æµä»“å‚¨ç‰¹å®šåœºæ™¯çš„è½åœ°æ‰§è¡Œç»†èŠ‚ä¸Šï¼ˆå¦‚ç°åœºä½œä¸šç®¡æ§ã€éšæ‚£æ’æŸ¥é¢‘æ¬¡ï¼‰ä»æœ‰å¾…å®Œå–„ã€‚å»ºè®®é‡ç‚¹å…³æ³¨â€œç¼ºå¤±â€å’Œâ€œéƒ¨åˆ†ç¬¦åˆâ€çš„æ¡æ¬¾ï¼Œç»“åˆæ”¹è¿›å»ºè®®å°½å¿«è½å®æ•´æ”¹ã€‚"
    if summary_stats['compliance_rate'] > 85:
        overall_conclusion = "ä¼ä¸šEHSç®¡ç†åˆ¶åº¦ä½“ç³»è¾ƒä¸ºå®Œå¤‡ï¼Œä¸æ³•è§„è¦æ±‚åŒ¹é…åº¦è¾ƒé«˜ã€‚å»ºè®®ç»§ç»­ä¿æŒï¼Œå¹¶å…³æ³¨æ–°æ³•è§„çš„å‘å¸ƒä¸æ›´æ–°ã€‚"
    
    doc.add_paragraph(overall_conclusion)
    
    # ç¬¬äºŒéƒ¨åˆ†ï¼šè¯¦ç»†è¯„ä»·çŸ©é˜µ
    doc.add_heading('ç¬¬äºŒéƒ¨åˆ†ï¼šè¯¦ç»†åˆè§„æ€§è¯„ä»·çŸ©é˜µ', level=1)
    
    # åˆ›å»ºè¡¨æ ¼
    table = doc.add_table(rows=1, cols=6)
    table.style = 'Table Grid'
    
    # è¡¨å¤´
    hdr_cells = table.rows[0].cells
    headers = ["åºå·", "æ³•è§„æ¡æ¬¾", "è¯„ä»·ç»“è®º", "å·®è·åˆ†æä¸è®ºæ®", "æ”¹è¿›å»ºè®®", "æ”¯æ’‘è¯æ®"]
    for i, header in enumerate(headers):
        hdr_cells[i].text = header
        hdr_cells[i].paragraphs[0].runs[0].font.bold = True
    
    # å¡«å……æ•°æ®
    for idx, row in df_results.iterrows():
        row_cells = table.add_row().cells
        row_cells[0].text = str(row.get('åºå·', idx + 1))
        row_cells[1].text = str(row.get('æ³•è§„æ­£æ–‡', ''))
        row_cells[2].text = str(row.get('è¯„ä»·ç»“è®º', ''))
        row_cells[3].text = str(row.get('å·®è·åˆ†æ', ''))
        row_cells[4].text = str(row.get('æ”¹è¿›å»ºè®®', ''))
        row_cells[5].text = str(row.get('æ”¯æ’‘è¯æ®', ''))
        
    # ä¿å­˜åˆ°å†…å­˜
    f = io.BytesIO()
    doc.save(f)
    f.seek(0)
    return f

# ====================
# Streamlit UI
# ====================

st.set_page_config(page_title="EHSä¸“å®¶åˆè§„ç³»ç»Ÿ (Expert Edition)", layout="wide")

if 'results' not in st.session_state: st.session_state.results = None

st.title("ğŸ›¡ï¸ EHSæ³•è§„åˆè§„æ€§æ™ºèƒ½è¯„ä»·ç³»ç»Ÿ (Expert Edition)")
st.markdown("ğŸ‘¨â€ğŸ« **ä¸“å®¶æ¨¡å¼**: åŸºäº20å¹´EHSç»éªŒçš„æ·±åº¦æ€ç»´é“¾åˆ†æï¼Œè‡ªåŠ¨ç”Ÿæˆ Gap Analysis ä¸æ”¹è¿›å»ºè®®ã€‚")

with st.sidebar:
    st.header("1. API é…ç½®")
    llm_base_url = st.text_input("API Base URL", value="https://generativelanguage.googleapis.com/v1beta/openai", help="ä¾‹å¦‚ https://api.openai.com/v1")
    llm_api_key = st.text_input("API Key", type="password")
    
    col_m1, col_m2 = st.columns(2)
    with col_m1:
        llm_model_name = st.text_input("Chat Model", value="gemini-2.0-flash")
    with col_m2:
        embedding_model_name = st.text_input("Embedding Model", value="text-embedding-004")

    if st.button("ğŸ”Œ æµ‹è¯• API è¿æ¥", use_container_width=True):
        if not llm_api_key:
            st.error("è¯·å…ˆå¡«å†™ API Key")
        else:
            with st.spinner("æ­£åœ¨æµ‹è¯•è¿æ¥..."):
                test_config = {"base_url": llm_base_url, "api_key": llm_api_key, "model": llm_model_name, "embedding_model": embedding_model_name}
                client = LLMClient(test_config)
                res = client.test_connection()
                if res['chat']: st.success(f"âœ… Chat: {llm_model_name} OK")
                else: st.error("âŒ Chat Failed")
                if res['embedding']: st.success(f"âœ… Embedding: {embedding_model_name} OK")
                else: st.error("âŒ Embedding Failed")

    st.divider()
    st.header("2. æ–‡ä»¶ä¸Šä¼ ")
    reg_files = st.file_uploader("ä¸Šä¼ æ³•è§„ (docx/zip)", type=['docx', 'zip'], accept_multiple_files=True, key="reg")
    policy_files = st.file_uploader("ä¸Šä¼ åˆ¶åº¦ (docx/xlsx/zip)", type=['docx', 'xlsx', 'zip'], accept_multiple_files=True, key="pol")

if st.button("ğŸš€ å¼€å§‹ä¸“å®¶çº§è¯„ä¼°", type="primary"):
    if not (reg_files and policy_files and llm_api_key):
        st.error("è¯·ç¡®ä¿æ–‡ä»¶å·²ä¸Šä¼ ä¸” API Key å·²å¡«å†™ã€‚" )
    else:
        llm_config = {"base_url": llm_base_url, "api_key": llm_api_key, "model": llm_model_name, "embedding_model": embedding_model_name}
        client = LLMClient(llm_config)
        vector_store = VectorStore()
        vector_store.set_client(client)
        
        # 1. æ„å»ºåˆ¶åº¦åº“
        policy_corpus = []
        for name, content in process_uploaded_files(policy_files):
            text = extract_text_from_content(name, content)
            if text and len(text.strip()) > 0: policy_corpus.append({'name': name, 'content': text})
            
        if not policy_corpus:
            st.error("æœ‰æ•ˆåˆ¶åº¦å†…å®¹ä¸ºç©ºã€‚" )
            st.stop()
            
        vector_store.add_documents(policy_corpus)
        
        # 2. è§£ææ³•è§„
        all_clauses = []
        for name, content in process_uploaded_files(reg_files):
            text = extract_text_from_content(name, content)
            clauses = parse_regulation_clauses(text)
            for i, c in enumerate(clauses):
                c['source_file'] = name 
                c['åºå·'] = i + 1
                all_clauses.append(c)
        
        if not all_clauses:
             st.error("æœªè§£æå‡ºä»»ä½•æ³•è§„æ¡æ¬¾ã€‚" )
             st.stop()

        st.info(f"å…±è¯†åˆ«å‡º {len(all_clauses)} æ¡æ³•è§„æ¡æ¬¾ï¼Œæ­£åœ¨æ‰§è¡Œä¸“å®¶çº§åˆ†æ (Deep Analysis)...")
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        results_list = []
        total_tasks = len(all_clauses)
        completed_tasks = 0
        
        with ThreadPoolExecutor(max_workers=10) as executor:
            future_to_clause = {executor.submit(evaluate_single_clause, clause, vector_store, client): clause for clause in all_clauses}
            for future in as_completed(future_to_clause):
                try:
                    res = future.result()
                    res['æ³•è§„æ–‡ä»¶'] = future_to_clause[future]['source_file']
                    res['åºå·'] = future_to_clause[future]['åºå·']
                    results_list.append(res)
                except Exception as exc: st.warning(f"åˆ†æå¼‚å¸¸: {exc}")
                completed_tasks += 1
                progress_bar.progress(completed_tasks / total_tasks)
                status_text.text(f"å·²å®Œæˆ: {completed_tasks}/{total_tasks} ...")
                
        st.success("åˆ†æå®Œæˆï¼")
        # æŒ‰åºå·æ’åº
        results_list.sort(key=lambda x: x['åºå·'])
        st.session_state.results = pd.DataFrame(results_list)

if st.session_state.results is not None:
    df = st.session_state.results
    
    st.divider()
    st.subheader("ğŸ“Š ä¸“å®¶è¯„ä¼°çœ‹æ¿")
    
    compliant_count = len(df[df['è¯„ä»·ç»“è®º'].str.contains("å®Œå…¨ç¬¦åˆ")])
    partial_count = len(df[df['è¯„ä»·ç»“è®º'].str.contains("éƒ¨åˆ†ç¬¦åˆ")])
    non_compliant_count = len(df[df['è¯„ä»·ç»“è®º'].str.contains("ç¼ºå¤±|ä¸ç¬¦åˆ")])
    na_count = len(df[df['è¯„ä»·ç»“è®º'].str.contains("ä¸é€‚ç”¨")])
    total_valid = len(df) - na_count
    compliance_rate = (compliant_count / total_valid * 100) if total_valid > 0 else 0
    
    summary_stats = {
        "total": len(df),
        "compliant": compliant_count,
        "partial": partial_count,
        "non_compliant": non_compliant_count,
        "na": na_count,
        "compliance_rate": compliance_rate
    }

    col1, col2, col3, col4 = st.columns(4)
    col1.metric("å®Œå…¨ç¬¦åˆ", compliant_count)
    col2.metric("éœ€å®Œå–„", partial_count)
    col3.metric("ç¼ºå¤±/ä¸ç¬¦åˆ", non_compliant_count)
    col4.metric("åˆè§„ç‡ (é€‚ç”¨é¡¹)", f"{compliance_rate:.1f}%")
    
    # ç»“æœå±•ç¤ºè¡¨æ ¼
    st.dataframe(
        df, 
        column_config={
            "æ³•è§„æ­£æ–‡": st.column_config.TextColumn("æ³•è§„æ¡æ¬¾", width="medium"),
            "å·®è·åˆ†æ": st.column_config.TextColumn("å·®è·åˆ†æä¸è®ºæ®", width="large"),
            "æ”¹è¿›å»ºè®®": st.column_config.TextColumn("æ”¹è¿›å»ºè®®", width="medium"),
            "æ”¯æ’‘è¯æ®": st.column_config.TextColumn("è¯æ®æ‘˜è¦", width="medium")
        }, 
        use_container_width=True, 
        height=600
    )
    
    # å¯¼å‡ºæŠ¥å‘Š
    st.subheader("ğŸ“¥ æŠ¥å‘Šå¯¼å‡º")
    col_d1, col_d2 = st.columns(2)
    
    with col_d1:
        word_file = generate_word_report(df, summary_stats)
        st.download_button(
            label="ğŸ“„ ä¸‹è½½ä¸“å®¶åˆè§„æ€§è¯„ä»·æŠ¥å‘Š (.docx)",
            data=word_file,
            file_name=f"EHS_Expert_Report_{time.strftime('%Y%m%d')}.docx",
            mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
        )
    
    with col_d2:
        csv = df.to_csv(index=False).encode('utf-8-sig')
        st.download_button("ğŸ“Š ä¸‹è½½è¯¦ç»†æ•°æ®è¡¨ (.csv)", csv, "ehs_compliance_data.csv", "text/csv")
